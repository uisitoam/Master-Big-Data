\documentclass[8pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{multicol}
\geometry{a4paper, bottom=3cm, top = 1cm, left=2cm, right=2cm}
\setlength{\parindent}{0pt}

\begin{document}

\title{Tecnologías de gestión de información no estructurada}
\author{Preguntas generados con ChatGPT a partir del temario}
\date{}
\maketitle
\begin{multicols}{2}
\begin{enumerate}
\item What is the main goal of Big Data in textual data management?
\begin{itemize}
    \item[a)] Storing vast amounts of data
    \item[b)] Simplifying data storage structures
    \item[c)] Transforming raw data into actionable knowledge
    \item[d)] Reducing redundancy in databases
\end{itemize}

\item Which of the following is an example of textual data as Big Data?
\begin{itemize}
    \item[a)] Temperature logs
    \item[b)] Scientific literature
    \item[c)] Financial spreadsheets
    \item[d)] Network configurations
\end{itemize}

\item What proportion of the world's data generated annually is textual?
\begin{itemize}
    \item[a)] Negligible
    \item[b)] A significant portion
    \item[c)] Entirely textual
    \item[d)] Only a fraction
\end{itemize}

\item What characterizes structured data?
\begin{itemize}
    \item[a)] Random arrangement of content
    \item[b)] Well-defined schemas
    \item[c)] Exclusively textual representation
    \item[d)] Lack of metadata
\end{itemize}

\item Why is textual data challenging to process?
\begin{itemize}
    \item[a)] It consumes less storage
    \item[b)] It lacks explicit structure
    \item[c)] It is highly numeric
    \item[d)] It requires no computational interpretation
\end{itemize}

\item What does Natural Language Processing (NLP) rely on for understanding text?
\begin{itemize}
    \item[a)] Heuristic and statistical approaches
    \item[b)] Manual interpretation
    \item[c)] Only deep semantic models
    \item[d)] Strictly predefined templates
\end{itemize}

\item Which system allows quick access to relevant textual data?
\begin{itemize}
    \item[a)] NLP models
    \item[b)] Text retrieval systems
    \item[c)] Data clustering tools
    \item[d)] Semantic models
\end{itemize}

\item What is the focus of text mining in Big Data?
\begin{itemize}
    \item[a)] Organizing data alphabetically
    \item[b)] Generating raw textual data
    \item[c)] Extracting valuable knowledge from text
    \item[d)] Reducing text length
\end{itemize}

\item In the "pull" mode of accessing information, what role does the user play?
\begin{itemize}
    \item[a)] Passive recipient
    \item[b)] Active seeker
    \item[c)] Content generator
    \item[d)] Recommendation builder
\end{itemize}

\item What is the goal of a Textual Information System (TIS)?
\begin{itemize}
    \item[a)] Create unstructured data repositories
    \item[b)] Match relevant information to users' needs
    \item[c)] Replace traditional search engines
    \item[d)] Perform manual data classification
\end{itemize}

\item What process involves grouping similar textual objects?
\begin{itemize}
    \item[a)] Text retrieval
    \item[b)] Clustering
    \item[c)] Categorization
    \item[d)] Visualization
\end{itemize}

\item Which task generates concise representations of large textual content?
\begin{itemize}
    \item[a)] Extraction
    \item[b)] Summarization
    \item[c)] Navigation
    \item[d)] Filtering
\end{itemize}

\item What is the main purpose of sentiment analysis in textual data?
\begin{itemize}
    \item[a)] Organizing text lexically
    \item[b)] Extracting and categorizing opinions
    \item[c)] Visualizing content patterns
    \item[d)] Translating textual data
\end{itemize}

\item How does a recommendation system in a TIS function?
\begin{itemize}
    \item[a)] By extracting patterns in data
    \item[b)] By proactively suggesting relevant information
    \item[c)] By creating unsupervised learning models
    \item[d)] By analyzing user-generated summaries
\end{itemize}

\item What technique discovers patterns and trends in large textual datasets?
\begin{itemize}
    \item[a)] Filtering
    \item[b)] Thematic analysis
    \item[c)] Snippet generation
    \item[d)] Data reduction
\end{itemize}

\item Which of the following is a characteristic of unsupervised thematic analysis?
\begin{itemize}
    \item[a)] Predefined themes
    \item[b)] Algorithm-generated topics
    \item[c)] Strict manual labeling
    \item[d)] Reliance on metadata only
\end{itemize}

\item Which approach is commonly used in most TIS for text representation?
\begin{itemize}
    \item[a)] Semantic deep learning
    \item[b)] Bag-of-words model
    \item[c)] Ontology-based parsing
    \item[d)] Strict rule-based systems
\end{itemize}

\item What is a key advantage of visualizing textual data?
\begin{itemize}
    \item[a)] Avoiding analysis complexity
    \item[b)] Enhancing pattern recognition
    \item[c)] Eliminating redundancy
    \item[d)] Simplifying query syntax
\end{itemize}

\item What does information extraction focus on?
\begin{itemize}
    \item[a)] Reducing data size
    \item[b)] Identifying entities and their relationships
    \item[c)] Simplifying textual metadata
    \item[d)] Categorizing user preferences
\end{itemize}

\item What does a snippet in a search result typically represent?
\begin{itemize}
    \item[a)] A dynamic summary based on the query
    \item[b)] An unrelated document
    \item[c)] The metadata of the file
    \item[d)] The full text of the search result
\end{itemize}


\item What is the primary goal of Natural Language Processing (NLP)?
\begin{itemize}
    \item[a)] Programming efficient algorithms
    \item[b)] Enabling machines to understand and generate human language
    \item[c)] Enhancing computer hardware performance
    \item[d)] Managing large datasets
\end{itemize}

\item Which of the following tasks is associated with lexical analysis?
\begin{itemize}
    \item[a)] Determining the meaning of a sentence
    \item[b)] Identifying grammatical relationships
    \item[c)] Breaking down text into meaningful units like words
    \item[d)] Resolving ambiguity in context
\end{itemize}

\item What is semantic analysis in NLP primarily concerned with?
\begin{itemize}
    \item[a)] Understanding sentence meaning
    \item[b)] Resolving syntactic ambiguities
    \item[c)] Analyzing word frequencies
    \item[d)] Determining sentence structure
\end{itemize}

\item In NLP, what is an example of pragmatic analysis?
\begin{itemize}
    \item[a)] Identifying parts of speech
    \item[b)] Determining the intent behind a statement
    \item[c)] Parsing grammatical structures
    \item[d)] Generating language models
\end{itemize}

\item What does syntactic analysis focus on?
\begin{itemize}
    \item[a)] Understanding the relationships between words in a sentence
    \item[b)] Translating words into numerical representations
    \item[c)] Recognizing emotions in text
    \item[d)] Creating visualizations of data
\end{itemize}

\item Why is NLP considered "complete in AI"?
\begin{itemize}
    \item[a)] It requires vast computational resources
    \item[b)] It is as difficult as other complex AI problems
    \item[c)] It focuses on data mining
    \item[d)] It always requires deep learning techniques
\end{itemize}

\item Which of the following is a common challenge in NLP?
\begin{itemize}
    \item[a)] Lack of computational tools
    \item[b)] Limited datasets
    \item[c)] Ambiguities in natural language
    \item[d)] Inefficient programming languages
\end{itemize}

\item What significant shift occurred in NLP research after the 1980s?
\begin{itemize}
    \item[a)] Transition from symbolic to statistical approaches
    \item[b)] Focus on hardware advancements
    \item[c)] Abandonment of speech recognition
    \item[d)] Exclusive reliance on human linguistic knowledge
\end{itemize}

\item What was a limitation of symbolic approaches to NLP?
\begin{itemize}
    \item[a)] Dependency on machine learning techniques
    \item[b)] Inability to scale for real-world applications
    \item[c)] Excessive reliance on unstructured data
    \item[d)] Lack of support from computational linguists
\end{itemize}

\item Early NLP systems like Eliza relied on:
\begin{itemize}
    \item[a)] Deep learning algorithms
    \item[b)] Superficial rule-based techniques
    \item[c)] Statistical analysis of large datasets
    \item[d)] Neural network architectures
\end{itemize}

\item What does \"POS tagging\" in NLP refer to?
\begin{itemize}
    \item[a)] Predicting sentence probability
    \item[b)] Assigning parts of speech to words
    \item[c)] Generating text data
    \item[d)] Analyzing grammatical dependencies
\end{itemize}

\item Which statistical model assumes words in a sequence are generated independently?
\begin{itemize}
    \item[a)] Bigram model
    \item[b)] Unigram model
    \item[c)] Neural network model
    \item[d)] Contextual model
\end{itemize}

\item What is the primary purpose of smoothing in language models?
\begin{itemize}
    \item[a)] Enhance training efficiency
    \item[b)] Reduce data dimensionality
    \item[c)] Assign probabilities to unseen events
    \item[d)] Optimize computational speed
\end{itemize}

\item What is a key advantage of statistical approaches over symbolic ones in NLP?
\begin{itemize}
    \item[a)] Higher dependency on handcrafted rules
    \item[b)] Greater robustness to real-world variability
    \item[c)] Requirement for linguistic experts
    \item[d)] Elimination of ambiguity in text
\end{itemize}

\item Which of the following is an example of a high-level text representation?
\begin{itemize}
    \item[a)] Character sequences
    \item[b)] Word embeddings with semantic relationships
    \item[c)] Unstructured text documents
    \item[d)] Paragraph-level sentiment scores
\end{itemize}

\item In speech recognition, language models help by:
\begin{itemize}
    \item[a)] Identifying phonemes
    \item[b)] Predicting probable word sequences
    \item[c)] Filtering noise from audio input
    \item[d)] Translating text to audio
\end{itemize}

\item How do language models assist in text categorization?
\begin{itemize}
    \item[a)] They tokenize input sentences
    \item[b)] They assign probabilities to thematic topics
    \item[c)] They parse grammatical structures
    \item[d)] They generate summaries of text
\end{itemize}

\item Named Entity Recognition (NER) involves:
\begin{itemize}
    \item[a)] Identifying grammatical errors
    \item[b)] Recognizing specific entities like names and locations
    \item[c)] Analyzing syntactic structures
    \item[d)] Detecting sentiment in text
\end{itemize}

\item What is one limitation of semantic analysis in NLP?
\begin{itemize}
    \item[a)] Requires too much memory
    \item[b)] It cannot handle labeled datasets
    \item[c)] Difficulty in processing ambiguous sentences
    \item[d)] Limited applicability in non-English texts
\end{itemize}

\item Which task would benefit most from discourse analysis?
\begin{itemize}
    \item[a)] Translating isolated sentences
    \item[b)] Summarizing multi-paragraph documents
    \item[c)] Identifying word synonyms
    \item[d)] Counting word frequencies
\end{itemize}

\item What does \"contextual word embedding\" refer to?
\begin{itemize}
    \item[a)] Representing words with fixed vector sizes
    \item[b)] Capturing meaning based on surrounding words
    \item[c)] Tokenizing text into characters
    \item[d)] Eliminating ambiguity from language
\end{itemize}

\item A unigram model is most suitable for:
\begin{itemize}
    \item[a)] Capturing long-range dependencies
    \item[b)] Representing independent word probabilities
    \item[c)] Parsing syntactic structures
    \item[d)] Analyzing discourse coherence
\end{itemize}

\item Why is deep analysis of text representation often challenging?
\begin{itemize}
    \item[a)] Limited linguistic knowledge
    \item[b)] Dependence on massive labeled datasets
    \item[c)] Over-reliance on character-level models
    \item[d)] Insufficient computational power
\end{itemize}

\item How does a statistical model infer semantic relationships?
\begin{itemize}
    \item[a)] Using predefined rules
    \item[b)] Analyzing word co-occurrences
    \item[c)] Assigning probabilities manually
    \item[d)] Ignoring outliers in data
\end{itemize}

\item Which feature differentiates pragmatic analysis from semantic analysis?
\begin{itemize}
    \item[a)] Focus on sentence grammar
    \item[b)] Dependence on textual context and intent
    \item[c)] Use of statistical models
    \item[d)] Reliance on syntactic parsing
\end{itemize}

\item What is one of the primary roles of textual data access technologies?
\begin{itemize}
    \item[a)] Reducing hardware dependencies
    \item[b)] Enhancing database storage efficiency
    \item[c)] Connecting users with relevant information at the right time
    \item[d)] Automating all data retrieval processes
\end{itemize}

\item In textual data access, what does the \"push\" mode imply?
\begin{itemize}
    \item[a)] Users actively query a system for information
    \item[b)] The system proactively delivers relevant information to users
    \item[c)] Data is manually transferred between users
    \item[d)] Systems focus solely on structured data
\end{itemize}

\item Which of the following best describes the \"pull\" mode in textual data access?
\begin{itemize}
    \item[a)] Systems automatically recommend content
    \item[b)] Users initiate the retrieval of relevant data
    \item[c)] Data is filtered through machine learning models
    \item[d)] Retrieval relies solely on pre-defined rules
\end{itemize}

\item What type of information needs is often associated with the \"pull\" mode?
\begin{itemize}
    \item[a)] Long-term, stable needs
    \item[b)] Dynamic, ad hoc needs
    \item[c)] Fixed and structured needs
    \item[d)] Automated, unsupervised needs
\end{itemize}

\item Why is ad hoc retrieval considered challenging?
\begin{itemize}
    \item[a)] It requires structured datasets
    \item[b)] There is limited user feedback for optimization
    \item[c)] It only applies to numeric data
    \item[d)] It mandates constant user interaction
\end{itemize}

\item What is the primary benefit of integrating \"push\" and \"pull\" modes in a system?
\begin{itemize}
    \item[a)] To eliminate user input requirements
    \item[b)] To ensure consistent data analysis
    \item[c)] To maximize flexibility in accessing data
    \item[d)] To automate data structuring processes
\end{itemize}

\item In multimodal interaction systems, what is one advantage of navigation?
\begin{itemize}
    \item[a)] It guarantees accurate query results
    \item[b)] Users can explore topics without a fixed goal
    \item[c)] It eliminates the need for keyword-based searches
    \item[d)] All results are pre-ranked for relevance
\end{itemize}

\item What does a topic map in a search interface typically support?
\begin{itemize}
    \item[a)] Statistical analysis of search queries
    \item[b)] User navigation through interconnected topics
    \item[c)] Automatic data labeling for machine learning
    \item[d)] Compression of large datasets
\end{itemize}

\item What action allows users to explore documents within a thematic region on a topic map?
\begin{itemize}
    \item[a)] Submitting new queries
    \item[b)] Clicking on thematic nodes
    \item[c)] Adjusting system settings
    \item[d)] Switching between datasets
\end{itemize}

\item What is an example of a \"long-range jump\" in a topic map system?
\begin{itemize}
    \item[a)] Zooming in on a child node
    \item[b)] Submitting a new search query
    \item[c)] Navigating between neighboring nodes
    \item[d)] Viewing a local map segment
\end{itemize}

\item What is the primary goal of a text retrieval (TR) system?
\begin{itemize}
    \item[a)] To store textual data efficiently
    \item[b)] To identify relevant documents for a user query
    \item[c)] To automate database schema creation
    \item[d)] To categorize images alongside text
\end{itemize}

\item How does a text retrieval system differ from a database system?
\begin{itemize}
    \item[a)] Text retrieval systems manage structured data
    \item[b)] Database systems are not designed for numeric operations
    \item[c)] Text retrieval focuses on unstructured text
    \item[d)] Database systems emphasize vague query handling
\end{itemize}

\item Why are keyword-based queries often insufficient for text retrieval?
\begin{itemize}
    \item[a)] They only support long-form queries
    \item[b)] They depend on predefined database schemas
    \item[c)] They lack specificity and completeness
    \item[d)] They cannot handle numeric datasets
\end{itemize}

\item In text retrieval, what is the role of relevance feedback?
\begin{itemize}
    \item[a)] To filter irrelevant datasets
    \item[b)] To improve future query results using user feedback
    \item[c)] To enhance system processing speeds
    \item[d)] To structure unstructured text
\end{itemize}

\item What is a key challenge in designing text retrieval systems?
\begin{itemize}
    \item[a)] Managing structured data
    \item[b)] Ensuring low computational overhead
    \item[c)] Modeling user information needs
    \item[d)] Standardizing query languages
\end{itemize}

\item What distinguishes document selection from ranking in retrieval systems?
\begin{itemize}
    \item[a)] Selection involves absolute relevance classification
    \item[b)] Ranking eliminates user decision-making
    \item[c)] Selection depends on user-defined thresholds
    \item[d)] Ranking focuses on binary classification
\end{itemize}

\item Why is ranking often preferred over selection in document retrieval?
\begin{itemize}
    \item[a)] It simplifies query processing
    \item[b)] It allows users to decide cut-off points based on relevance
    \item[c)] It eliminates query ambiguity
    \item[d)] It is computationally less demanding
\end{itemize}

\item What is the purpose of scoring functions in ranking?
\begin{itemize}
    \item[a)] To classify documents into structured datasets
    \item[b)] To measure the probability of a document's relevance
    \item[c)] To analyze semantic relationships between terms
    \item[d)] To generate automatic summaries
\end{itemize}

\item Which strategy is commonly used in ranking-based systems?
\begin{itemize}
    \item[a)] Binary classification
    \item[b)] Probability-based ordering
    \item[c)] Manual document tagging
    \item[d)] Random document sampling
\end{itemize}

\item What assumption underpins probability ranking principles in retrieval?
\begin{itemize}
    \item[a)] Document relevance is independent of user feedback
    \item[b)] A user's utility for one document is independent of others
    \item[c)] Query complexity is inversely proportional to relevance
    \item[d)] Only exact matches are deemed relevant
\end{itemize}

\item What is one major challenge in combining \"push\" and \"pull\" modes?
\begin{itemize}
    \item[a)] Integrating structured and unstructured data
    \item[b)] Predicting long-term user needs
    \item[c)] Balancing system-driven and user-driven interactions
    \item[d)] Avoiding over-recommendation of irrelevant data
\end{itemize}

\item How does a multimodal system enhance text retrieval?
\begin{itemize}
    \item[a)] By prioritizing structured over unstructured data
    \item[b)] By seamlessly integrating query and navigation modes
    \item[c)] By automating relevance feedback mechanisms
    \item[d)] By filtering out incomplete datasets
\end{itemize}

\item Why is document classification more challenging than ranking?
\begin{itemize}
    \item[a)] It requires predefined thresholds for all queries
    \item[b)] It mandates real-time user interaction
    \item[c)] It depends on absolute measures of relevance
    \item[d)] It excludes probabilistic approaches
\end{itemize}

\item What makes ad hoc information retrieval particularly difficult?
\begin{itemize}
    \item[a)] Reliance on structured data inputs
    \item[b)] Lack of user feedback for query optimization
    \item[c)] Dependence on static datasets
    \item[d)] Overlap with database retrieval techniques
\end{itemize}

\item Which of the following improves user experience in retrieval systems?
\begin{itemize}
    \item[a)] Using longer keyword queries
    \item[b)] Enhancing thematic navigation tools
    \item[c)] Restricting access to unstructured data
    \item[d)] Avoiding multimodal interactions
\end{itemize}

\item In similarity-based retrieval models, the relevance of a document is determined by:
\begin{itemize}
    \item[a)] Its metadata attributes compared to other documents.
    \item[b)] The extent to which its content overlaps with the query terms.
    \item[c)] The probability distribution of terms across all documents.
    \item[d)] Its absolute frequency of term occurrences regardless of context.
\end{itemize}

\item Which of the following best describes the role of the binary random variable $R$ in probabilistic retrieval models?
\begin{itemize}
    \item[a)] It identifies whether a document is correctly indexed.
    \item[b)] It measures the overlap between documents and the query vector.
    \item[c)] It indicates whether a document is relevant to a given query.
    \item[d)] It calculates the similarity score between two documents.
\end{itemize}

\item What is the primary limitation of the classic probabilistic model when estimating relevance?
\begin{itemize}
    \item[a)] It assumes all query terms have equal importance.
    \item[b)] It requires precomputed similarity measures.
    \item[c)] It cannot handle unseen documents or queries.
    \item[d)] It assumes document lengths are always uniform.
\end{itemize}

\item Which retrieval model explicitly incorporates TF (Term Frequency) and IDF (Inverse Document Frequency) in its scoring?
\begin{itemize}
    \item[a)] Vector Space Model with pivoted length normalization.
    \item[b)] Query Likelihood Retrieval Model.
    \item[c)] Boolean Retrieval Model.
    \item[d)] Language Modeling with Dirichlet Smoothing.
\end{itemize}

\item In the context of vector space models, why is document length normalization crucial?
\begin{itemize}
    \item[a)] To ensure that rare terms are penalized in longer documents.
    \item[b)] To equalize the likelihood of term matches across documents of varying lengths.
    \item[c)] To reward longer documents for their higher information density.
    \item[d)] To prioritize documents containing all query terms regardless of length.
\end{itemize}

\item In the Vector Space Model, a document is represented as:
\begin{itemize}
    \item[a)] A sequence of terms and phrases in hierarchical order.
    \item[b)] A graph of term dependencies and their weights.
    \item[c)] A high-dimensional vector where each dimension corresponds to a term.
    \item[d)] A matrix of term frequencies and collection frequencies.
\end{itemize}

\item Why does the vector space model use a dot product to compute similarity between a document and a query?
\begin{itemize}
    \item[a)] It emphasizes the rare terms that occur across all documents.
    \item[b)] It measures the degree of overlap in the term weights of both vectors.
    \item[c)] It normalizes the effect of document length automatically.
    \item[d)] It reduces computational complexity by ignoring term frequency.
\end{itemize}

\item What challenge arises from representing documents using a bag-of-words approach in vector space models?
\begin{itemize}
    \item[a)] The inability to account for term co-occurrence within documents.
    \item[b)] Excessive reliance on document length normalization.
    \item[c)] Overemphasis on stop words due to their high IDF.
    \item[d)] Loss of term frequency information for longer documents.
\end{itemize}

\item How does TF-IDF weighting address the imbalance caused by frequently occurring terms like "the" or "and"?
\begin{itemize}
    \item[a)] It assigns a weight proportional to the absolute term frequency in a document.
    \item[b)] It penalizes common terms by assigning them a lower IDF score.
    \item[c)] It increases the document length to balance the frequency.
    \item[d)] It excludes stop words entirely from the vocabulary.
\end{itemize}

\item Which of the following represents a key improvement of the pivoted length normalization method over standard TF-IDF?
\begin{itemize}
    \item[a)] It uses sublinear transformation to adjust term frequency scores dynamically.
    \item[b)] It applies a binary weighting system for stop words.
    \item[c)] It adjusts the TF-IDF weight based on a document’s deviation from the average length.
    \item[d)] It excludes high-frequency terms from the document corpus entirely.
\end{itemize}

\item If two documents \(d_1\) and \(d_2\) have identical term frequencies but differ in length, which of the following will most likely occur in a well-normalized Vector Space Model?
\begin{itemize}
    \item[a)] Both documents will be ranked equally for any query.
    \item[b)] \(d_1\) will be ranked higher if it is shorter.
    \item[c)] \(d_2\) will be ranked higher if it is longer.
    \item[d)] Neither document will be ranked for queries containing rare terms.
\end{itemize}

\item In TF-IDF weighting, which term would likely have the highest contribution to document relevance?
\begin{itemize}
    \item[a)] A common term appearing in almost all documents.
    \item[b)] A moderately rare term with high frequency in a specific document.
    \item[c)] A term occurring equally across all documents.
    \item[d)] A term absent from the query vector.
\end{itemize}

\item The introduction of sublinear term frequency transformations (e.g., logarithmic scaling) in vector space models primarily aims to:
\begin{itemize}
    \item[a)] Prevent excessive penalization of infrequent terms.
    \item[b)] Adjust term weight contributions in long documents.
    \item[c)] Avoid dominance by terms that appear excessively within a single document.
    \item[d)] Reduce the computational complexity of similarity calculations.
\end{itemize}

\item How does document frequency (DF) influence the weight of a term in the TF-IDF model?
\begin{itemize}
    \item[a)] Higher DF values increase the importance of a term.
    \item[b)] Lower DF values lead to higher inverse document frequency (IDF) scores.
    \item[c)] DF directly determines the length of a document vector.
    \item[d)] DF scores are used to normalize term weights across queries.
\end{itemize}

\item What fundamental assumption underlies the use of cosine similarity in vector space models?
\begin{itemize}
    \item[a)] The vectors are normalized to unit length to account for document length differences.
    \item[b)] The angles between vectors are irrelevant in determining similarity.
    \item[c)] Only binary term presence contributes to similarity measures.
    \item[d)] All terms are weighted equally regardless of their importance.
\end{itemize}


\item What problem does inverse document frequency (IDF) aim to solve in retrieval models? 
\begin{itemize}
    \item[a)] Overemphasis on document length in similarity calculations. 
    \item[b)] Dominance of common terms that appear in most documents. 
    \item [c)] Insufficient weighting of query terms in long documents. 
    \item[d)] Redundancy caused by repeated terms in queries. 
\end{itemize} 
     

\item Which of the following best illustrates the issue with using raw term frequency (TF) without transformation in vector space models? 
\begin{itemize}
    \item[a)] Rare terms are assigned disproportionately high weights. \item[b)] Documents containing repetitive text may dominate rankings. 
    \item[c)] Query words that do not occur in any document are assigned a weight of zero. 
    \item[d)] Stop words are excluded automatically from the term vector. 
\end{itemize}  

\item How does the BM25 model address the diminishing returns of repeated term occurrences in a document?
\begin{itemize}
    \item[a)] By applying a logarithmic scaling to the term frequency
    \item[b)] By normalizing term frequency using document length
    \item[c)] By using an upper-bounded term frequency transformation function
    \item[d)] By excluding terms that occur more than a fixed number of times
\end{itemize}
 

\item Why might two documents with the same term frequencies receive different relevance scores in a retrieval model?
\begin{itemize}
    \item[a)] One document contains stop words that are ignored in ranking
    \item[b)] Differences in their lengths affect the normalization factor
    \item[c)] Their similarity is measured with respect to different queries
    \item[d)] Their term distributions across the corpus vary significantly
\end{itemize}
 

\item The key conceptual difference between vector space and probabilistic retrieval models is:
\begin{itemize}
    \item[a)] Vector space models rely on cosine similarity, while probabilistic models do not
    \item[b)] Probabilistic models rank documents based on the likelihood of relevance, not similarity
    \item[c)] Vector space models ignore document frequency, while probabilistic models depend on it
    \item[d)] Probabilistic models always require training data, whereas vector space models do not
\end{itemize}
 

\item In probabilistic retrieval models, the relevance of a document is expressed as:
\begin{itemize}
    \item[a)] A function of the similarity score between the document and query vectors
    \item[b)] A binary decision based on document length normalization
    \item[c)] A conditional probability given the document and query
    \item[d)] A sum of term weights across all matched terms
\end{itemize}
 

\item What assumption underlies the Query Likelihood Model in probabilistic retrieval?
\begin{itemize}
    \item[a)] Documents are equally likely to be relevant
    \item[b)] Queries are generated as samples from the language model of a document
    \item[c)] Users select documents based solely on cosine similarity
    \item[d)] Document length has no influence on term probabilities
\end{itemize}
 

\item What issue arises in the Query Likelihood Model if a query term does not appear in a document?
\begin{itemize}
    \item[a)] The document receives an overly high probability score
    \item[b)] The document cannot be ranked due to zero probability
    \item[c)] The term is ignored entirely in the scoring process
    \item[d)] The term contributes a penalty instead of a reward
\end{itemize}
 

\item How does smoothing address the zero-probability problem in the Query Likelihood Model?
\begin{itemize}
    \item[a)] By adding a small, non-zero probability to all terms in the document language model
    \item[b)] By excluding rare terms from the calculation entirely
    \item[c)] By normalizing term frequencies across the entire corpus
    \item[d)] By increasing the weight of terms that match query words
\end{itemize}
 

\item Which smoothing technique adjusts term probabilities using the collection-wide frequency of words?
\begin{itemize}
    \item[a)] Laplace Smoothing
    \item[b)] Dirichlet Prior Smoothing
    \item[c)] Pivoted Length Normalization
    \item[d)] Term Frequency Smoothing
\end{itemize}
 

\item What is the primary role of smoothing in probabilistic retrieval models?
\begin{itemize}
    \item[a)] To increase the weight of frequently occurring terms
    \item[b)] To account for unseen query terms in the document model
    \item[c)] To reduce the length normalization bias
    \item[d)] To penalize documents containing rare terms
\end{itemize}
 

\item How does the Dirichlet Prior Smoothing technique differ from Laplace Smoothing?
\begin{itemize}
    \item[a)] It applies a fixed probability to all unseen terms
    \item[b)] It considers both document-specific and collection-wide term frequencies
    \item[c)] It eliminates the need for term frequency normalization
    \item[d)] It assigns equal probabilities to all query terms, regardless of their frequency
\end{itemize}
 

\item In the Query Likelihood Model, a document will have a higher score if:
\begin{itemize}
    \item[a)] It contains more query terms with high collection frequency
    \item[b)] Its language model assigns high probabilities to the query terms
    \item[c)] Its length is significantly above the collection average
    \item[d)] It minimizes the number of rare terms included in its content
\end{itemize}
 

\item What key limitation does smoothing aim to address in the basic Query Likelihood Model?
\begin{itemize}
    \item[a)] Overemphasis on document length normalization
    \item[b)] Underrepresentation of stop words in ranking functions
    \item[c)] Zero probabilities assigned to documents lacking query terms
    \item[d)] Difficulty in ranking documents with high TF-IDF scores
\end{itemize}
 

\item How does the concept of a "document language model" contribute to retrieval in probabilistic models?
\begin{itemize}
    \item[a)] It represents the document as a distribution over terms for query generation
    \item[b)] It transforms the query vector into a probabilistic form
    \item[c)] It penalizes long documents based on term frequency
    \item[d)] It assigns a static weight to each document in the collection
\end{itemize}
 

\item Why is the independence assumption for query terms considered a simplification in probabilistic retrieval models?
\begin{itemize}
    \item[a)] It allows term weights to be directly proportional to document length
    \item[b)] It ignores term dependencies but simplifies the computation of probabilities
    \item[c)] It ensures that all terms are treated equally regardless of frequency
    \item[d)] It enables documents to be represented using binary vectors only
\end{itemize}
 

\item In language modeling for retrieval, the probability of a query given a document is calculated as:
\begin{itemize}
    \item[a)] A sum of term frequencies across all query terms in the document
    \item[b)] A product of probabilities for each query term, conditioned on the document
    \item[c)] A ratio of document length to average document length in the corpus
    \item[d)] A difference between query term frequency and document frequency
\end{itemize}
 

\item Which of the following is a key advantage of the Query Likelihood Model?
\begin{itemize}
    \item[a)] It directly incorporates cosine similarity for ranking
    \item[b)] It avoids explicit term weighting by relying on term probabilities
    \item[c)] It eliminates the need for document length normalization
    \item[d)] It penalizes documents containing rare terms
\end{itemize}
 

\item What factor primarily distinguishes the BM25 model from classical probabilistic models?
\begin{itemize}
    \item[a)] The use of a binary relevance assumption
    \item[b)] The incorporation of term frequency saturation and length normalization
    \item[c)] Its reliance on global term distributions rather than document-specific statistics
    \item[d)] The absence of inverse document frequency in scoring
\end{itemize}
 

\item How does smoothing with the Dirichlet prior achieve balance between document-specific and collection-wide statistics?
\begin{itemize}
    \item[a)] By adding a fixed value to the term frequency for every term in the document
    \item[b)] By adjusting term probabilities based on a prior estimate from the entire corpus
    \item[c)] By normalizing document length against the average document length
    \item[d)] By assigning equal weights to all terms regardless of their frequency
\end{itemize}
 

\item Which of the following best explains why BM25 is widely adopted in search engine implementations?
\begin{itemize}
    \item[a)] Its complexity allows fine-grained term analysis
    \item[b)] It combines TF, IDF, and length normalization effectively into a flexible scoring function
    \item[c)] It avoids the use of smoothing for unseen terms
    \item[d)] It directly models the semantic meaning of query terms
\end{itemize}
 

\item What issue does pivoted length normalization attempt to address that is not handled by basic TF-IDF weighting?
\begin{itemize}
    \item[a)] Bias towards documents containing rare terms
    \item[b)] Over-penalization of long documents during ranking
    \item[c)] Underrepresentation of common terms in short documents
    \item[d)] Zero probability of relevance for documents with unmatched query terms
\end{itemize}
 

\item Why might BM25+ outperform the original BM25 in certain retrieval scenarios?
\begin{itemize}
    \item[a)] It excludes document length normalization entirely
    \item[b)] It includes a small constant to reduce over-penalization of longer documents
    \item[c)] It prioritizes stop words to improve matching for natural language queries
    \item[d)] It replaces IDF with term co-occurrence probabilities
\end{itemize}
 

\item How does the concept of "term frequency saturation" improve retrieval functions like BM25?
\begin{itemize}
    \item[a)] It ensures a linear increase in relevance scores with term repetition
    \item[b)] It caps the contribution of a term to avoid dominance by excessive occurrences
    \item[c)] It balances the importance of rare and frequent terms uniformly
    \item[d)] It adjusts the weight of terms based on their positional importance
\end{itemize}
 

\item In a retrieval scenario using a language modeling approach, why might a query fail to retrieve relevant documents without smoothing?
\begin{itemize}
    \item[a)] The query terms might be assigned excessively high probabilities
    \item[b)] The language model does not differentiate between rare and common terms
    \item[c)] The probability of generating unseen query terms is assumed to be zero
    \item[d)] The document length normalization factor is omitted from the calculation
\end{itemize}
 

\item What is a common critique of bag-of-words representations in information retrieval?
\begin{itemize}
    \item[a)] They overemphasize rare terms by default
    \item[b)] They fail to capture the semantic relationships between terms
    \item[c)] They are computationally expensive compared to probabilistic models
    \item[d)] They require smoothing to handle missing query terms
\end{itemize}
 

\item How does the introduction of IDF weighting resolve a fundamental problem in the vector space model?
\begin{itemize}
    \item[a)] It ensures that longer documents are ranked higher
    \item[b)] It adjusts the weight of terms based on their specificity within the collection
    \item[c)] It prioritizes documents with higher term frequency across the entire corpus
    \item[d)] It guarantees that stop words are completely ignored in retrieval
\end{itemize}
 

\item What is the theoretical basis for combining term frequency (TF) and inverse document frequency (IDF) in retrieval models?
\begin{itemize}
    \item[a)] To reward documents with balanced lengths and moderate term frequencies
    \item[b)] To balance the importance of query terms based on their relevance and rarity
    \item[c)] To reduce computational complexity in similarity calculations
    \item[d)] To equalize the distribution of term weights across the query and document vectors
\end{itemize}
 

\item Which of the following scenarios is most likely to benefit from the use of BM25-F?
\begin{itemize}
    \item[a)] Retrieving documents based on structured fields like title and abstract
    \item[b)] Ranking unstructured text documents by cosine similarity
    \item[c)] Identifying semantic relationships in queries with synonyms
    \item[d)] Scoring documents without smoothing for unseen terms
\end{itemize}
 

\item Why is the choice of smoothing parameter critical in the Query Likelihood Model?
\begin{itemize}
    \item[a)] It controls the weight of rare terms in long documents
    \item[b)] It determines the influence of collection-wide statistics on document scores
    \item[c)] It ensures zero probabilities are assigned to irrelevant documents
    \item[d)] It balances the TF-IDF scores across different document fields
\end{itemize}

% Preguntas
\item What is the primary task of a tokenizer in an IR system?
\begin{itemize}
\item[a)] Compressing data
\item[b)] Splitting documents into countable features or tokens
\item[c)] Assigning weights to tokens
\item[d)] Sorting tokens by frequency
\end{itemize}

\item Which of the following represents a common tokenization strategy?
\begin{itemize}
\item[a)] Stop word normalization
\item[b)] Forward indexing
\item[c)] Whitespace-based tokenization
\item[d)] Term-at-a-time ranking
\end{itemize}

\item Why is it efficient to use term IDs instead of string terms in tokenization?
\begin{itemize}
\item[a)] Term IDs are easier to generate
\item[b)] Term IDs save memory and allow O(1) lookups
\item[c)] Term IDs enable stop-word filtering
\item[d)] Term IDs increase document count accuracy
\end{itemize}

\item In tokenization, what does the term "feature generation" imply?
\begin{itemize}
\item[a)] Generating unique document identifiers
\item[b)] Defining the building blocks of document objects
\item[c)] Calculating term frequency weights
\item[d)] Filtering irrelevant documents
\end{itemize}

\item How does a whitespace tokenizer treat the sentence "Data is key"?
\begin{itemize}
\item[a)] {Data: 1, is: 1, key: 1}
\item[b)] {Data: 1, Key: 1}
\item[c)] {data: 1, is: 1, key: 1}
\item[d)] {Data: 1, Is: 1, Key: 1}
\end{itemize}

\item Which is NOT a key feature of the inverted index?
\begin{itemize}
\item[a)] Lexicon
\item[b)] Forward index
\item[c)] Postings file
\item[d)] Document frequency
\end{itemize}

\item What is the primary role of the lexicon in an inverted index?
\begin{itemize}
\item[a)] Mapping documents to terms
\item[b)] Storing offsets to postings file entries
\item[c)] Calculating term weights
\item[d)] Compressing document IDs
\end{itemize}

\item What is stored in the postings file of an inverted index?
\begin{itemize}
\item[a)] Term frequency scores only
\item[b)] Positions of terms within documents
\item[c)] Document metadata
\item[d)] All documents with zero scores
\end{itemize}

\item In indexing, what is the significance of merging runs?
\begin{itemize}
\item[a)] To improve tokenization efficiency
\item[b)] To create a single sorted postings file
\item[c)] To calculate document weights
\item[d)] To map term IDs to lexicon entries
\end{itemize}

\item What happens during the creation of a forward index?
\begin{itemize}
\item[a)] Terms are mapped to documents
\item[b)] Documents are mapped to term lists
\item[c)] Lexicons are compressed for fast retrieval
\item[d)] Token frequencies are smoothed
\end{itemize}

\item Why is term-at-a-time ranking preferred in many IR systems?
\begin{itemize}
\item[a)] It scores only documents with non-zero scores
\item[b)] It requires no accumulators
\item[c)] It scores all documents regardless of relevance
\item[d)] It avoids stop-word removal
\end{itemize}

\item Which scoring algorithm uses a priority queue to maintain top-k documents?
\begin{itemize}
\item[a)] Term-at-a-time
\item[b)] Document-at-a-time
\item[c)] BM25 scoring
\item[d)] Proximity ranking
\end{itemize}

\item How does a scorer utilize the inverted index?
\begin{itemize}
\item[a)] By retrieving document metadata
\item[b)] By scanning the forward index
\item[c)] By scoring only documents with query terms
\item[d)] By reducing storage requirements
\end{itemize}

\item What is the purpose of filtering in document ranking?
\begin{itemize}
\item[a)] To update score accumulators
\item[b)] To skip irrelevant documents based on metadata
\item[c)] To reduce the lexicon size
\item[d)] To prioritize proximity-based matches
\end{itemize}

\item In scoring algorithms, what is typically used to measure term relevance?
\begin{itemize}
\item[a)] Term frequency
\item[b)] Term position
\item[c)] Document length
\item[d)] IDF weighting
\end{itemize}

\item Why is compression critical in search engine implementations?
\begin{itemize}
\item[a)] To improve tokenization accuracy
\item[b)] To reduce memory usage and increase read efficiency
\item[c)] To simplify query processing
\item[d)] To support index sharding
\end{itemize}

\item How does caching improve query performance?
\begin{itemize}
\item[a)] By precomputing scores for all documents
\item[b)] By storing frequently accessed postings in memory
\item[c)] By reducing the size of the lexicon
\item[d)] By removing irrelevant documents
\end{itemize}

\item Which caching strategy follows Zipf's law?
\begin{itemize}
\item[a)] Term-at-a-time caching
\item[b)] Document metadata caching
\item[c)] Least Recently Used (LRU)
\item[d)] Priority-based caching
\end{itemize}

\item What is index sharding?
\begin{itemize}
\item[a)] Dividing the index into smaller sections
\item[b)] Merging multiple postings files
\item[c)] Storing lexicons in memory
\item[d)] Compressing document IDs
\end{itemize}

\item How is a shard typically assigned in a distributed search engine?
\begin{itemize}
\item[a)] By document frequency
\item[b)] By term positions
\item[c)] By the number of nodes or threads
\item[d)] By lexicon size
\end{itemize}

\item What is the main advantage of using a postings file in an inverted index?
\begin{itemize}
\item[a)] It improves the storage format for metadata
\item[b)] It enables quick lookup of documents containing specific terms
\item[c)] It reduces the need for document clustering
\item[d)] It simplifies query tokenization
\end{itemize}

\item Why is it essential for the postings file to store term positions within documents?
\begin{itemize}
\item[a)] To track the term frequency across the corpus
\item[b)] To implement proximity-based heuristics for phrase matching
\item[c)] To remove irrelevant terms during indexing
\item[d)] To simplify lexicon creation
\end{itemize}

\item Which of the following is a critical design consideration for the forward index?
\begin{itemize}
\item[a)] Mapping terms to their positions in the postings file
\item[b)] Allowing efficient access to all terms in a specific document
\item[c)] Reducing the size of the lexicon stored in memory
\item[d)] Supporting stop-word removal during scoring
\end{itemize}

\item How does index sharding optimize query performance?
\begin{itemize}
\item[a)] By ensuring all terms are stored in a single index
\item[b)] By parallelizing search tasks across multiple shards
\item[c)] By merging postings files during query processing
\item[d)] By caching frequently accessed terms
\end{itemize}

\item Which step in the indexing process ensures efficient term lookup for queries?
\begin{itemize}
\item[a)] Assigning unique term IDs during tokenization
\item[b)] Sorting terms by frequency in the lexicon
\item[c)] Merging intermediate runs into a single postings file
\item[d)] Removing duplicates from the forward index
\end{itemize}


\item Which scoring model is most associated with term frequency-inverse document frequency (TF-IDF)?
\begin{itemize}
\item[a)] Okapi BM25
\item[b)] Vector Space Model
\item[c)] Boolean Retrieval Model
\item[d)] Proximity-Based Ranking
\end{itemize}

\item What is the key difference between term-at-a-time and document-at-a-time ranking?
\begin{itemize}
\item[a)] The use of lexicons in scoring
\item[b)] The method of updating score accumulators
\item[c)] The requirement for compression in lexicon files
\item[d)] The inclusion of metadata in ranking scores
\end{itemize}

\item Why is it unnecessary to score documents with zero-term matches during ranking?
\begin{itemize}
\item[a)] Their scores will always be zero
\item[b)] They contain no relevant metadata
\item[c)] They cannot be cached effectively
\item[d)] They are not stored in the forward index
\end{itemize}

\item In document-at-a-time ranking, what data structure is used to maintain the top-k documents?
\begin{itemize}
\item[a)] Linked list
\item[b)] Priority queue
\item[c)] Hash map
\item[d)] Bloom filter
\end{itemize}

\item Which ranking strategy minimizes memory usage for score accumulators?
\begin{itemize}
\item[a)] Term-at-a-time ranking
\item[b)] Document-at-a-time ranking
\item[c)] Filtering-based ranking
\item[d)] BM25 with priority queue
\end{itemize}

\item Why is the lexicon typically stored in memory?
\begin{itemize}
\item[a)] To allow fast lookup of term-to-document mappings
\item[b)] To ensure all query terms are scored
\item[c)] To avoid compression overhead during scoring
\item[d)] To cache frequently used documents
\end{itemize}

\item How does compression of the postings file improve query performance?
\begin{itemize}
\item[a)] By reducing the size of the lexicon
\item[b)] By decreasing disk seek times for term lookups
\item[c)] By optimizing document scoring algorithms
\item[d)] By caching term IDs in memory
\end{itemize}

\item Which strategy ensures that the cache stores only the most useful terms?
\begin{itemize}
\item[a)] Randomized eviction
\item[b)] Least Recently Used (LRU) policy
\item[c)] Term frequency normalization
\item[d)] Proximity-based filtering
\end{itemize}

\item What is the role of filters in the scoring process?
\begin{itemize}
\item[a)] To prioritize documents with higher term frequencies
\item[b)] To exclude documents that do not meet specific criteria
\item[c)] To compress the postings file for faster lookups
\item[d)] To limit the number of terms in the query
\end{itemize}

\item What is the purpose of a priority queue in query processing?
\begin{itemize}
\item[a)] To store all document scores temporarily
\item[b)] To maintain a list of the k most relevant documents
\item[c)] To ensure efficient term lookups in the lexicon
\item[d)] To compress term positions within documents
\end{itemize}

\item In the context of search engines, what does "relevance feedback" typically involve?
\begin{itemize}
\item[a)] Adjusting term weights based on user interaction data
\item[b)] Compressing the lexicon for faster retrieval
\item[c)] Removing irrelevant documents from the index
\item[d)] Updating stop-word lists for tokenization
\end{itemize}

\item Why is document filtering commonly applied before scoring in IR systems?
\begin{itemize}
\item[a)] To reduce the size of the lexicon
\item[b)] To exclude documents that do not satisfy user-defined constraints
\item[c)] To prioritize terms with high frequency
\item[d)] To enable caching of frequently accessed documents
\end{itemize}

\item Which method is commonly used to merge multiple partial inverted indexes during indexing?
\begin{itemize}
\item[a)] Binary search
\item[b)] Merge sort
\item[c)] Quick sort
\item[d)] Hash mapping
\end{itemize}

\item What is the primary advantage of using document metadata in ranking?
\begin{itemize}
\item[a)] It allows faster compression of postings files
\item[b)] It enables advanced filters such as date or document type constraints
\item[c)] It reduces the need for proximity heuristics
\item[d)] It simplifies the process of assigning term IDs
\end{itemize}

\item Which ranking method optimizes query time by processing documents in their shard locations?
\begin{itemize}
\item[a)] Document-at-a-time ranking
\item[b)] Term-at-a-time ranking
\item[c)] Distributed ranking with index sharding
\item[d)] BM25 ranking with LRU caching
\end{itemize}

\item What is the primary purpose of search engine evaluation?
\begin{itemize}
\item[a)] To design new algorithms
\item[b)] To compare retrieval methods and assess their utility
\item[c)] To increase computational efficiency
\item[d)] To refine user query formulations
\end{itemize}

\item What are the three main dimensions of search engine evaluation?
\begin{itemize}
\item[a)] Precision, Recall, Relevance
\item[b)] Effectiveness, Usability, Efficiency
\item[c)] Accuracy, Reliability, Scalability
\item[d)] Ranking, Indexing, Filtering
\end{itemize}

\item Which evaluation methodology is central to modern search engine testing?
\begin{itemize}
\item[a)] A-B testing
\item[b)] Pooling
\item[c)] Cranfield evaluation methodology
\item[d)] Mean Reciprocal Rank testing
\end{itemize}

\item In the Cranfield methodology, what is a critical component for fair comparison?
\begin{itemize}
\item[a)] A unique query set for each method
\item[b)] Relevance judgments consistent across systems
\item[c)] Randomized query sampling
\item[d)] The inclusion of user feedback during testing
\end{itemize}

\item How is relevance defined in binary judgments?
\begin{itemize}
\item[a)] By the frequency of terms in a document
\item[b)] As either relevant or non-relevant for a specific query
\item[c)] By the user's time spent on a document
\item[d)] As marginally or highly relevant
\end{itemize}

\item What is the main limitation of using binary relevance judgments?
\begin{itemize}
\item[a)] They are computationally expensive
\item[b)] They oversimplify the relevance spectrum
\item[c)] They do not account for system efficiency
\item[d)] They cannot be reused across queries
\end{itemize}

\item Which of the following is NOT a benefit of Cranfield methodology?
\begin{itemize}
\item[a)] Reusability of test collections
\item[b)] Objectivity in algorithm comparisons
\item[c)] Real-time user interaction during testing
\item[d)] Compatibility with various retrieval systems
\end{itemize}

\item In the context of evaluation, what are relevance judgments?
\begin{itemize}
\item[a)] Binary ratings assigned by system administrators
\item[b)] User-assigned labels indicating document utility
\item[c)] Automatically generated labels from test collections
\item[d)] Annotations based on computational heuristics
\end{itemize}

\item Why is user involvement critical in search engine evaluation?
\begin{itemize}
\item[a)] To determine query efficiency
\item[b)] To establish relevance judgments
\item[c)] To perform system ranking
\item[d)] To compute MAP scores
\end{itemize}

\item What distinguishes search engine evaluation from database evaluation?
\begin{itemize}
\item[a)] The need for computational efficiency
\item[b)] The emphasis on user feedback and ranking
\item[c)] The reliance on binary classification
\item[d)] The focus on query optimization
\end{itemize}

\item Precision measures...
\begin{itemize}
\item[a)] The proportion of retrieved documents that are relevant
\item[b)] The number of relevant documents retrieved out of the total
\item[c)] The time taken to retrieve documents
\item[d)] The average relevance score of retrieved documents
\end{itemize}

\item Recall measures...
\begin{itemize}
\item[a)] The total number of documents retrieved
\item[b)] The proportion of relevant documents retrieved
\item[c)] The completeness of the query processing
\item[d)] The ranking accuracy of the system
\end{itemize}

\item What is the ideal precision and recall value for a system?
\begin{itemize}
\item[a)] 1.0 for precision, 0.5 for recall
\item[b)] Both should be close to 0.8
\item[c)] Both should be 1.0
\item[d)] 0.5 for precision, 1.0 for recall
\end{itemize}

\item The F1 score combines...
\begin{itemize}
\item[a)] Precision and ranking
\item[b)] Recall and ranking
\item[c)] Precision and recall
\item[d)] Precision and user interaction
\end{itemize}

\item Why is F1 score preferred over arithmetic mean of precision and recall?
\begin{itemize}
\item[a)] It emphasizes high recall values
\item[b)] It balances precision and recall effectively
\item[c)] It simplifies evaluation metrics
\item[d)] It penalizes algorithms with low recall
\end{itemize}

\item What is the formula for F1 score?
\begin{itemize}
    \item [a)] $F1=2\cdot \frac{\text{Precision}\cdot \text{Recall}}{\text{Precision} + \text{Recall}}$ 
    \item [b)] $F1=\frac{2\cdot \text{Precision}\cdot \text{Recall}}{\text{Precision} + \text{Recall}}$
    \item [c)] $F1 = \frac{\text{Precision} \cdot \text{Recall}}{2}$
    \item [d)] $F1 = \frac{\text{Recall}}{\text{Precision}}$
\end{itemize}

\item Why is high recall often associated with low precision?
\begin{itemize}
\item[a)] Systems retrieve too many irrelevant documents
\item[b)] User interactions reduce the ranking accuracy
\item[c)] Systems cannot filter non-relevant documents efficiently
\item[d)] Query terms increase the lexicon size
\end{itemize}

\item What does precision\@k represent?
\begin{itemize}
\item[a)] The percentage of queries answered correctly within k seconds
\item[b)] The proportion of relevant documents in the top-k results
\item[c)] The average precision for k queries
\item[d)] The relevance score weighted by k factors
\end{itemize}

\item Why is F1 score considered a harmonic mean?
\begin{itemize}
\item[a)] It averages precision and recall without bias
\item[b)] It smoothens variance in precision-recall tradeoff
\item[c)] It penalizes extreme values of precision or recall
\item[d)] It prioritizes efficiency over accuracy
\end{itemize}

\item A system with perfect recall but low precision would likely...
\begin{itemize}
\item[a)] Return only a subset of relevant documents
\item[b)] Retrieve all relevant and many irrelevant documents
\item[c)] Exclude some relevant documents
\item[d)] Return documents in random order
\end{itemize}

\item Why is it important to evaluate ranked lists in search engine evaluation?
\begin{itemize}
\item[a)] To ensure all documents are retrieved
\item[b)] To consider the position of relevant documents
\item[c)] To remove irrelevant documents from the corpus
\item[d)] To optimize query speed
\end{itemize}

\item How is precision-recall (PR) curve generated?
\begin{itemize}
\item[a)] By plotting precision at every rank of the retrieved list
\item[b)] By averaging the number of retrieved documents
\item[c)] By calculating the F1 score across all queries
\item[d)] By plotting the ranking of irrelevant documents
\end{itemize}

\item What does MAP (Mean Average Precision) measure?
\begin{itemize}
\item[a)] Average precision across multiple queries
\item[b)] Average recall across all ranked lists
\item[c)] Precision for the top k documents
\item[d)] Total number of relevant documents in the corpus
\end{itemize}

\item Why is MAP preferred over simple precision for comparing algorithms?
\begin{itemize}
\item[a)] It considers the rank of every relevant document
\item[b)] It penalizes irrelevant documents in the corpus
\item[c)] It ignores user preferences
\item[d)] It is computationally easier to calculate
\end{itemize}

\item What is the significance of a high MAP score?
\begin{itemize}
\item[a)] It indicates the system returns fewer documents
\item[b)] It means relevant documents are consistently ranked high
\item[c)] It ensures that recall is always greater than precision
\item[d)] It represents fewer false positives in ranking
\end{itemize}

\item In MAP calculation, what does the denominator in the precision formula represent?
\begin{itemize}
\item[a)] Total number of retrieved documents
\item[b)] Total number of relevant documents in the collection
\item[c)] Number of relevant documents retrieved so far
\item[d)] Total number of documents in the query
\end{itemize}

\item How does gMAP differ from MAP?
\begin{itemize}
\item[a)] gMAP emphasizes easy queries over difficult ones
\item[b)] gMAP penalizes systems with low performance on certain queries
\item[c)] gMAP averages precision over fewer queries
\item[d)] gMAP does not consider ranking positions
\end{itemize}

\item What is the reciprocal rank in known-item search?
\begin{itemize}
\item[a)] The inverse of the rank of the first relevant document
\item[b)] The average rank of all retrieved documents
\item[c)] The position of the least relevant document
\item[d)] The harmonic mean of all ranks
\end{itemize}

\item What is the Mean Reciprocal Rank (MRR) used for?
\begin{itemize}
\item[a)] Evaluating binary relevance judgments
\item[b)] Comparing systems with high MAP scores
\item[c)] Measuring efficiency in multi-level judgments
\item[d)] Assessing performance in single-relevant-item tasks
\end{itemize}

\item How does average precision differ from precision@k?
\begin{itemize}
\item[a)] Average precision considers all retrieved documents, while precision@k focuses on the top k results
\item[b)] Precision@k evaluates ranking position, but average precision does not
\item[c)] Average precision is used for multi-level judgments, while precision@k is for binary judgments
\item[d)] They are calculated identically, but for different queries
\end{itemize}

\item What is the purpose of Normalized Discounted Cumulative Gain (NDCG)?
\begin{itemize}
\item[a)] To measure system efficiency in user studies
\item[b)] To evaluate ranked lists with multiple levels of relevance
\item[c)] To optimize retrieval for binary relevance judgments
\item[d)] To assess recall in large datasets
\end{itemize}

\item In NDCG, why are gains discounted by their rank?
\begin{itemize}
\item[a)] To emphasize higher-ranked documents more than lower-ranked ones
\item[b)] To reduce computational complexity
\item[c)] To adjust for binary relevance scores
\item[d)] To improve retrieval speed
\end{itemize}

\item How is the ideal DCG (IDCG) defined for a query?
\begin{itemize}
\item[a)] The DCG of a randomly ordered ranked list
\item[b)] The DCG of a perfectly ranked list of relevant documents
\item[c)] The total gain of the top k documents in any order
\item[d)] The harmonic mean of precision and recall scores
\end{itemize}

\item What does a higher NDCG score indicate?
\begin{itemize}
\item[a)] The system retrieves fewer irrelevant documents
\item[b)] Relevant documents are ranked closer to the top
\item[c)] The system uses fewer computational resources
\item[d)] Precision increases at the expense of recall
\end{itemize}

\item What is the primary benefit of NDCG over MAP?
\begin{itemize}
\item[a)] It handles non-binary relevance judgments
\item[b)] It simplifies ranking calculations
\item[c)] It focuses only on precision@10
\item[d)] It measures efficiency instead of accuracy
\end{itemize}

\item How are relevance levels used in calculating DCG?
\begin{itemize}
\item[a)] They are multiplied by their rank
\item[b)] They are discounted logarithmically by their position
\item[c)] They are converted to binary values before summation
\item[d)] They are averaged over all queries
\end{itemize}

\item Why is normalization important in NDCG?
\begin{itemize}
\item[a)] To scale scores between 0 and 1 for comparison across queries
\item[b)] To remove bias caused by query length
\item[c)] To adjust for variations in corpus size
\item[d)] To simplify multi-level judgments
\end{itemize}

\item What is a potential drawback of NDCG?
\begin{itemize}
\item[a)] It is insensitive to document order
\item[b)] It cannot be applied to binary relevance judgments
\item[c)] It relies heavily on manual relevance judgments
\item[d)] It ignores the top k documents in the ranking
\end{itemize}

\item What is the role of pooling in test collection evaluation?
\begin{itemize}
\item[a)] To evaluate multi-level judgments more efficiently
\item[b)] To combine top-k results from multiple systems for judgment
\item[c)] To automate the labeling of non-relevant documents
\item[d)] To speed up relevance judgment calculations
\end{itemize}

\item Why are unjudged documents in the pool assumed to be non-relevant?
\begin{itemize}
\item[a)] To reduce human effort in relevance judgment
\item[b)] To simplify ranking algorithms
\item[c)] To minimize computational overhead
\item[d)] To improve the F1 score
\end{itemize}

\item What is the primary advantage of dense word embeddings over one-hot encoding?
\begin{itemize}
\item[a)] They improve the efficiency of hardware computation
\item[b)] They capture semantic relationships between words
\item[c)] They require fewer parameters to train neural networks
\item[d)] They eliminate the need for labeled datasets
\end{itemize}

\item What hypothesis underpins the development of word embeddings?
\begin{itemize}
\item[a)] Latent Semantic Hypothesis
\item[b)] Distributional Hypothesis
\item[c)] Frequency Hypothesis
\item[d)] Neural Transformation Hypothesis
\end{itemize}

\item How do classical vectorial representations represent words in text?
\begin{itemize}
\item[a)] As dense embeddings trained using neural networks
\item[b)] As binary or weighted vectors in a term-document matrix
\item[c)] As sequences of characters with frequency counts
\item[d)] As graph-based nodes connected by edges
\end{itemize}

\item What is a major limitation of one-hot encoding?
\begin{itemize}
\item[a)] It requires pre-trained embeddings
\item[b)] It does not encode semantic similarity between words
\item[c)] It cannot represent sparse data efficiently
\item[d)] It cannot handle high-dimensional vectors
\end{itemize}

\item What is the "curse of dimensionality" in one-hot encoding?
\begin{itemize}
\item[a)] Data sparsity caused by high-dimensional vectors
\item[b)] The inability to train deep learning models efficiently
\item[c)] Overfitting caused by high variance in data
\item[d)] Lack of representation for unknown words
\end{itemize}

\item What mathematical operation is often used to measure similarity in classical vectorial models?
\begin{itemize}
\item[a)] Dot product
\item[b)] Euclidean distance
\item[c)] Cosine similarity
\item[d)] Matrix multiplication
\end{itemize}

\item How do word embeddings improve upon classical models?
\begin{itemize}
\item[a)] By using supervised learning on labeled datasets
\item[b)] By leveraging co-occurrence statistics in dense spaces
\item[c)] By optimizing sparse matrix multiplications
\item[d)] By replacing one-hot encoding with binary representations
\end{itemize}

\item Which term describes embeddings that capture both syntactic and semantic regularities?
\begin{itemize}
\item[a)] Feature-based embeddings
\item[b)] Fixed-length distributed representations
\item[c)] Probabilistic latent representations
\item[d)] Sparse vector embeddings
\end{itemize}

\item In word embeddings, what does the equation "king + woman - man $=$ queen" illustrate?
\begin{itemize}
\item[a)] One-hot encoding
\item[b)] Distributional semantics
\item[c)] Arithmetic on embedding vectors
\item[d)] Semantic anomaly detection
\end{itemize}

\item What problem does FastText address that Word2Vec struggles with?
\begin{itemize}
\item[a)] Handling large vocabularies
\item[b)] Understanding subword-level semantics
\item[c)] Reducing training time for embeddings
\item[d)] Capturing context-sensitive representations
\end{itemize}

\item Which model introduced neural network-based embeddings?
\begin{itemize}
\item[a)] Skip-Gram
\item[b)] CBOW
\item[c)] NNLM (Bengio et al. 2003)
\item[d)] Glove
\end{itemize}

\item What is the main difference between Skip-Gram and CBOW in Word2Vec?
\begin{itemize}
\item[a)] Skip-Gram predicts the center word from context words, while CBOW does the reverse
\item[b)] CBOW uses large corpora, while Skip-Gram uses small datasets
\item[c)] Skip-Gram is faster to train than CBOW
\item[d)] CBOW captures word positions, while Skip-Gram ignores them
\end{itemize}

\item What optimization method is commonly used in Word2Vec to improve efficiency?
\begin{itemize}
\item[a)] Hierarchical Softmax
\item[b)] Negative Sampling
\item[c)] Both a and b
\item[d)] Backpropagation
\end{itemize}

\item How does FastText improve over Word2Vec for rare words?
\begin{itemize}
\item[a)] By encoding characters instead of words
\item[b)] By using n-gram embeddings
\item[c)] By normalizing co-occurrence counts
\item[d)] By extending the context window
\end{itemize}

\item What distinguishes ELMo embeddings from Word2Vec?
\begin{itemize}
\item[a)] ELMo produces context-dependent embeddings
\item[b)] ELMo uses only CBOW architecture
\item[c)] ELMo ignores sentence-level context
\item[d)] ELMo embeddings are static
\end{itemize}

\item What is the defining feature of Transformer-based models like BERT?
\begin{itemize}
\item[a)] Attention mechanisms
\item[b)] N-gram modeling
\item[c)] Sparse embeddings
\item[d)] Bi-directional RNNs
\end{itemize}

\item In Transformers, what does "self-attention" achieve?
\begin{itemize}
\item[a)] It computes relationships between words in the context
\item[b)] It reduces training complexity in embeddings
\item[c)] It eliminates the need for tokenization
\item[d)] It focuses on sentence-level classification
\end{itemize}

\item What type of tasks can BERT embeddings handle well?
\begin{itemize}
\item[a)] Sequence generation tasks
\item[b)] Semantic search and information retrieval
\item[c)] Machine translation
\item[d)] Graph-based learning
\end{itemize}

\item How does GPT differ fundamentally from BERT?
\begin{itemize}
\item[a)] GPT uses only encoders, while BERT uses decoders
\item[b)] GPT is unidirectional, while BERT is bidirectional
\item[c)] GPT uses fine-tuning, while BERT does not
\item[d)] GPT models context through n-grams
\end{itemize}

\item Which model uses both encoder and decoder structures?
\begin{itemize}
\item[a)] GPT
\item[b)] BERT
\item[c)] T5
\item[d)] CBOW
\end{itemize}

\item What innovation is central to the GloVe model?
\begin{itemize}
\item[a)] Using word-context matrices for embeddings
\item[b)] Ratios of word co-occurrence probabilities
\item[c)] Replacing Softmax with hierarchical layers
\item[d)] Applying subword embeddings for rare words
\end{itemize}

\item What is the purpose of masked language modeling in BERT?
\begin{itemize}
\item[a)] To predict missing words in a sentence
\item[b)] To calculate co-occurrence probabilities
\item[c)] To encode positional relationships between words
\item[d)] To reduce training time
\end{itemize}

\item What challenge is addressed by subword-level embeddings in FastText?
\begin{itemize}
\item[a)] Handling out-of-vocabulary (OOV) words
\item[b)] Increasing context window size
\item[c)] Learning embeddings faster
\item[d)] Improving batch training
\end{itemize}

\item What is a key feature of contextual embeddings like ELMo?
\begin{itemize}
\item[a)] They assign the same embedding to a word regardless of its context
\item[b)] They adapt embeddings based on the word's context in a sentence
\item[c)] They use binary vectors for efficiency
\item[d)] They rely on fixed-length dense vectors
\end{itemize}

\item Why are Transformers considered cutting-edge in NLP?
\begin{itemize}
\item[a)] They rely on n-gram models
\item[b)] They eliminate sequential processing with attention mechanisms
\item[c)] They focus only on text classification tasks
\item[d)] They require no pre-training
\end{itemize}

\item How does T5 (Text-to-Text Transfer Transformer) differ from BERT and GPT?
\begin{itemize}
\item[a)] It uses an encoder-only architecture
\item[b)] It treats every NLP task as a sequence-to-sequence problem
\item[c)] It relies on character-level embeddings for better accuracy
\item[d)] It generates embeddings without training
\end{itemize}

\item What is the main limitation of Word2Vec with respect to out-of-vocabulary (OOV) words?
\begin{itemize}
\item[a)] It cannot generate embeddings for words not in the training corpus
\item[b)] It requires significant computational resources
\item[c)] It produces context-insensitive embeddings
\item[d)] It fails to capture semantic relationships
\end{itemize}

\item In FastText, how are embeddings generated for OOV words?
\begin{itemize}
\item[a)] By averaging the embeddings of similar words
\item[b)] By constructing embeddings from n-grams of the word
\item[c)] By using a random initialization followed by fine-tuning
\item[d)] By ignoring OOV words entirely
\end{itemize}

\item What distinguishes context-specific embeddings from static embeddings?
\begin{itemize}
\item[a)] Context-specific embeddings adjust based on surrounding words
\item[b)] Static embeddings change with fine-tuning
\item[c)] Context-specific embeddings use binary representations
\item[d)] Static embeddings are computationally faster to train
\end{itemize}

\item What is a practical application of BERT in information retrieval?
\begin{itemize}
\item[a)] Generating summaries for long documents
\item[b)] Matching queries to relevant documents using semantic meaning
\item[c)] Predicting the next sentence in a paragraph
\item[d)] Translating documents between languages
\end{itemize}

\item What is a "masked token" in the context of BERT pre-training?
\begin{itemize}
\item[a)] A placeholder used to predict missing words during training
\item[b)] A token that represents rare words in the dataset
\item[c)] A token excluded from the training corpus
\item[d)] A token that indicates sentence boundaries
\end{itemize}

\item How do attention mechanisms improve NLP models?
\begin{itemize}
\item[a)] By reducing the computational cost of matrix operations
\item[b)] By focusing on relevant parts of the input sequence
\item[c)] By replacing embeddings with binary representations
\item[d)] By optimizing word similarity measures
\end{itemize}

\item What is the role of "self-attention" in Transformers?
\begin{itemize}
\item[a)] It eliminates dependencies on labeled data
\item[b)] It encodes relationships between words in the same input sequence
\item[c)] It maps embeddings to a hierarchical structure
\item[d)] It computes probabilities for masked tokens
\end{itemize}

\item Which property of embeddings enables analogical reasoning (e.g., "king + woman - man $=$ queen")?
\begin{itemize}
\item[a)] High-dimensionality
\item[b)] Semantic regularities in vector space
\item[c)] Syntactic dependencies
\item[d)] Contextual alignment of embeddings
\end{itemize}

\item What is the main advantage of using GloVe over Word2Vec?
\begin{itemize}
\item[a)] GloVe combines global co-occurrence statistics with local context
\item[b)] GloVe requires less training data than Word2Vec
\item[c)] GloVe embeddings are better for subword-level tasks
\item[d)] GloVe supports unsupervised learning natively
\end{itemize}

\item How does ELMo produce context-dependent embeddings?
\begin{itemize}
\item[a)] By using bidirectional LSTM-based language models
\item[b)] By training on fixed-length n-grams
\item[c)] By employing hierarchical Softmax
\item[d)] By reducing dimensionality of one-hot vectors
\end{itemize}

\item Why is the Transformer architecture considered scalable?
\begin{itemize}
\item[a)] It uses fixed-size embeddings for all datasets
\item[b)] It processes input sequences in parallel using attention
\item[c)] It eliminates the need for recurrent connections
\item[d)] It is optimized for small-scale tasks
\end{itemize}

\item What is the purpose of positional encoding in Transformers?
\begin{itemize}
\item[a)] To assign unique IDs to tokens
\item[b)] To encode the order of words in a sequence
\item[c)] To train embeddings faster
\item[d)] To replace attention mechanisms
\end{itemize}

\item Which Transformer model is used primarily for text generation tasks?
\begin{itemize}
\item[a)] BERT
\item[b)] GPT
\item[c)] T5
\item[d)] FastText
\end{itemize}

\item What improvement does the T5 model offer over BERT and GPT?
\begin{itemize}
\item[a)] It uses both encoder and decoder structures for sequence-to-sequence tasks
\item[b)] It eliminates the need for fine-tuning
\item[c)] It reduces the training data required for embeddings
\item[d)] It focuses only on text classification tasks
\end{itemize}


\item What is a key challenge with embeddings when dealing with OOV words?
\begin{itemize}
\item[a)] They require labeled training data for embeddings
\item[b)] They cannot represent words not seen during training
\item[c)] They increase the dimensionality of embeddings
\item[d)] They reduce the semantic accuracy of embeddings
\end{itemize}

\item How does FastText address the OOV problem?
\begin{itemize}
\item[a)] By breaking words into subword units
\item[b)] By leveraging co-occurrence counts
\item[c)] By pre-training on larger corpora
\item[d)] By interpolating missing embeddings
\end{itemize}

\item What is a significant drawback of count-based models like LSA?
\begin{itemize}
\item[a)] They cannot capture semantic regularities
\item[b)] They lack scalability for large vocabularies
\item[c)] They ignore global co-occurrence statistics
\item[d)] They are computationally less efficient than prediction-based models
\end{itemize}

\item Why is fine-tuning necessary after pretraining models like BERT?
\begin{itemize}
\item[a)] To adapt the pre-trained model to specific downstream tasks
\item[b)] To reduce the overfitting of embeddings
\item[c)] To remove irrelevant data from the training corpus
\item[d)] To improve the unsupervised learning phase
\end{itemize}

\item Which of the following is an example of a downstream task for embeddings?
\begin{itemize}
\item[a)] Text classification
\item[b)] Language model pretraining
\item[c)] Vector space optimization
\item[d)] Corpus augmentation
\end{itemize}

\item What differentiates subword embeddings from word embeddings?
\begin{itemize}
\item[a)] Subword embeddings use smaller context windows
\item[b)] Subword embeddings can represent parts of words, such as prefixes or suffixes
\item[c)] Subword embeddings require labeled datasets
\item[d)] Subword embeddings ignore rare words in the corpus
\end{itemize}

\item What aspect of embedding vectors makes them suitable for clustering tasks?
\begin{itemize}
\item[a)] High sparsity
\item[b)] Semantic proximity in vector space
\item[c)] Uniform dimensionality across all tokens
\item[d)] Static nature of vectors
\end{itemize}

\item Why are attention mechanisms critical for machine translation?
\begin{itemize}
\item[a)] They provide context for each word in the input and output sequences
\item[b)] They replace embeddings with binary representations
\item[c)] They align the embeddings to word frequencies
\item[d)] They reduce computational overhead
\end{itemize}

\item How does the Glove model derive semantic information?
\begin{itemize}
\item[a)] By focusing on ratios of co-occurrences between word pairs
\item[b)] By using hierarchical Softmax for faster training
\item[c)] By splitting words into subword units
\item[d)] By reducing embeddings to binary vectors
\end{itemize}

\item What is a key feature of embeddings produced by neural network language models?
\begin{itemize}
\item[a)] They encode both syntactic and semantic properties of words
\item[b)] They eliminate the need for labeled datasets entirely
\item[c)] They require fewer dimensions than count-based models
\item[d)] They are static and context-independent
\end{itemize}


\item What is the primary challenge for web crawlers in handling dynamic content?
\begin{itemize}
\item[a)] Respecting the robots.txt file
\item[b)] Recognizing duplicate pages
\item[c)] Managing traps with infinite dynamically generated content
\item[d)] Handling different file types
\end{itemize}

\item What does the robots.txt file specify for a crawler?
\begin{itemize}
\item[a)] Pages that should be indexed
\item[b)] Pages that should not be crawled
\item[c)] Pages that need higher ranking
\item[d)] Pages that require frequent updates
\end{itemize}

\item Focused crawling is characterized by:
\begin{itemize}
\item[a)] Randomly selecting URLs from a seed set
\item[b)] Prioritizing pages related to a specific topic
\item[c)] Downloading all forum posts on the web
\item[d)] Crawling only static pages
\end{itemize}

\item What is the most common crawling strategy to balance server load?
\begin{itemize}
\item[a)] Depth-first search
\item[b)] Random crawling
\item[c)] Breadth-first search
\item[d)] Incremental crawling
\end{itemize}

\item How does a crawler detect hidden URLs?
\begin{itemize}
\item[a)] By parsing robots.txt
\item[b)] Through manual discovery
\item[c)] By analyzing unlinked content sources
\item[d)] By following user interactions
\end{itemize}

\item What innovation allows Google to manage files across large clusters?
\begin{itemize}
\item[a)] Distributed MapReduce
\item[b)] Google File System (GFS)
\item[c)] Hadoop Index Manager
\item[d)] Parallel Computing Framework
\end{itemize}

\item What is the purpose of the shuffle and sort step in MapReduce?
\begin{itemize}
\item[a)] To compress the input data
\item[b)] To distribute keys among reducers
\item[c)] To delete duplicate records
\item[d)] To reduce memory overhead
\end{itemize}

\item Which challenge is unique to web-scale indexing compared to traditional indexing?
\begin{itemize}
\item[a)] Creating inverted indexes
\item[b)] Processing data in parallel across machines
\item[c)] Ranking documents by relevance
\item[d)] Storing indexes on single machines
\end{itemize}

\item What does the Map phase in MapReduce typically output in an inverted indexing task?
\begin{itemize}
\item[a)] Word-frequency pairs
\item[b)] Document-summary pairs
\item[c)] Tokenized sentences
\item[d)] Anchor-text descriptions
\end{itemize}

\item Why is it beneficial to use a distributed file system like GFS in indexing?
\begin{itemize}
\item[a)] Ensures data is stored in a single location
\item[b)] Provides robust fault tolerance
\item[c)] Accelerates file compression
\item[d)] Optimizes single-threaded processing
\end{itemize}

\item Which statement about PageRank is true?
\begin{itemize}
\item[a)] It only counts direct inlinks
\item[b)] It assumes every page has at least one pseudo-link
\item[c)] It ignores anchor text
\item[d)] It cannot be combined with other ranking algorithms
\end{itemize}

\item In the random surfer model, what is the significance of parameter $\alpha$?
\begin{itemize}
\item[a)] It determines the likelihood of jumping to any random page
\item[b)] It controls the rate of damping in PageRank
\item[c)] It represents the average time spent on a page
\item[d)] It balances inlink and outlink weightage
\end{itemize}

\item HITS differs from PageRank because it:
\begin{itemize}
\item[a)] Focuses only on authority scores
\item[b)] Computes both hub and authority scores
\item[c)] Requires random jump probabilities
\item[d)] Ignores adjacency matrix multiplication
\end{itemize}

\item What role does anchor text play in link-based ranking?
\begin{itemize}
\item[a)] Improves ranking robustness
\item[b)] Provides additional descriptive context for target pages
\item[c)] Prevents spam links from influencing scores
\item[d)] Simplifies inverted index creation
\end{itemize}

\item How does PageRank handle dead-end pages (zero outlinks)?
\begin{itemize}
\item[a)] Ignores them in computations
\item[b)] Smooths the matrix with a random jump probability
\item[c)] Deletes such pages from the graph
\item[d)] Relies solely on anchor text for ranking
\end{itemize}

\item What is the primary goal of learning to rank in search engines?
\begin{itemize}
\item[a)] Optimizing storage efficiency
\item[b)] Combining multiple features into a single ranking function
\item[c)] Maximizing crawl frequency
\item[d)] Creating anchor-text-based relevance scores
\end{itemize}

\item Logistic regression in ranking assumes:
\begin{itemize}
\item[a)] Relevance is a nonlinear combination of features
\item[b)] Feature weights are static
\item[c)] Features can be combined linearly
\item[d)] Ranking scores must be binary
\end{itemize}

\item Which feature might NOT be included in a learning-to-rank model?
\begin{itemize}
\item[a)] PageRank score
\item[b)] URL format
\item[c)] User query
\item[d)] Crawler delay
\end{itemize}

\item Why do advanced ranking models optimize metrics like MAP or NDCG?
\begin{itemize}
\item[a)] To handle noisy data
\item[b)] To align ranking with user satisfaction
\item[c)] To predict relevance without human feedback
\item[d)] To simplify training data preparation
\end{itemize}

\item How can personalization improve ranking in learning to rank?
\begin{itemize}
\item[a)] By weighting anchor text more heavily
\item[b)] By creating user-specific PageRank scores
\item[c)] By limiting the scope of crawled pages
\item[d)] By prioritizing static over dynamic pages
\end{itemize}

\item Vertical search engines are more effective because they:
\begin{itemize}
\item[a)] Use general scoring functions
\item[b)] Focus on a specialized domain
\item[c)] Avoid user feedback
\item[d)] Only index static pages
\end{itemize}

\item Lifelong learning in search engines enables:
\begin{itemize}
\item[a)] Static indexing and scoring
\item[b)] Continuous improvement based on user behavior
\item[c)] Prioritization of historical relevance
\item[d)] Manual tuning of ranking functions
\end{itemize}

\item Integrating search and recommendation systems can provide:
\begin{itemize}
\item[a)] Randomized ranking
\item[b)] Enhanced decision support
\item[c)] Spam-resistant indexing
\item[d)] Faster crawling
\end{itemize}

\item Intelligent systems aim to connect which three elements?
\begin{itemize}
\item[a)] User-Agent-Query
\item[b)] Data-User-Service
\item[c)] Link-Anchor-Hub
\item[d)] Query-Document-Feedback
\end{itemize}

\item What does Google's Knowledge Graph exemplify?
\begin{itemize}
\item[a)] Bag-of-words implementation
\item[b)] Large-scale semantic analysis
\item[c)] Link-based ranking
\item[d)] URL optimization
\end{itemize}

\item Why is incremental crawling beneficial?
\begin{itemize}
\item[a)] It avoids re-crawling unchanged pages
\item[b)] It increases the speed of initial indexing
\item[c)] It reduces the number of servers required
\item[d)] It focuses only on high-utility pages
\end{itemize}

\item A focused crawler is more efficient than a general crawler because it:
\begin{itemize}
\item[a)] Operates without a seed set
\item[b)] Avoids parsing robots.txt files
\item[c)] Targets a pre-specified topic
\item[d)] Ignores page duplication
\end{itemize}

\item Which of the following best describes parallel crawling?
\begin{itemize}
\item[a)] Analyzing one server at a time
\item[b)] Simultaneously fetching pages across multiple threads or machines
\item[c)] Restricting crawling to local networks
\item[d)] Alternating between depth-first and breadth-first crawling
\end{itemize}

\item What is a critical consideration for crawlers when interacting with a web server?
\begin{itemize}
\item[a)] Matching all metadata
\item[b)] Avoiding excessive requests to prevent overload
\item[c)] Parsing all HTML elements
\item[d)] Ignoring embedded media files
\end{itemize}

\item What kind of information might a crawler prioritize when respecting the robots.txt file?
\begin{itemize}
\item[a)] URL patterns disallowed for crawling
\item[b)] Priority levels of indexed pages
\item[c)] External link frequency
\item[d)] Target page's PageRank
\end{itemize}

\item What is the purpose of MapReduce in indexing?
\begin{itemize}
\item[a)] To normalize input text data
\item[b)] To facilitate parallel processing of data across large clusters
\item[c)] To extract semantic information from pages
\item[d)] To minimize query time
\end{itemize}

\item What component ensures fault tolerance in the MapReduce framework?
\begin{itemize}
\item[a)] Load balancer
\item[b)] Reducer fallback mechanisms
\item[c)] Task re-execution on other servers
\item[d)] Enhanced scheduler prioritization
\end{itemize}

\item How does the Google File System (GFS) ensure reliability of stored data?
\begin{itemize}
\item[a)] Dynamic re-indexing
\item[b)] Replicating data chunks across multiple servers
\item[c)] Incremental page fetching
\item[d)] Regular checksum verifications
\end{itemize}

\item In MapReduce, what happens to the outputs of the Map function?
\begin{itemize}
\item[a)] They are directly added to the index
\item[b)] They are grouped and sorted by key for the Reduce function
\item[c)] They are analyzed for spam detection
\item[d)] They are used to update the PageRank
\end{itemize}

\item Why is it necessary to distribute an inverted index across machines?
\begin{itemize}
\item[a)] To handle the scale of web data efficiently
\item[b)] To ensure content relevance
\item[c)] To prioritize link-based ranking
\item[d)] To minimize downtime during retrieval
\end{itemize}

\item Which algorithm introduced the concept of authority and hub scores?
\begin{itemize}
\item[a)] PageRank
\item[b)] HITS
\item[c)] BM25
\item[d)] Personalized PageRank
\end{itemize}

\item What differentiates Personalized PageRank from standard PageRank?
\begin{itemize}
\item[a)] Incorporation of random jumps
\item[b)] Use of query-specific relevance
\item[c)] Focus on external links
\item[d)] Elimination of anchor text
\end{itemize}

\item How does the HITS algorithm define a good hub page?
\begin{itemize}
\item[a)] A page with many inlinks
\item[b)] A page that links to high-authority pages
\item[c)] A page that is frequently updated
\item[d)] A page that balances inlinks and outlinks
\end{itemize}

\item PageRank captures indirect citations by:
\begin{itemize}
\item[a)] Counting secondary and tertiary links recursively
\item[b)] Penalizing pages with too many outlinks
\item[c)] Assigning equal weight to all links
\item[d)] Ignoring low-quality pages
\end{itemize}

\item What is the primary limitation of HITS compared to PageRank?
\begin{itemize}
\item[a)] HITS does not account for link quality
\item[b)] HITS is query-dependent
\item[c)] HITS cannot handle spam effectively
\item[d)] HITS disregards link structure in ranking
\end{itemize}


\item Why are multiple features combined in learning-to-rank models?
\begin{itemize}
\item[a)] To remove redundant data
\item[b)] To improve robustness and accuracy of ranking
\item[c)] To minimize query latency
\item[d)] To prioritize feedback from users
\end{itemize}

\item What is a potential drawback of regression-based learning-to-rank methods?
\begin{itemize}
\item[a)] Lack of scalability
\item[b)] Inability to optimize ranking metrics like NDCG directly
\item[c)] Dependence on PageRank
\item[d)] Ignoring link-based features
\end{itemize}

\item Which metric is often used to evaluate ranking accuracy in training data?
\begin{itemize}
\item[a)] Precision at N
\item[b)] Mean Reciprocal Rank (MRR)
\item[c)] Normalized Discounted Cumulative Gain (NDCG)
\item[d)] ROC-AUC
\end{itemize}

\item What distinguishes logistic regression from linear regression in ranking tasks?
\begin{itemize}
\item[a)] Logistic regression maps scores to probabilities between 0 and 1
\item[b)] Logistic regression ignores feature weights
\item[c)] Logistic regression requires no training data
\item[d)] Logistic regression is unsuitable for ranking
\end{itemize}

\item Advanced machine learning ranking algorithms aim to:
\begin{itemize}
\item[a)] Prioritize simplicity over accuracy
\item[b)] Directly optimize ranking metrics
\item[c)] Eliminate the need for feature extraction
\item[d)] Generate new queries for users
\end{itemize}

\item Vertical search engines differ from general search engines by:
\begin{itemize}
\item[a)] Using broader document corpora
\item[b)] Catering to specialized user groups or domains
\item[c)] Relying only on anchor text analysis
\item[d)] Avoiding machine learning-based ranking
\end{itemize}

\item Personalized search engines benefit users by:
\begin{itemize}
\item[a)] Ignoring search history for privacy
\item[b)] Adapting results based on individual preferences
\item[c)] Using static ranking algorithms
\item[d)] Favoring general queries over specific ones
\end{itemize}

\item What is the goal of integrating recommendation systems into search engines?
\begin{itemize}
\item[a)] To improve crawling efficiency
\item[b)] To provide users with task-specific support
\item[c)] To reduce server load
\item[d)] To minimize reliance on link-based ranking
\end{itemize}

\item The "Data-User-Service Triangle" emphasizes:
\begin{itemize}
\item[a)] Focus on vertical search engines
\item[b)] Linking data, users, and services for intelligent systems
\item[c)] Replacing crawling with predictive analytics
\item[d)] Standardizing all search algorithms
\end{itemize}

\item How does Google's Knowledge Graph improve search?
\begin{itemize}
\item[a)] By enhancing URL ranking
\item[b)] By introducing semantic representations of entities and relationships
\item[c)] By using traditional bag-of-words models
\item[d)] By simplifying web crawling
\end{itemize}

\item Lifelong learning in search engines relies on:
\begin{itemize}
\item[a)] Static indexing of historical data
\item[b)] Continuous improvement based on user interaction data
\item[c)] User-provided manual rankings
\item[d)] Eliminating relevance feedback
\end{itemize}

\item Intelligent systems aim to improve productivity by:
\begin{itemize}
\item[a)] Automating every step of a user's workflow
\item[b)] Combining user effort with machine efficiency
\item[c)] Replacing human input in decision-making
\item[d)] Ignoring contextual user behavior
\end{itemize}

\item Which future trend involves combining search, navigation, and recommendation?
\begin{itemize}
\item[a)] Dynamic indexing
\item[b)] Integrated information management
\item[c)] Anchor text analysis
\item[d)] Static PageRank optimization
\end{itemize}

\item How does interactive task support differ from traditional search?
\begin{itemize}
\item[a)] It focuses on crawling static pages
\item[b)] It combines human and machine collaboration for task completion
\item[c)] It eliminates the need for relevance feedback
\item[d)] It prioritizes speed over accuracy
\end{itemize}

\item Which feature best characterizes modern intelligent systems?
\begin{itemize}
\item[a)] Focus on broad generalization
\item[b)] Emphasis on semantic analysis and personalized interaction
\item[c)] Reliance on manual query processing
\item[d)] Avoidance of machine learning
\end{itemize}

\item What is the primary goal of text data access in comparison to text data analysis?
\begin{itemize}
\item[a)] To summarize large datasets
\item[b)] To focus on relevant data for further processing
\item[c)] To create predictive models from data
\item[d)] To emphasize sentiment analysis
\end{itemize}

\item Which scenario highlights the importance of advanced text analysis tools?
\begin{itemize}
\item[a)] When manual text processing is sufficient
\item[b)] When the dataset is small and static
\item[c)] For time-critical applications requiring fast decision-making
\item[d)] When search engines alone suffice for text retrieval
\end{itemize}

\item What is a key benefit of text analysis in scientific research?
\begin{itemize}
\item[a)] Automating document indexing
\item[b)] Integrating terminology across disciplines
\item[c)] Simplifying the extraction of word associations
\item[d)] Reducing the need for search engines
\end{itemize}

\item A disaster management system can benefit from real-time text analysis by:
\begin{itemize}
\item[a)] Creating predictive models for stock markets
\item[b)] Extracting topic clusters from social media discussions
\item[c)] Monitoring warning signs in tweets about potential natural disasters
\item[d)] Organizing data into hierarchical categories
\end{itemize}

\item Text mining is often referred to as a "datascope" because:
\begin{itemize}
\item[a)] It focuses solely on text summarization
\item[b)] It reveals hidden patterns and knowledge in large datasets
\item[c)] It exclusively analyzes opinions and sentiments
\item[d)] It automates the manual processes of text translation
\end{itemize}

\item Humans are described as "subjective sensors" because they:
\begin{itemize}
\item[a)] Collect numerical and relational data
\item[b)] Provide unbiased data about the real world
\item[c)] Express observations and opinions through text
\item[d)] Record multimedia data like video or audio
\end{itemize}

\item What advantage does treating text data as human sensor data provide?
\begin{itemize}
\item[a)] Enables integration with physical sensor data
\item[b)] Allows automated sentiment tagging
\item[c)] Simplifies clustering algorithms
\item[d)] Reduces ambiguity in natural language processing
\end{itemize}

\item In the data mining framework, text data is particularly valuable because it:
\begin{itemize}
\item[a)] Is easier to process than numerical data
\item[b)] Provides insight into user preferences and opinions
\item[c)] Requires fewer algorithms to analyze
\item[d)] Contains fewer dimensions than non-text data
\end{itemize}

\item A general text mining problem aims to:
\begin{itemize}
\item[a)] Create standardized datasets from text
\item[b)] Transform text into actionable knowledge for decision-making
\item[c)] Analyze text without using non-textual metadata
\item[d)] Predict trends based solely on numerical data
\end{itemize}

\item How can non-text data enhance text analysis?
\begin{itemize}
\item[a)] By replacing text-based clustering techniques
\item[b)] By providing context, such as time or location, for text interpretation
\item[c)] By reducing the need for feature extraction in text
\item[d)] By enabling real-time text translation
\end{itemize}

\item What is a key challenge when mining knowledge from text data?
\begin{itemize}
\item[a)] Extracting objective statements separately from subjective opinions
\item[b)] Representing data in relational databases
\item[c)] Identifying errors in physical sensor readings
\item[d)] Avoiding repetitive clustering tasks
\end{itemize}

\item Mining knowledge about the observer (text producer) involves:
\begin{itemize}
\item[a)] Extracting the main topic of the text
\item[b)] Predicting the author's sentiment or mood
\item[c)] Identifying the publication date of the text
\item[d)] Summarizing key numerical data points
\end{itemize}

\item What is a typical goal of text-based predictive analytics?
\begin{itemize}
\item[a)] Visualizing the structure of text data
\item[b)] Forecasting non-textual variables based on text correlations
\item[c)] Summarizing documents into key topics
\item[d)] Identifying synonyms and collocations
\end{itemize}

\item What is one way to generate effective features for text-based predictive models?
\begin{itemize}
\item[a)] Use high-level semantic features like topics instead of individual words
\item[b)] Ignore variations in word usage across contexts
\item[c)] Rely exclusively on raw word counts
\item[d)] Minimize the use of text mining algorithms
\end{itemize}

\item Associating non-text data with text analysis allows:
\begin{itemize}
\item[a)] Static representations of knowledge
\item[b)] Context-sensitive insights and trend detection
\item[c)] Reduction of computational complexity
\item[d)] Easier extraction of synonyms from documents
\end{itemize}

\item Which task falls under the category of mining knowledge about the observed world?
\begin{itemize}
\item[a)] Analyzing public sentiment towards policies
\item[b)] Identifying linguistic patterns in English texts
\item[c)] Extracting factual statements about entities or events
\item[d)] Predicting future events based on text trends
\end{itemize}

\item What is one major distinction between mining knowledge about text producers and observed worlds?
\begin{itemize}
\item[a)] Producers provide objective facts, while the observed world includes opinions
\item[b)] Text producers often include subjective statements, while observed worlds provide facts
\item[c)] Observed worlds rely on metadata, while producers focus on clustering
\item[d)] Producers focus on syntactic rules, while observed worlds involve lexicons
\end{itemize}

\item Predictive models that leverage text and non-text data often benefit from:
\begin{itemize}
\item[a)] Ignoring correlations between data types
\item[b)] Combining features from text-based topics and historical numerical data
\item[c)] Reducing all metadata to a single category
\item[d)] Prioritizing physical sensors over subjective text
\end{itemize}

\item Sentiment analysis primarily aims to:
\begin{itemize}
\item[a)] Summarize factual information in text
\item[b)] Understand subjective opinions in text data
\item[c)] Predict numerical values from non-text data
\item[d)] Enhance text-based clustering techniques
\end{itemize}

\item How does clustering contribute to text mining?
\begin{itemize}
\item[a)] It predicts future variables based on word usage
\item[b)] It groups similar text objects for exploratory analysis
\item[c)] It focuses exclusively on summarizing text data
\item[d)] It automates metadata extraction
\end{itemize}

\item Joint analysis of text and non-text data enables:
\begin{itemize}
\item[a)] Standalone text classification models
\item[b)] A deeper understanding of context-sensitive knowledge
\item[c)] Simplified natural language processing tasks
\item[d)] Elimination of metadata dependencies
\end{itemize}

\item Which metadata is commonly associated with text data?
\begin{itemize}
\item[a)] User browsing history
\item[b)] Sensor calibration details
\item[c)] Time and location of text creation
\item[d)] Physical sensor measurements
\end{itemize}

\item Why are high-level features like topics preferred for text mining models?
\begin{itemize}
\item[a)] They simplify sentiment analysis
\item[b)] They reduce computational cost significantly
\item[c)] They address ambiguity and variations in word usage
\item[d)] They remove all subjective elements from text
\end{itemize}

\item Text categorization differs from text clustering because it:
\begin{itemize}
\item[a)] Groups similar objects for exploration
\item[b)] Assigns text to predefined categories
\item[c)] Ignores linguistic patterns in text
\item[d)] Focuses only on opinion mining
\end{itemize}

\item What is a common use case for text summarization?
\begin{itemize}
\item[a)] Discovering new associations between words
\item[b)] Generating concise overviews of document content
\item[c)] Identifying author sentiment across a corpus
\item[d)] Predicting stock market trends
\end{itemize}

\item What is the purpose of combining text data analysis with data mining techniques?
\begin{itemize}
\item[a)] To streamline document clustering
\item[b)] To extract actionable knowledge for decision-making
\item[c)] To eliminate subjective elements from datasets
\item[d)] To prioritize multimedia data over textual data
\end{itemize}

\item Why is the division between text data access and text data analysis stages described as "artificial"?
\begin{itemize}
\item[a)] Because both stages rely on identical algorithms
\item[b)] Because sophisticated applications interleave these stages iteratively
\item[c)] Because text access inherently includes text mining features
\item[d)] Because both stages ignore non-textual metadata
\end{itemize}

\item How can text data analysis enhance business intelligence?
\begin{itemize}
\item[a)] By identifying real-time trends in sensor-generated data
\item[b)] By revealing customer opinions and comparing competitor products
\item[c)] By optimizing the storage of customer reviews
\item[d)] By clustering numerical data into actionable categories
\end{itemize}

\item How do "subjective sensors" differ from traditional physical sensors?
\begin{itemize}
\item[a)] Subjective sensors provide unbiased numerical data
\item[b)] Subjective sensors interpret and express personal observations of the real world
\item[c)] Subjective sensors rely solely on relational database formats
\item[d)] Subjective sensors are programmed to analyze only text data
\end{itemize}

\item Joint mining of text and non-text data enables:
\begin{itemize}
\item[a)] Separate analysis of text and numerical data
\item[b)] Integration of context-sensitive insights with metadata
\item[c)] Complete automation of all text summarization tasks
\item[d)] Reduction of storage space for multimedia data
\end{itemize}

\item When combining text and non-text data, what can temporal metadata (e.g., timestamps) enable?
\begin{itemize}
\item[a)] Real-time clustering of unrelated text data
\item[b)] Discovery of trends and comparisons across time periods
\item[c)] Replacement of text data with numerical data
\item[d)] Isolation of subjective opinions from metadata
\end{itemize}

\item What is a significant benefit of combining predictive text models with historical non-text data?
\begin{itemize}
\item[a)] Simplified storage of text summaries
\item[b)] Improved accuracy of predictions through complementary features
\item[c)] Elimination of data preprocessing requirements
\item[d)] Automation of all text mining tasks
\end{itemize}

\item Which text mining task focuses on identifying and interpreting patterns in lexical data?
\begin{itemize}
\item[a)] Opinion mining
\item[b)] Word association mining
\item[c)] Predictive analytics
\item[d)] Topic modeling
\end{itemize}

\item In predictive analytics, why might semantic topics be more effective than word-level features?
\begin{itemize}
\item[a)] They reduce the need for clustering
\item[b)] They provide higher-level insights, addressing ambiguity and variations in word usage
\item[c)] They eliminate all subjective elements from text data
\item[d)] They rely solely on metadata for contextual analysis
\end{itemize}


\item What is a primary objective of topic modeling in text mining?
\begin{itemize}
\item[a)] Predicting numerical trends based on text
\item[b)] Extracting high-level themes from large corpora of text
\item[c)] Associating metadata with clustered text
\item[d)] Reducing redundancy in text summarization
\end{itemize}

\item Opinion mining and sentiment analysis focus on:
\begin{itemize}
\item[a)] Identifying factual statements about real-world entities
\item[b)] Extracting subjective attitudes and emotions from text
\item[c)] Combining text data with geographical metadata
\item[d)] Clustering text data into exploratory groups
\end{itemize}

\item What distinguishes text categorization from clustering?
\begin{itemize}
\item[a)] Categorization assigns text to predefined classes, while clustering groups similar texts without predefined labels
\item[b)] Categorization uses exploratory analysis, while clustering focuses on summaries
\item[c)] Categorization relies solely on numerical metadata, while clustering uses lexical data
\item[d)] Categorization is less structured than clustering
\end{itemize}

\item How can metadata, such as location, enhance the analysis of text data?
\begin{itemize}
\item[a)] By creating standalone summaries for documents
\item[b)] By providing new perspectives for comparative analysis
\item[c)] By clustering documents based solely on geographical factors
\item[d)] By ignoring temporal trends in text data
\end{itemize}

\item Which text mining task might benefit the most from integrating multimedia non-text data?
\begin{itemize}
\item[a)] Sentiment analysis
\item[b)] Predictive analytics
\item[c)] Word association mining
\item[d)] Clustering
\end{itemize}

\item Associating topics with time can generate:
\begin{itemize}
\item[a)] Trend analysis of textual themes over specific periods
\item[b)] Sentiment predictions for text producers
\item[c)] Standalone summaries of time-specific clusters
\item[d)] Metadata-free clustering models
\end{itemize}

\item What is a potential use case for text mining in disaster response?
\begin{itemize}
\item[a)] Automating document summarization for past events
\item[b)] Monitoring social media for early warning signs of natural disasters
\item[c)] Identifying customer sentiment in product reviews
\item[d)] Summarizing technical documentation for first responders
\end{itemize}

\item Why might a text mining model integrate data from social media platforms?
\begin{itemize}
\item[a)] To simplify text preprocessing requirements
\item[b)] To analyze real-time opinions or trends on specific topics
\item[c)] To reduce dependency on physical sensors
\item[d)] To increase clustering efficiency in text datasets
\end{itemize}

\item Predictive analytics in text mining often leverages correlations between:
\begin{itemize}
\item[a)] Metadata and numerical data
\item[b)] Text content and external real-world variables
\item[c)] Word frequency and document size
\item[d)] Clustering algorithms and lexical features
\end{itemize}

\item How does joint analysis of text and metadata enhance clustering?
\begin{itemize}
\item[a)] It reduces computational complexity
\item[b)] It enables clustering based on time, location, or other contextual factors
\item[c)] It eliminates subjective data elements
\item[d)] It avoids the need for word-level feature extraction
\end{itemize}

\item What is a common output of text summarization tasks?
\begin{itemize}
\item[a)] Extracted real-world variables for forecasting
\item[b)] Concise representations of key information in a text
\item[c)] Sentiment analysis of subjective opinions
\item[d)] Trends in geographical metadata
\end{itemize}

\item What is the primary goal of clustering in text analysis?
\begin{itemize}
\item[a)] Summarizing document content
\item[b)] Grouping similar objects to reveal inherent structure
\item[c)] Predicting future trends in text datasets
\item[d)] Extracting semantic relationships between terms
\end{itemize}

\item Which of the following is a unique advantage of clustering algorithms?
\begin{itemize}
\item[a)] They require labeled training data
\item[b)] They can only process small text datasets effectively
\item[c)] They are unsupervised and applicable to any text dataset
\item[d)] They rely on predefined similarity metrics for optimal performance
\end{itemize}

\item What does term clustering typically enable in text mining applications?
\begin{itemize}
\item[a)] Improved document summarization
\item[b)] Creation of a thesaurus or finding semantic concepts
\item[c)] Real-time sentiment analysis
\item[d)] Redundant removal of query results
\end{itemize}

\item Clustering results are often used to:
\begin{itemize}
\item[a)] Identify predefined categories in a dataset
\item[b)] Provide an overview of data for exploratory analysis
\item[c)] Replace document similarity measures
\item[d)] Automate topic summarization tasks
\end{itemize}

\item What is the concept of "clustering bias"?
\begin{itemize}
\item[a)] The tendency of clusters to include noise from unrelated documents
\item[b)] The specific perspective used to define similarity for clustering
\item[c)] The inability of clustering algorithms to process high-dimensional data
\item[d)] The use of probabilistic models in clustering methods
\end{itemize}

\item Which type of clustering method merges smaller groups incrementally to form larger clusters?
\begin{itemize}
\item[a)] Divisive clustering
\item[b)] Agglomerative clustering
\item[c)] Model-based clustering
\item[d)] Hierarchical divisive clustering
\end{itemize}

\item What is a key characteristic of model-based clustering?
\begin{itemize}
\item[a)] Hard cluster assignment
\item[b)] Creation of a dendrogram
\item[c)] Assignment of objects to clusters with probabilistic distributions
\item[d)] Use of fixed similarity metrics for partitioning
\end{itemize}

\item What is an advantage of similarity-based clustering over model-based clustering?
\begin{itemize}
\item[a)] Ability to perform soft cluster assignment
\item[b)] Direct reliance on predefined similarity functions
\item[c)] Flexibility to encode complex probabilistic constraints
\item[d)] Application to both text and multimedia data
\end{itemize}

\item In the K-means algorithm, what role does the centroid play?
\begin{itemize}
\item[a)] Acts as a measure of cluster similarity
\item[b)] Represents the central point of a cluster
\item[c)] Identifies the largest outlier in a cluster
\item[d)] Maximizes inter-cluster separation
\end{itemize}

\item Which clustering approach generates clusters by repeatedly dividing a dataset?
\begin{itemize}
\item[a)] Agglomerative clustering
\item[b)] K-means clustering
\item[c)] Complete-link clustering
\item[d)] Average-link clustering
\end{itemize}

\item What is a key criterion for evaluating the coherence of a cluster?
\begin{itemize}
\item[a)] The separation of objects in different clusters
\item[b)] The similarity of objects within the same cluster
\item[c)] The utility of clusters in specific applications
\item[d)] The hierarchical structure of clusters
\end{itemize}

\item Why is choosing the right feature representation important in clustering?
\begin{itemize}
\item[a)] To increase the number of clusters generated
\item[b)] To minimize computational complexity
\item[c)] To capture crucial concepts that differentiate clusters
\item[d)] To reduce the reliance on similarity functions
\end{itemize}

\item What is one challenge in evaluating term clustering?
\begin{itemize}
\item[a)] Lack of predefined document categories
\item[b)] Difficulty in defining word-to-word similarity algorithms
\item[c)] Over-reliance on cluster labels for scoring
\item[d)] Inability to handle high-dimensional data
\end{itemize}

\item Which measure ensures that clusters are far from each other?
\begin{itemize}
\item[a)] Coherence
\item[b)] Separation
\item[c)] Utility
\item[d)] Soft clustering
\end{itemize}

\item When evaluating clusters for search applications, which metric can be used to assess improvement?
\begin{itemize}
\item[a)] Precision at N
\item[b)] Mean Average Precision (MAP)
\item[c)] Normalized Cumulative Discounted Gain (NDCG)
\item[d)] All of the above
\end{itemize}

\item What does the single-link clustering algorithm focus on?
\begin{itemize}
\item[a)] Maximum distance between elements in clusters
\item[b)] Minimum distance between elements in clusters
\item[c)] Average distance between cluster centroids
\item[d)] Random assignment of clusters
\end{itemize}

\item How does complete-link clustering differ from single-link clustering?
\begin{itemize}
\item[a)] It merges clusters with the largest maximum distance between elements
\item[b)] It produces looser clusters with larger diameters
\item[c)] It prioritizes clusters with overlapping elements
\item[d)] It keeps clusters compact by minimizing intra-cluster distances
\end{itemize}

\item What is the significance of the Expectation-Maximization (EM) algorithm in K-means?
\begin{itemize}
\item[a)] It automates the selection of similarity metrics
\item[b)] It alternates between assigning points to clusters and updating centroids
\item[c)] It replaces similarity-based clustering methods entirely
\item[d)] It focuses on hierarchical clustering alone
\end{itemize}

\item How does hierarchical clustering represent the merging of clusters?
\begin{itemize}
\item[a)] As a weighted similarity matrix
\item[b)] As a term-document frequency chart
\item[c)] As a dendrogram
\item[d)] As a centroid-based assignment graph
\end{itemize}

\item When combining clustering with search engines, what feature can clustering provide?
\begin{itemize}
\item[a)] Faster indexing of documents
\item[b)] Summarization of query results
\item[c)] Identification of redundant metadata
\item[d)] Automatic query reformulation
\end{itemize}

\item What is a unique feature of supervised clustering compared to unsupervised clustering?
\begin{itemize}
\item[a)] It automatically generates predefined categories.
\item[b)] It incorporates user-provided constraints on cluster membership.
\item[c)] It requires hierarchical structures for clusters.
\item[d)] It eliminates the need for similarity measures.
\end{itemize}

\item Which aspect of clustering is typically not automated and often requires human input?
\begin{itemize}
\item[a)] Determining the number of clusters
\item[b)] Calculating intra-cluster coherence
\item[c)] Generating cluster labels
\item[d)] Computing inter-cluster separation
\end{itemize}

\item In text clustering, why might a user want to set the number of clusters beforehand?
\begin{itemize}
\item[a)] To ensure faster evaluation metrics computation
\item[b)] To tailor the clustering output to specific application needs
\item[c)] To automatically label the clusters
\item[d)] To minimize redundancy in document parsing
\end{itemize}

\item Which of the following is a practical application of term clustering?
\begin{itemize}
\item[a)] Generating more diverse search results
\item[b)] Reducing memory requirements for clustering algorithms
\item[c)] Enabling query expansion by identifying similar terms
\item[d)] Automating the generation of training data
\end{itemize}

\item How does Brown clustering differ from basic agglomerative clustering?
\begin{itemize}
\item[a)] It incorporates a probabilistic approach to clustering terms hierarchically.
\item[b)] It relies on cosine similarity exclusively.
\item[c)] It is designed specifically for document-level clustering.
\item[d)] It avoids the use of similarity metrics entirely.
\end{itemize}

\item What makes evaluating utility in clustering more challenging than coherence or separation?
\begin{itemize}
\item[a)] It requires computing cluster purity scores.
\item[b)] It depends on the specific application and the overall effectiveness of the system.
\item[c)] It focuses entirely on statistical measures like F1 or MAP.
\item[d)] It eliminates the need for human evaluation.
\end{itemize}

\item Why are predefined categories useful in evaluating clustering algorithms?
\begin{itemize}
\item[a)] They simplify cluster labeling tasks.
\item[b)] They provide a benchmark to compare the clustering output against ground truth.
\item[c)] They eliminate the need for similarity metrics.
\item[d)] They reduce computation time for hierarchical clustering.
\end{itemize}

\item What is the main limitation of K-means regarding evaluation of clusters?
\begin{itemize}
\item[a)] Sensitivity to outliers
\item[b)] Difficulty in interpreting centroid values
\item[c)] Dependence on initial random centroids
\item[d)] Over-reliance on cosine similarity
\end{itemize}

\item When using clustering in exploratory analysis, how can iterative refinement help?
\begin{itemize}
\item[a)] By allowing users to adjust similarity thresholds dynamically
\item[b)] By automating the labeling of all clusters
\item[c)] By generating additional metadata for documents
\item[d)] By preventing over-segmentation of clusters
\end{itemize}

\item Which clustering measure is most useful when comparing text documents with varying term frequencies?
\begin{itemize}
\item[a)] Jaccard similarity
\item[b)] Euclidean distance
\item[c)] Cosine similarity
\item[d)] Mutual information
\end{itemize}

\item In search engines, clustering is particularly useful for:
\begin{itemize}
\item[a)] Identifying outliers in query logs
\item[b)] Organizing retrieval results into groups for easier navigation
\item[c)] Reducing latency during document indexing
\item[d)] Automating user intent prediction
\end{itemize}

\item How can hierarchical clustering improve the navigation of a text corpus?
\begin{itemize}
\item[a)] By providing a tree structure that allows users to drill down into specific clusters
\item[b)] By generating a single large cluster for faster processing
\item[c)] By summarizing documents based on cosine similarity
\item[d)] By automating document labeling during analysis
\end{itemize}

\item Which task can benefit from clustering both terms and documents simultaneously?
\begin{itemize}
\item[a)] Topic modeling
\item[b)] Sentiment analysis
\item[c)] Metadata extraction
\item[d)] Query reformulation
\end{itemize}

\item What is the primary advantage of using soft clustering methods?
\begin{itemize}
\item[a)] They reduce computation time by avoiding similarity calculations.
\item[b)] They allow objects to belong to multiple clusters with associated probabilities.
\item[c)] They provide deterministic output for hierarchical clustering.
\item[d)] They eliminate user-defined parameters like the number of clusters.
\end{itemize}

\item What is a typical way to handle outliers in clustering algorithms like K-means?
\begin{itemize}
\item[a)] Reassign them to the nearest cluster during centroid computation
\item[b)] Ignore them entirely during the clustering process
\item[c)] Create dedicated clusters exclusively for outliers
\item[d)] Normalize similarity metrics to account for outliers
\end{itemize}

\item What is the primary difference between clustering and categorization techniques for text data?
\begin{itemize}
\item[a)] Clustering uses supervised learning, while categorization is unsupervised.
\item[b)] Clustering generates predefined labels, whereas categorization creates clusters.
\item[c)] Categorization uses predefined categories, while clustering groups based on similarity.
\item[d)] Categorization is always hierarchical, while clustering is flat.
\end{itemize}

\item In the context of text categorization, what is the purpose of "text annotation"?
\begin{itemize}
\item[a)] To train a clustering model using labeled data.
\item[b)] To infer properties about entities from text.
\item[c)] To represent text with multiple levels, such as keywords and categories.
\item[d)] To eliminate redundancy in text corpora.
\end{itemize}

\item Which approach is most suitable for categorization when no labeled training data is available?
\begin{itemize}
\item[a)] Rule-based manual categorization.
\item[b)] Supervised machine learning.
\item[c)] Generative classifiers.
\item[d)] Lazy learning algorithms.
\end{itemize}


\item What is a primary limitation of the manual rule-based approach in text categorization?
\begin{itemize}
\item[a)] It is overly reliant on training data.
\item[b)] It cannot handle categorical hierarchies.
\item[c)] It is labor-intensive and does not scale well.
\item[d)] It fails in the absence of domain knowledge.
\end{itemize}


\item How do lazy learners like k-NN differ from discriminative classifiers?
\begin{itemize}
\item[a)] Lazy learners use rule-based systems, while discriminative classifiers use probabilistic models.
\item[b)] Lazy learners require no explicit training, while discriminative classifiers rely on training data.
\item[c)] Lazy learners are computationally faster than discriminative classifiers during testing.
\item[d)] Lazy learners are inherently multiclass, while discriminative classifiers are binary.
\end{itemize}


\item In Naive Bayes classifiers, why is the independence assumption considered "naive"?
\begin{itemize}
\item[a)] It assumes a linear relationship among features.
\item[b)] It ignores dependencies between features.
\item[c)] It only applies to binary classification.
\item[d)] It overestimates the significance of rare features.
\end{itemize}


\item What is the primary goal of feature selection in text categorization?
\begin{itemize}
\item[a)] To increase the dimensionality of the feature space.
\item[b)] To improve classification performance by identifying discriminative features.
\item[c)] To eliminate the need for labeled training data.
\item[d)] To generate novel features for training data.
\end{itemize}


\item Which feature representation is likely most effective for capturing sentiment in text?
\begin{itemize}
\item[a)] Average sentence length.
\item[b)] Unigram and bigram word tokens.
\item[c)] Part-of-speech tags.
\item[d)] Sentence parse trees.
\end{itemize}


\item What is a significant computational challenge of using k-NN for text categorization?
\begin{itemize}
\item[a)] Building the training dataset.
\item[b)] Calculating centroids for each category.
\item[c)] Performing a search query for each test instance.
\item[d)] Storing weights for feature vectors.
\end{itemize}


\item Which of the following is an example of a discriminative classifier?
\begin{itemize}
\item[a)] Naive Bayes.
\item[b)] k-Nearest Neighbors.
\item[c)] Support Vector Machines.
\item[d)] Centroid Classifier.
\end{itemize}


\item How does SVM ensure robustness in text categorization?
\begin{itemize}
\item[a)] By using probabilistic feature distributions.
\item[b)] By maximizing the margin between decision boundaries and classes.
\item[c)] By relying on local structure in the feature space.
\item[d)] By combining rule-based heuristics with training data.
\end{itemize}


\item Which evaluation metric is most commonly used to assess text categorization performance?
\begin{itemize}
\item[a)] Cosine similarity.
\item[b)] F1 score.
\item[c)] Term frequency-inverse document frequency (TF-IDF).
\item[d)] Perplexity.
\end{itemize}


\item What is the main advantage of using cross-validation in text categorization evaluation?
\begin{itemize}
\item[a)] It reduces the size of the training set.
\item[b)] It provides a higher accuracy score.
\item[c)] It helps detect overfitting by testing on multiple splits.
\item[d)] It simplifies feature selection.
\end{itemize}


\item What does a confusion matrix primarily show in text categorization evaluation?
\begin{itemize}
\item[a)] The overall accuracy of the classifier.
\item[b)] The relationship between features and labels.
\item[c)] True positives, false positives, and class-level predictions.
\item[d)] The effect of training data on testing accuracy.
\end{itemize}


\item Why is unigram word representation often insufficient for sentiment classification?
\begin{itemize}
\item[a)] It fails to capture negations like "not good."
\item[b)] It increases the computational complexity.
\item[c)] It cannot differentiate synonyms.
\item[d)] It does not handle rare words effectively.
\end{itemize}


\item What does the perceptron classifier aim to optimize during training?
\begin{itemize}
\item[a)] The centroid distance for each feature vector.
\item[b)] The classification margin for multiclass problems.
\item[c)] The weight vector that separates the classes linearly.
\item[d)] The probabilities of features for each class label.
\end{itemize}


\item What is the key disadvantage of bigram word representations?
\begin{itemize}
\item[a)] They fail to capture negations.
\item[b)] They miss out on rare informative single words.
\item[c)] They are computationally less efficient than unigram representations.
\item[d)] They introduce ambiguity in sentiment classification.
\end{itemize}


\item Which approach combines multiple binary classifiers to handle multiclass problems?
\begin{itemize}
\item[a)] One-vs-All and All-vs-All.
\item[b)] Lazy learning.
\item[c)] Naive Bayes smoothing.
\item[d)] Structural feature extraction.
\end{itemize}


\item What is the role of smoothing in Naive Bayes classification?
\begin{itemize}
\item[a)] To enhance feature selection.
\item[b)] To handle unseen or rare feature occurrences.
\item[c)] To normalize feature weights.
\item[d)] To convert probabilities into confidence scores.
\end{itemize}


\item Why are structural features combined with lexical features effective in text categorization?
\begin{itemize}
\item[a)] They reduce the dimensionality of the feature space.
\item[b)] They capture orthogonal perspectives of the text.
\item[c)] They improve the speed of classification algorithms.
\item[d)] They eliminate the need for additional training data.
\end{itemize}


\item What is the primary reason for combining multiple feature sets (e.g., unigram and bigram words) in text categorization?
\begin{itemize}
\item[a)] To reduce computational complexity.
\item[b)] To handle sparsity in the training data.
\item[c)] To capture diverse linguistic contexts and enrich the feature space.
\item[d)] To simplify the training process.
\end{itemize}


\item What does a centroid represent in the nearest-centroid classifier?
\begin{itemize}
\item[a)] A single document vector closest to the class label.
\item[b)] The average vector of all document vectors for a given class.
\item[c)] The document vector with the maximum similarity to all other documents in the class.
\item[d)] The highest-weighted features of the class.
\end{itemize}


\item Which of the following is a major limitation of k-NN in text categorization?
\begin{itemize}
\item[a)] It cannot handle multiclass problems.
\item[b)] It requires labeled training data.
\item[c)] It is computationally expensive during testing.
\item[d)] It assumes a linear relationship between features.
\end{itemize}


\item What distinguishes a generative classifier like Naive Bayes from a discriminative classifier like SVM?
\begin{itemize}
\item[a)] Generative classifiers model data distribution, while discriminative classifiers optimize decision boundaries.
\item[b)] Generative classifiers require labeled data, while discriminative classifiers do not.
\item[c)] Generative classifiers operate in low-dimensional spaces, while discriminative classifiers operate in high-dimensional spaces.
\item[d)] Generative classifiers predict probabilities, while discriminative classifiers predict labels directly.
\end{itemize}


\item Why might a classifier default to predicting the majority class label in an imbalanced dataset?
\begin{itemize}
\item[a)] To maximize precision.
\item[b)] To simplify feature extraction.
\item[c)] To minimize the loss function during training.
\item[d)] To avoid the computational cost of handling minority classes.
\end{itemize}


\item What is the main advantage of using the perceptron algorithm for linear classification?
\begin{itemize}
\item[a)] It works well with non-linear datasets.
\item[b)] It is robust against noisy training data.
\item[c)] It provides an interpretable weight vector for features.
\item[d)] It can calculate exact probabilities for classification.
\end{itemize}



\item How can edit features, like those in SYNTACTICDIFF, improve text classification tasks?
\begin{itemize}
\item[a)] By reducing feature dimensionality through tokenization.
\item[b)] By capturing structural differences between source and reference texts.
\item[c)] By enabling faster parsing of grammatical errors.
\item[d)] By eliminating noise in low-frequency words.
\end{itemize}


\item In n-fold cross-validation, what does a high variance in evaluation metrics between folds indicate?
\begin{itemize}
\item[a)] The algorithm is robust to unseen data.
\item[b)] The algorithm has overfitted certain subsets of data.
\item[c)] The dataset lacks sufficient features for classification.
\item[d)] The dataset is too balanced for effective evaluation.
\end{itemize}


\item What is a significant computational tradeoff between k-NN and Naive Bayes classifiers?
\begin{itemize}
\item[a)] k-NN is slower during training, while Naive Bayes is slower during testing.
\item[b)] k-NN is faster during training, while Naive Bayes is faster during testing.
\item[c)] Both are equally computationally expensive.
\item[d)] k-NN requires more memory, while Naive Bayes requires more labeled data.
\end{itemize}


\item How does SVM create a robust decision boundary for linear classification?
\begin{itemize}
\item[a)] By assigning weights to individual features.
\item[b)] By maximizing the margin between classes in the feature space.
\item[c)] By iteratively refining the centroid of each class.
\item[d)] By predicting probabilities for class membership.
\end{itemize}


\item What is a typical use case for a hierarchical categorization system?
\begin{itemize}
\item[a)] Clustering documents based on similarity.
\item[b)] Classifying documents with nested subcategories.
\item[c)] Predicting the sentiment of a document.
\item[d)] Tokenizing documents into sentence-level features.
\end{itemize}


\item Why is smoothing necessary for Naive Bayes in sparse datasets?
\begin{itemize}
\item[a)] To reduce the dimensionality of the feature vectors.
\item[b)] To prevent zero probabilities for unseen features.
\item[c)] To increase the independence of features.
\item[d)] To enable multiclass classification.
\end{itemize}


\item How does the decision boundary of a perceptron differ from that of k-NN?
\begin{itemize}
\item[a)] Perceptron considers global data distribution, while k-NN focuses on local neighbors.
\item[b)] Perceptron creates non-linear boundaries, while k-NN creates linear ones.
\item[c)] Perceptron requires labeled data, while k-NN does not.
\item[d)] Perceptron uses centroids, while k-NN uses prototypes.
\end{itemize}


\item What is the role of grammatical parse trees in feature extraction?
\begin{itemize}
\item[a)] They eliminate the need for unigram and bigram tokens.
\item[b)] They capture high-level syntactic structures for classification.
\item[c)] They simplify the classification of short text documents.
\item[d)] They ensure semantic coherence in category labels.
\end{itemize}


\item Which classifier would you choose for a text categorization task with extremely imbalanced class labels?
\begin{itemize}
\item[a)] k-Nearest Neighbors.
\item[b)] Naive Bayes with smoothing.
\item[c)] Support Vector Machines.
\item[d)] Rule-based classifier.
\end{itemize}


\item In the context of feature engineering, why is the choice of tokenizer critical for classification accuracy?
\begin{itemize}
\item[a)] It determines the computational complexity of the classifier.
\item[b)] It directly affects the feature representation quality.
\item[c)] It eliminates redundancy in feature extraction.
\item[d)] It normalizes the input data distribution.
\end{itemize}


\item How can weighting schemes improve the performance of k-NN classifiers?
\begin{itemize}
\item[a)] By balancing class distribution in training data.
\item[b)] By assigning higher influence to closer neighbors.
\item[c)] By eliminating the need for centroids in classification.
\item[d)] By simplifying the tokenization process.
\end{itemize}


\item What is the significance of the diagonal in a confusion matrix for text categorization?
\begin{itemize}
\item[a)] It represents the overall accuracy of the classifier.
\item[b)] It shows the misclassification rates for each label.
\item[c)] It contains the true positive rates for each category.
\item[d)] It indicates class imbalance in the training data.
\end{itemize}


\item Which recent advances in machine learning have significantly improved abstractive summarization?
\begin{itemize}
\item[a)] Support Vector Machines
\item[b)] Transformer-based architectures
\item[c)] Decision Trees
\item[d)] Rule-based systems
\end{itemize}


\item Why might extractive summarization methods perform better in evaluations using ROUGE?
\begin{itemize}
\item[a)] They are less computationally expensive.
\item[b)] They contain exact text fragments from the original document.
\item[c)] They can generate summaries without training.
\item[d)] They require less preprocessing than abstractive methods.
\end{itemize}


\item What is a limitation of using cosine similarity in MMR-based summarization?
\begin{itemize}
\item[a)] It cannot measure redundancy.
\item[b)] It fails to capture semantic similarity between sentences.
\item[c)] It increases computational costs exponentially.
\item[d)] It prioritizes irrelevant sentences.
\end{itemize}


\item Which summarization technique is best suited for multimodal content (e.g., text and images)?
\begin{itemize}
\item[a)] Extractive summarization.
\item[b)] Multimodal summarization with transformers.
\item[c)] MMR-based extractive summarization.
\item[d)] Rule-based sentence extraction.
\end{itemize}


\item What is a potential ethical concern associated with automated summarization?
\begin{itemize}
\item[a)] High computational costs.
\item[b)] Misrepresentation due to biased summaries.
\item[c)] Excessive redundancy in outputs.
\item[d)] The inability to process large documents.
\end{itemize}


\item What is a primary challenge in multilingual summarization?
\begin{itemize}
\item[a)] High training data requirements for each language.
\item[b)] The inability to use ROUGE for evaluation.
\item[c)] Over-reliance on extractive summarization techniques.
\item[d)] Lack of similarity metrics across languages.
\end{itemize}


\item Which summarization technique is most suited for real-time applications?
\begin{itemize}
\item[a)] Extractive summarization using precomputed vectors.
\item[b)] Abstractive summarization with recurrent neural networks.
\item[c)] Online summarization algorithms.
\item[d)] Summarization using pre-trained large language models.
\end{itemize}


\item What distinguishes neural approaches to summarization from traditional techniques?
\begin{itemize}
\item[a)] Neural approaches require no evaluation metrics.
\item[b)] Neural methods can learn contextual and semantic relationships.
\item[c)] Neural methods are only suitable for extractive tasks.
\item[d)] Neural approaches do not rely on training data.
\end{itemize}


\item Why is summarization important in fields like financial analysis?
\begin{itemize}
\item[a)] It automates trading decisions.
\item[b)] It consolidates vast text and data into human-readable formats.
\item[c)] It replaces traditional reporting systems.
\item[d)] It eliminates the need for manual data labeling.
\end{itemize}


\item Which metric is commonly used for abstractive summarization evaluation?
\begin{itemize}
\item[a)] Mean Reciprocal Rank (MRR)
\item[b)] ROUGE
\item[c)] Jaccard Index
\item[d)] KL-Divergence
\end{itemize}

\end{enumerate}

\end{multicols}

\newpage
\section*{Answer Key}

\begin{multicols}{8}
\textbf{1. c)} \\
\textbf{2. b)} \\
\textbf{3. b)} \\
\textbf{4. b)} \\
\textbf{5. b)} \\
\textbf{6. a)} \\
\textbf{7. b)} \\
\textbf{8. c)} \\
\textbf{9. b)} \\
\textbf{10. b)} \\
\textbf{11. b)} \\
\textbf{12. b)} \\
\textbf{13. b)} \\
\textbf{14. b)} \\
\textbf{15. b)} \\
\textbf{16. b)} \\
\textbf{17. b)} \\
\textbf{18. b)} \\
\textbf{19. b)} \\
\textbf{20. a)} \\
\textbf{21. b)} \\
\textbf{22. c)} \\
\textbf{23. a)} \\
\textbf{24. b)} \\
\textbf{25. a)} \\
\textbf{26. b)} \\
\textbf{27. c)} \\
\textbf{28. a)} \\
\textbf{29. b)} \\
\textbf{30. b)} \\
\textbf{31. b)} \\
\textbf{32. b)} \\
\textbf{33. c)} \\
\textbf{34. b)} \\
\textbf{35. b)} \\
\textbf{36. b)} \\
\textbf{37. b)} \\
\textbf{38. b)} \\
\textbf{39. c)} \\
\textbf{40. b)} \\
\textbf{41. b)} \\
\textbf{42. b)} \\
\textbf{43. b)} \\
\textbf{44. b)} \\
\textbf{45. b)} \\
\textbf{46. c)} \\
\textbf{47. b)} \\
\textbf{48. b)} \\
\textbf{49. b)} \\
\textbf{50. b)} \\
\textbf{51. c)} \\
\textbf{52. b)} \\
\textbf{53. b)} \\
\textbf{54. b)} \\
\textbf{55. b)} \\
\textbf{56. b)} \\
\textbf{57. c)} \\
\textbf{58. c)} \\
\textbf{59. b)} \\
\textbf{60. c)} \\
\textbf{61. a)} \\
\textbf{62. b)} \\
\textbf{63. b)} \\
\textbf{64. b)} \\
\textbf{65. b)} \\
\textbf{66. c)} \\
\textbf{67. b)} \\
\textbf{68. c)} \\
\textbf{69. b)} \\
\textbf{70. b)} \\
\textbf{71. b)} \\
\textbf{72. c)} \\
\textbf{73. c)} \\
\textbf{74. a)} \\
\textbf{75. b)} \\
\textbf{76. c)} \\
\textbf{77. b)} \\
\textbf{78. a)} \\
\textbf{79. b)} \\
\textbf{80. c)} \\
\textbf{81. b)} \\
\textbf{82. b)} \\
\textbf{83. c)} \\
\textbf{84. b)} \\
\textbf{85. a)} \\
\textbf{86. b)} \\
\textbf{87. b)} \\
\textbf{88. c)} \\
\textbf{89. b)} \\
\textbf{90. b)} \\
\textbf{91. c)} \\
\textbf{92. b)} \\
\textbf{93. b)} \\
\textbf{94. a)} \\
\textbf{95. b)} \\
\textbf{96. b)} \\
\textbf{97. b)} \\
\textbf{98. b)} \\
\textbf{99. c)} \\
\textbf{100. a)} \\
\textbf{101. b)} \\
\textbf{102. b)} \\
\textbf{103. b)} \\
\textbf{104. b)} \\
\textbf{105. b)} \\
\textbf{106. b)} \\
\textbf{107. b)} \\
\textbf{108. b)} \\
\textbf{109. b)} \\
\textbf{110. c)} \\
\textbf{111. b)} \\
\textbf{112. b)} \\
\textbf{113. b)} \\
\textbf{114. a)} \\
\textbf{115. b)} \\ 
\textbf{116. b)} \\
\textbf{117. c)} \\
\textbf{118. b)} \\
\textbf{119. b)} \\
\textbf{120. a)} \\
\textbf{121. b)} \\
\textbf{122. b)} \\
\textbf{123. b)} \\
\textbf{124. b)} \\
\textbf{125. b)} \\
\textbf{126. a)} \\
\textbf{127. b)} \\
\textbf{128. c)} \\
\textbf{129. b)} \\
\textbf{130. d)} \\
\textbf{131. b)} \\
\textbf{132. b)} \\
\textbf{133. c)} \\
\textbf{134. a)} \\
\textbf{135. c)} \\
\textbf{136. b)} \\
\textbf{137. b)} \\
\textbf{138. b)} \\
\textbf{139. b)} \\
\textbf{140. c)} \\
\textbf{141. b)} \\
\textbf{142. b)} \\
\textbf{143. a)} \\
\textbf{144. b)} \\
\textbf{145. b)} \\
\textbf{146. a)} \\
\textbf{147. b)} \\
\textbf{148. b)} \\
\textbf{149. b)} \\
\textbf{150. b)} \\
\textbf{151. a)} \\
\textbf{152. b)} \\
\textbf{153. b)} \\
\textbf{154. b)} \\
\textbf{155. c)} \\
\textbf{156. b)} \\
\textbf{157. b)} \\
\textbf{158. c)} \\
\textbf{159. b)} \\
\textbf{160. b)} \\
\textbf{161. b)} \\
\textbf{162. c)} \\
\textbf{163. b)} \\
\textbf{164. b)} \\
\textbf{165. b)} \\
\textbf{166. a)} \\
\textbf{167. b)} \\
\textbf{168. c)} \\
\textbf{169. c)} \\
\textbf{170. b)} \\
\textbf{171. b)} \\
\textbf{172. a)} \\
\textbf{173. b)} \\
\textbf{174. c)} \\
\textbf{175. b)} \\
\textbf{176. b)} \\
\textbf{177. a)} \\
\textbf{178. a)} \\
\textbf{179. a)} \\
\textbf{180. b)} \\
\textbf{181. b)} \\
\textbf{182. b)} \\
\textbf{183. a)} \\
\textbf{184. d)} \\
\textbf{185. a)} \\
\textbf{186. b)} \\
\textbf{187. a)} \\
\textbf{188. b)} \\
\textbf{189. b)} \\
\textbf{190. a)} \\
\textbf{191. b)} \\
\textbf{192. a)} \\
\textbf{193. c)} \\
\textbf{194. b)} \\
\textbf{195. a)} \\
\textbf{196. b)} \\
\textbf{197. b)} \\
\textbf{198. b)} \\
\textbf{199. b)} \\
\textbf{200. a)} \\
\textbf{201. c)} \\
\textbf{202. b)} \\
\textbf{203. b)} \\
\textbf{204. c)} \\
\textbf{205. b)} \\
\textbf{206. c)} \\
\textbf{207. a)} \\
\textbf{208. c)} \\
\textbf{209. b)} \\
\textbf{210. a)} \\
\textbf{211. a)} \\
\textbf{212. a)} \\
\textbf{213. b)} \\
\textbf{214. b)} \\
\textbf{215. c)} \\
\textbf{216. b)} \\
\textbf{217. a)} \\
\textbf{218. a)} \\
\textbf{219. b)} \\
\textbf{220. b)} \\
\textbf{221. b)} \\
\textbf{222. a)} \\
\textbf{223. b)} \\
\textbf{224. a)} \\
\textbf{225. b)} \\
\textbf{226. a)} \\
\textbf{227. b)} \\
\textbf{228. b)} \\
\textbf{229. b)} \\
\textbf{230. a)} \\
\textbf{231. a)} \\
\textbf{232. b)} \\
\textbf{233. b)} \\
\textbf{234. b)} \\
\textbf{235. a)} \\
\textbf{236. b)} \\
\textbf{237. a)} \\
\textbf{238. d)} \\
\textbf{239. a)} \\
\textbf{240. a)} \\
\textbf{241. b)} \\
\textbf{242. b)} \\
\textbf{243. a)} \\
\textbf{244. a)} \\
\textbf{245. a)} \\
\textbf{246. c)} \\
\textbf{247. b)} \\
\textbf{248. b)} \\
\textbf{249. c)} \\
\textbf{250. c)} \\
\textbf{251. b)} \\
\textbf{252. b)} \\
\textbf{253. b)} \\
\textbf{254. a)} \\
\textbf{255. b)} \\
\textbf{256. b)} \\
\textbf{257. a)} \\
\textbf{258. b)} \\
\textbf{259. b)} \\
\textbf{260. b)} \\
\textbf{261. b)} \\
\textbf{262. c)} \\
\textbf{263. d)} \\
\textbf{264. b)} \\
\textbf{265. b)} \\
\textbf{266. b)} \\
\textbf{267. b)} \\
\textbf{268. b)} \\
\textbf{269. b)} \\
\textbf{270. b)} \\
\textbf{271. a)} \\
\textbf{272. c)} \\
\textbf{273. b)} \\
\textbf{274. b)} \\
\textbf{275. a)} \\
\textbf{276. b)} \\
\textbf{277. c)} \\
\textbf{278. b)} \\
\textbf{279. b)} \\
\textbf{280. a)} \\
\textbf{281. b)} \\
\textbf{282. b)} \\
\textbf{283. b)} \\
\textbf{284. a)} \\
\textbf{285. b)} \\
\textbf{286. b)} \\
\textbf{287. b)} \\
\textbf{288. c)} \\
\textbf{289. a)} \\
\textbf{290. b)} \\
\textbf{291. b)} \\
\textbf{292. b)} \\
\textbf{293. b)} \\
\textbf{294. b)} \\
\textbf{295. b)} \\
\textbf{296. b)} \\
\textbf{297. b)} \\
\textbf{298. b)} \\
\textbf{299. b)} \\
\textbf{300. b)} \\
\textbf{301. b)} \\
\textbf{302. c)} \\
\textbf{303. b)} \\
\textbf{304. c)} \\
\textbf{305. b)} \\
\textbf{306. c)} \\
\textbf{307. a)} \\
\textbf{308. b)} \\
\textbf{309. b)} \\
\textbf{310. b)} \\
\textbf{311. a)} \\
\textbf{312. b)} \\
\textbf{313. b)} \\
\textbf{314. a)} \\
\textbf{315. b)} \\
\textbf{316. c)} \\
\textbf{317. b)} \\
\textbf{318. b)} \\
\textbf{319. b)} \\
\textbf{320. b)} \\
\textbf{321. b)} \\
\textbf{322. c)} \\
\textbf{323. c)} \\
\textbf{324. b)} \\
\textbf{325. b)} \\
\textbf{326. b)} \\
\textbf{327. b)} \\
\textbf{328. b)} \\
\textbf{329. b)} \\
\textbf{330. b)} \\
\textbf{331. b)} \\
\textbf{332. b)} \\
\textbf{333. b)} \\
\textbf{334. b)} \\
\textbf{335. b)} \\
\textbf{336. b)} \\
\textbf{337. a)} \\
\textbf{338. b)} \\
\textbf{339. b)} \\
\textbf{340. a)} \\
\textbf{341. b)} \\
\textbf{342. b)} \\
\textbf{343. b)} \\
\textbf{344. b)} \\
\textbf{345. b)} \\
\textbf{346. b)} \\
\textbf{347. c)} \\
\textbf{348. b)} \\
\textbf{349. b)} \\
\textbf{350. b)} \\
\textbf{351. b)} \\
\textbf{352. c)} \\
\textbf{353. b)} \\
\textbf{354. b)} \\
\textbf{355. b)} \\
\textbf{356. b)} \\
\textbf{357. c)} \\
\textbf{358. b)} \\
\textbf{359. b)} \\
\textbf{360. d)} \\
\textbf{361. b)} \\
\textbf{362. d)} \\
\textbf{363. b)} \\
\textbf{364. c)} \\
\textbf{365. b)} \\
\textbf{366. b)} \\
\textbf{367. c)} \\
\textbf{368. b)} \\
\textbf{369. c)} \\
\textbf{370. a)} \\
\textbf{371. b)} \\
\textbf{372. b)} \\
\textbf{373. c)} \\
\textbf{374. a)} \\
\textbf{375. c)} \\
\textbf{376. b)} \\
\textbf{377. a)} \\
\textbf{378. a)} \\
\textbf{379. b)} \\
\textbf{380. a)} \\
\textbf{381. c)} \\
\textbf{382. c)} \\
\textbf{383. a)} \\
\textbf{384. c)} \\
\textbf{385. b)} \\
\textbf{386. b)} \\
\textbf{387. b)} \\
\textbf{388. b)} \\
\textbf{389. c)} \\
\textbf{390. c)} \\
\textbf{391. b)} \\
\textbf{392. b)} \\
\textbf{393. c)} \\
\textbf{394. c)} \\
\textbf{395. a)} \\
\textbf{396. c)} \\
\textbf{397. b)} \\
\textbf{398. a)} \\
\textbf{399. b)} \\
\textbf{400. b)} \\
\textbf{401. c)} \\
\textbf{402. b)} \\
\textbf{403. c)} \\
\textbf{404. a)} \\
\textbf{405. c)} \\
\textbf{406. c)} \\
\textbf{407. b)} \\
\textbf{408. b)} \\
\textbf{409. b)} \\
\textbf{410. b)} \\
\textbf{411. b)} \\
\textbf{412. b)} \\
\textbf{413. a)} \\
\textbf{414. b)} \\
\textbf{415. c)} \\
\textbf{416. b)} \\
\textbf{417. b)} \\
\textbf{418. c)} \\
\textbf{419. b)} \\
\textbf{420. b)} \\
\textbf{421. b)} \\
\textbf{422. b)} \\
\textbf{423. b)} \\
\textbf{424. a)} \\
\textbf{425. c)} \\
\textbf{426. b)} \\
\textbf{427. b)} \\
\textbf{428. b)} 
\end{multicols}

\begin{comment}
b, c, b, b, a, b, b, b, b, b, a, b, c, b, d, b, b, c, a, c, b, b, b, b, c, b, b, a, b, b, a, b, b, b, b, a, b, b, b, c, b, b, c, b, b, b, c, b, b, b, a, b, c, c, b, b, a, b, c, b, b, a, a, a, b, b, b, a, d, a, b, a, b, b, a, b, a, c, b, a, b, b, b, b, a, c, b, b, c, b, c, a, c, b, a, a, a, b, b, c, b, a, a, b, b, b, a, b, a, b, a, b, b, b, a, a, b, b, b, a, b, a, d, a, a, b, b, a, a, a, c, b, b, c, c, b, b, b, a, b, b, a, b, b, b, b, c, d, b, b, b, b, b, b, b, a, c, b, b, a, b, c, b, b, a, b, b, b, a, b, b, b, c, a, b, b, b, b, b, b, b, b, b, b, b, b, c, b, c, b, c, a, b, b, b, a, b, b, a, b, c, b, b, b, b, b, c, c, b, b, b, b, b, b, b, b, b, b, b, b, b, a, b, b, a, b, b, b, b, b, b, c, b, b, b, b, c, b, b, b, b, c, b, b, d, b, d, b, c, b, b, c, b, c, a, b, b, c, a, c, b, a, a, b, a, c, c, a, c, b, b, b, b, c, c, b, b, c, c, a, c, b, a, b, b, c, b, c, a, c, c, b, b, b, b, b, b, a, b, c, b, b, c, b, b, b, b, b, a, c, b, b, b 
\end{comment}

\end{document}