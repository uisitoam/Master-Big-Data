\chapter{Representaciones avanzadas de texto}\label{Chapter6} 
% chktex-file 8
% chktex-file 12
% chktex-file 13
% chktex-file 44

HAce 10 años empezo una revolucion en el campo de la representacion de texto con Word2vec. Pasamos de representaciones sprse a representaciones densas de mebeddngs que capturan mejor la semantica y significado del texto. 

CHATGPT tiene un impacto social enorme


Esta revolucion que cambia y por que cambia. Lo tradicional en claisifcacion automatica, el paradigma, se tiene un traing data etiqueta, se lo doy al sistema aprende y puedo poner en produccion. El cuello de botella es que necesito etiquetas en el training data. Da capacidades limitadas de compresion de lenguaje, ya que para ello tendria que meter gran cantidad de formas de decir lo mismo, incluir toda la casuistica.

Primera vuelta de tuerca, preentrenar y finetune. LOs modelos se preentrenan: cogemos una gran cantidad de datos (no toda la web pero casi) y jugando al juego de ocultar palabras (haciendo masking) y predecir la palabra oculta. Si lo dice mal, la penalizo. Si lo hace bien, consigo un modelo que entiende bien el lenguaje, la semantica, otros tipos de conocimientos sobre el mundo. Esto es autoentrenamiento sin etiquetado.  


Una vez entrenado, lo ajusto y personalizo para mi tarea con el fine tune (adaptation). 





La idea clave de los embbeding es construir automaticamente un representacion de las palabras donde las palabras que son similares conceptualemente estan cerca en el espacio vectorial. Los alrededores de una palabra (izquierda y derecha) son los que determinan el significado de la palabra (ejemplo de tezgüino). Las palabras parecidad van a tener contextos izquierdo y derecho parecidos. Esta es la base teórica para la integración semántica de la representación de las palabras. \\

Un word embedding es una lista, un vector denso de longitud fija. PAra construirlas se puede hacer de muchas formas: por ejemplo, estadísticas de coocurrencia. Esto es un espacio de bajas dimensiones. Muchas veces se aprenden con redes de neuronas. \\

Para que sirve esto? si yo tengo los embeddings de todas las palabras se pueden buscar las palabras cercanas en el espacio embebido para buscar sinonimos, por ejemplo. 

Una estrategia posible es que los embeddings sea la entrada a una red de neuronas para entrenamiento supervisada. Tambien podemos representar el corpus con los embeddings y ver los posibles clusters. 

Los vectores de las palabras admiten operaciones !

Los redes de neuronas. Se coge una red de neuronas y la hago jugar a un juego predictivo de ir escondiendo palabras y que vaya adivinando. Aqui el proceso de backpropagation juega un papel fundamental. Queremos que palabras que significan lo mismo vayan a la misma representación. Otras alternativas son mas algebraicas, usando cuentas de coocurrencia 


Predecir la palabra j-esima dadas las anteriores (tipo gpt). Una red de neuronas que hago esto bien es util. En el ambito de los LLM da un gran conocimiento del mundo, tanto semantico como sintactico y gramatical. 

estos ebeddings no es que se produjera la red de neuronas para darlos, se encontro que era un subproducto de la red de neuronas.

VEamos varios ejemplos

NNLM: Contruyo una red de neuronas para predecir una palabra i esima dada las anteriores y el output layer es la probabilidad de cada palabra en el vocabulario dadas las anteriores. Internamente representa los embeddings del texto. Los mebeddigs son los pesos de la capa de entrada a la capa oculta. (la e simboliza embedding) x coge la representacion de las embeddings, lo mete en una capa interna (h).


WORD2VEC. Embeddings independietnes del contexto, es decir, palabras polisemicas tienen un solo embedding para todos los significados, aquí estas palabras sufren. Esto lo hace leyendo un corpus masivo y de ese (auto)aprendizaje, porque no etiquetamos nada, aprende las palabras. Una forma de predecir es CBOW , predecir una palabra dando las anteriores y posteriores.... Da una representacion distribuida. DEF. 

BERT (la B de Bert es de bidireccional). Embeddings dependiente del contexto, para palabras polisemicas, da un embedding diferente a cada significado.

BERT solo hace encoding (hace todo lo necesario para crear una representación interna del texto), GPT solo decoding (me pasas la representacion al lenguaje que te pido) y otros tienen ambas, por ejemplo T5. CHATGPT ademas esta tuneado para seguir instrucciones; luego se adapta a conversar y genera una representacion de la isntruccion y producirla en el lenguaje que se pida. 


T5: sequence-to-sequence. en funcion e cierto patron de entrada produce cierto aprametro de salida. T5 se utilizan para reranking (ranking hecho con un BM25 o algo clasico), pero con T5 es muy costoso. El T5 aprende de un proceso de enmascaramiento (preeentraenamiento). Mono T5 esta finetuneado con MSMarco, coleccion de pasajes que puso bing. 

count-based models. LSA
No basados en redes neuronales pero igualmente son capaces de generar representaciones densas. La idea del LSA: sabemos el problema de las representaciones sparse (dos sinonimos no guardan relacion) idealmente no querria indexar por palabras, sino por conceptos. El problema es que los conceptos no son explicitos. LSA usa la matriz originales y transformarla a un espacio de conceptos usando una estrategia de coocurrencia cpn metodos algebraicos. 

HAL:  busca todos los contextos (ventanas de aparicion de las palabras en los corpus) y analiza la cuenta de coocurrencia entre esa palabra y los contextos. 

COAL: poco detalle

LR-MVL: un poco mas moderno. Si tiene en cuenta contexto izquierdo y derecho. 

GLOVE: mejora sobre estrategias como Word2Vec (problema mira las apariciones de las palabras en el corpus, no usa informacion global). GLOVE tiene en cuenta informacion de estadísticas globales, no en las coocurrencias individuales sino globales.

FINAL REMARKS. CHALLENGES.

OOV- muchos modelos de embeddings no gestionan palabras no vistas anteriormente. 