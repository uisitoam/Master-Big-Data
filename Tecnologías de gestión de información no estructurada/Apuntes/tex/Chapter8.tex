\chapter{Web search}\label{Chapter8} 
% chktex-file 8
% chktex-file 12
% chktex-file 13
% chktex-file 44

Los motores de búsqueda web son una de las aplicaciones más importantes de la recuperación de texto. Aunque muchos algoritmos de recuperación de información se desarrollaron antes del nacimiento de la web, esta creó la mejor oportunidad para aplicar esos algoritmos a un problema de aplicación importante que a todos les importa. Naturalmente, hubo que hacer algunas extensiones adicionales de los algoritmos de búsqueda clásicos para abordar completamente los nuevos desafíos encontrados en la búsqueda web.

Primero, este es un desafío de escalabilidad. ¿Cómo podemos manejar el tamaño de la web y asegurar la cobertura completa de toda su información (ya sea textual o no)? ¿Cómo podemos servir a muchos usuarios rápidamente respondiendo todas sus consultas? Antes del nacimiento de la web, la escala de búsqueda era relativamente pequeña, generalmente enfocada en bibliotecas, por lo que estas preguntas no eran serias.

El segundo problema es que hay mucha información de baja calidad conocida como spam. La optimización de motores de búsqueda es el intento de aumentar el rango de una página en particular aprovechando cómo se puntúan las páginas, por ejemplo, agregando muchas palabras que no son necesariamente relevantes para el contenido real o creando muchos enlaces falsos a una página en particular para hacer que parezca más popular de lo que realmente es. Se han diseñado muchos enfoques diferentes para detectar y prevenir tales prácticas de spam [Spirin y Han 2012].

El tercer desafío es la naturaleza dinámica de la web. Se crean y actualizan nuevas páginas muy rápidamente. Esto hace que sea más difícil mantener el índice fresco con el contenido más reciente. Estos son solo algunos de los desafíos que tenemos que resolver para construir un motor de búsqueda web de alta calidad. A pesar de estos desafíos, también hay algunas oportunidades interesantes que podemos aprovechar para mejorar los resultados de búsqueda. Por ejemplo, podemos imaginar que usar enlaces entre páginas puede mejorar la puntuación.

Los algoritmos de los que hablamos, como el modelo de espacio vectorial, son generales: pueden aplicarse a cualquier aplicación de búsqueda. Por otro lado, tampoco aprovechan las características especiales de las páginas o documentos en aplicaciones específicas como la búsqueda web. Debido a estos desafíos y oportunidades, hay nuevas técnicas que se han desarrollado específicamente para la búsqueda web. Una de estas técnicas es la indexación y búsqueda paralela. Esto aborda el problema de la escalabilidad; en particular, el marco MapReduce de Google es muy influyente.

También hay técnicas que se han desarrollado para abordar el problema del spam. Tendremos que evitar que esas páginas de spam sean clasificadas en los primeros lugares. También hay técnicas para lograr una clasificación robusta a la luz de los optimizadores de motores de búsqueda. Vamos a usar una amplia variedad de señales para clasificar las páginas de modo que no sea fácil engañar al motor de búsqueda con un truco en particular.

La tercera línea de técnicas es el análisis de enlaces; estas son técnicas que pueden permitirnos mejorar los resultados de búsqueda aprovechando información adicional sobre la naturaleza en red de la web. Por supuesto, utilizaremos múltiples características para la clasificación, no solo el análisis de enlaces. También podemos explotar todo tipo de características como el diseño de las páginas web o el texto ancla que describe un enlace a otra página.

El primer componente de un motor de búsqueda web es el rastreador. Este es un programa que descarga el contenido de las páginas web que deseamos buscar. El segundo componente es el indexador, que tomará estas páginas descargadas y creará un índice invertido. El tercer componente es la recuperación, que responde a la consulta de un usuario hablando con el navegador del usuario. El navegador mostrará los resultados de la búsqueda y permitirá al usuario interactuar con la web. Estas interacciones con el usuario permiten oportunidades para retroalimentación (discutido en el Capítulo 7) y evaluación (discutido en el Capítulo 9). En la siguiente sección, discutiremos el rastreo. Ya hemos descrito todos los pasos de indexación excepto el rastreo en detalle en el Capítulo 8.

Después de nuestra discusión sobre el rastreo, pasamos a los desafíos particulares de la indexación web. Luego, discutimos cómo podemos aprovechar los enlaces entre páginas en el análisis de enlaces. La última técnica que discutimos es el aprendizaje para clasificar, que es una forma de combinar muchas características diferentes para la clasificación.

\section{Web Crawling}

Ejemplo: la USC le pone delay a las peticiones de google 

El rastreador también se llama araña o robot de software que rastrea (recorre, analiza y descarga) páginas en la web. Construir un rastreador de juguete es relativamente fácil porque solo necesitas comenzar con un conjunto de páginas semilla, obtener páginas de la web y analizar los nuevos enlaces de estas páginas. Luego los agregamos a una cola y luego exploramos los enlaces de esas páginas en una búsqueda en anchura hasta que estemos satisfechos.

Construir un rastreador real es bastante complicado y hay algunos problemas complejos con los que inevitablemente tenemos que lidiar. Un problema es la robustez: ¿Qué pasa si el servidor no responde o devuelve basura que no se puede analizar? ¿Qué pasa si hay una trampa que genera páginas dinámicamente que atraen a tu rastreador a seguir rastreando el mismo sitio en círculos? Otro problema es que no queremos sobrecargar un servidor en particular con demasiadas solicitudes de rastreo. Eso puede causar que el sitio experimente una denegación de servicio; algunos sitios también bloquearán direcciones IP que creen que los están rastreando o creando demasiadas solicitudes. De manera similar, un rastreador debe respetar el protocolo de exclusión de robots. Un archivo llamado robots.txt en la raíz del sitio le dice a los rastreadores qué rutas no tienen permitido rastrear. También necesitas manejar diferentes tipos de archivos como imágenes, PDFs o cualquier otro tipo de formatos en la web que contengan información útil para tu aplicación de búsqueda. Idealmente, el rastreador debería reconocer páginas duplicadas para no repetirse o quedar atrapado en un bucle. Finalmente, puede ser útil descubrir URLs ocultas; estas son URLs que pueden no estar vinculadas desde ninguna página pero que aún contienen contenido que te gustaría indexar.

Entonces, ¿cuáles son las principales estrategias de rastreo? En general, la búsqueda en anchura es la más común porque equilibra naturalmente la carga del servidor. El rastreo paralelo también es muy natural porque esta tarea es muy fácil de paralelizar. Una variación interesante se llama rastreo enfocado. Aquí, vamos a rastrear algunas páginas sobre un tema en particular, por ejemplo, todas las páginas sobre automóviles. Esto generalmente comenzará con una consulta que usas para obtener algunos resultados. Luego, rastreas gradualmente más. Una versión aún más extrema del rastreo enfocado es (por ejemplo) descargar e indexar todas las publicaciones de un foro en particular. En este caso, podríamos tener una URL como http://www.text-data-book-forum.com/boards?id=3 que se refiere a la tercera publicación en el foro. Al cambiar el parámetro id, podemos iterar a través de todas las publicaciones del foro e indexarlas bastante fácilmente. En este escenario, es especialmente importante agregar un retraso entre las solicitudes para que el servidor no se vea abrumado.

Otro desafío en el rastreo es encontrar nuevas páginas que se hayan creado desde la última vez que se ejecutó el rastreador. Esto es muy desafiante si las nuevas páginas no han sido vinculadas a ninguna página antigua. Si lo están, entonces probablemente puedas encontrarlas siguiendo los enlaces desde las páginas existentes en tu índice.

Finalmente, podríamos enfrentar el escenario de rastreo incremental o rastreo repetido. Supongamos que quieres poder crear un motor de búsqueda web. Claramente, primero rastreas datos de la web. En el futuro, solo necesitamos rastrear las páginas actualizadas. Esta es una pregunta de investigación muy interesante: ¿cómo podemos determinar cuándo una página necesita ser rastreada nuevamente (o incluso cuándo se ha creado una nueva página)? Hay dos factores principales a considerar aquí, el primero de los cuales es si una página en particular se actualizaría con frecuencia. Si la página es una página estática que no ha cambiado durante meses, probablemente no sea necesario volver a rastrearla todos los días, ya que es poco probable que cambie con frecuencia. Por otro lado, si es (por ejemplo) una página de resultados deportivos que se actualiza con mucha frecuencia, es posible que necesites volver a rastrearla incluso varias veces en el mismo día. El segundo factor a considerar es con qué frecuencia una página en particular es accedida por los usuarios del sistema de motores de búsqueda. Si es una página de alta utilidad, es más importante asegurarse de que esté actualizada. Compáralo con otra página que nunca ha sido buscada por ningún usuario durante un año; aunque esa página impopular haya cambiado mucho, probablemente no sea necesario rastrear esa página, o al menos no es tan urgente, para mantener su frescura.

\section{Web Indexing}
En esta sección, discutiremos cómo crear un índice a escala web. Después de que nuestro rastreador entregue gigabytes o terabytes de datos, el siguiente paso es usar el indexador para crear el índice invertido. En general, podemos usar las técnicas estándar de recuperación de información para crear el índice, pero hay nuevos desafíos que tenemos que resolver para la indexación a escala web. Los dos principales desafíos son la escalabilidad y la eficiencia.

El índice será tan grande que no puede caber en una sola máquina o en un solo disco, por lo que tenemos que almacenar los datos en múltiples máquinas. Además, debido a que los datos son tan grandes, es beneficioso procesar los datos en paralelo para que podamos producir el índice rápidamente. Para abordar estos desafíos, Google ha realizado una serie de innovaciones. Una es el Sistema de Archivos de Google (GFS), que es un sistema de archivos distribuido general que puede ayudar a los programadores a gestionar archivos almacenados en un clúster de máquinas. La segunda es MapReduce, que es un marco de software general para soportar la computación paralela. Hadoop es la implementación de código abierto más conocida de MapReduce, ahora utilizada en muchas aplicaciones.

La Figura 10.1 muestra la arquitectura del Sistema de Archivos de Google (GFS). Utiliza un mecanismo de gestión centralizado muy simple para gestionar todas las ubicaciones específicas de los archivos. Es decir, mantiene un espacio de nombres de archivos y una tabla de búsqueda para saber exactamente dónde se almacena cada archivo. El cliente de la aplicación habla con el nodo maestro de GFS, que obtiene ubicaciones específicas de los archivos para procesar. Este sistema de archivos almacena sus archivos en máquinas en fragmentos de tamaño fijo; cada archivo de datos se separa en muchos fragmentos de 64 MB. Estos fragmentos se replican para garantizar la fiabilidad. Todos estos detalles son algo de lo que el programador no tiene que preocuparse, y todo está a cargo de este sistema de archivos. Desde la perspectiva de la aplicación, el programador vería un archivo normal. El programa no tiene que saber exactamente dónde está almacenado, y puede simplemente invocar operadores de alto nivel para procesar el archivo. Otra característica es que la transferencia de datos es directamente entre la aplicación y los servidores de fragmentos, por lo que también es eficiente en este sentido.

Encima del GFS, Google propuso MapReduce como un marco general para la programación paralela. Esto soporta tareas como la construcción de un índice invertido. Al igual que GFS, este marco oculta características de bajo nivel del programador. Como resultado, el programador puede hacer un esfuerzo mínimo para crear una aplicación que se pueda ejecutar en un gran clúster en paralelo. Algunos de los detalles de bajo nivel ocultos en el marco son las comunicaciones, el balanceo de carga y la ejecución de tareas. La tolerancia a fallos también está incorporada; si un servidor se cae, algunas tareas pueden no completarse. Aquí, el mecanismo de MapReduce sabría que la tarea no se ha completado y automáticamente asignaría la tarea a otros servidores que puedan hacer el trabajo. Nuevamente, el programador no tiene que preocuparse por esto.

En MapReduce, los datos de entrada se separan en varios pares (clave, valor). Lo que exactamente es el valor dependerá de los datos. Cada par se enviará a una función de mapa que escribe el programador. La función de mapa procesará estos pares (clave, valor) y generará varios otros pares (clave, valor). Por supuesto, la nueva clave suele ser diferente de la clave antigua que se da al mapa como entrada. Todas las salidas de todas las llamadas al mapa se recopilan y ordenan en función de la clave. El resultado es que todos los valores que están asociados con la misma clave se agruparán. Para cada clave única, ahora tenemos un conjunto de valores que están adjuntos a esta clave. Estos son los datos que se envían a la función de reducción. Cada instancia de reducción manejará una clave diferente. Esta función procesa su entrada, que es una clave y un conjunto de valores, para producir otro conjunto de pares (clave, valor) como salida. Este es el marco general de MapReduce. Ahora, el programador solo necesita escribir la función de mapa y la función de reducción. Todo lo demás está a cargo del marco MapReduce. Con un marco así, los datos de entrada se pueden dividir en múltiples partes que se procesan en paralelo primero por el mapa, y luego se procesan nuevamente en paralelo una vez que llegamos a la etapa de reducción.

La Figura 10.3 muestra un ejemplo de conteo de palabras. La entrada son archivos que contienen palabras tokenizadas y la salida que queremos generar es el número de ocurrencias de cada palabra. Este tipo de conteo sería útil para evaluar la popularidad de una palabra en una gran colección o lograr un efecto de ponderación IDF para la búsqueda. Entonces, ¿cómo podemos resolver este problema? Un pensamiento natural es que esta tarea se puede hacer en paralelo simplemente contando diferentes partes del archivo en paralelo y combinando todos los conteos. Esa es precisamente la idea de lo que podemos hacer con MapReduce: podemos paralelizar líneas en este archivo de entrada. Más específicamente, podemos asumir que la entrada a cada función de mapa es un par (clave, valor) que representa el número de línea y la cadena en esa línea.

La primera línea es el par (1, HelloWorldByeWorld). Este par se enviará a una función de mapa que cuenta las palabras en esta línea. En este caso, solo hay cuatro palabras y cada palabra obtiene un conteo de uno. El pseudocódigo del mapa mostrado en la parte inferior de la figura es bastante simple. Simplemente necesita iterar sobre todas las palabras en esta línea, y luego simplemente llamar a una función Collect, lo que significa que luego enviaría la palabra y el contador al colector. El colector intentaría ordenar todos estos pares clave-valor de diferentes funciones de mapa. El programador especifica esta función como una forma de procesar cada parte de los datos. Por supuesto, la segunda línea será manejada por una instancia diferente de la función de mapa, que producirá una salida similar. Como se mencionó, el colector hará el agrupamiento o clasificación interna. En esta etapa, puedes ver que hemos recopilado múltiples pares. Cada par es una palabra y su conteo en la línea. Una vez que vemos todos estos pares, podemos ordenarlos en función de la clave, que es la palabra. Cada palabra ahora está adjunta a varios valores, es decir, varios conteos. Estos nuevos pares (clave, valor) se alimentarán a una función de reducción.

La Figura 10.4 muestra cómo la función de reducción termina el trabajo de contar las ocurrencias totales de esta palabra. Ya tiene estos conteos parciales, por lo que todo lo que necesita hacer es simplemente sumarlos. Tenemos un contador y luego iteramos sobre todas las palabras que vemos en esta matriz, como se muestra en el pseudocódigo en la parte inferior de la figura. Finalmente, salimos la clave y el conteo total, que es precisamente lo que queremos como salida de todo este programa. Como podemos ver, esto ya es muy similar a la construcción de un índice invertido; la salida aquí está indexada por una palabra, y tenemos un diccionario del vocabulario. Lo que falta son los IDs de los documentos y los conteos de frecuencia específicos de las palabras en cada documento en particular. Podemos modificar esto ligeramente para construir realmente un índice invertido en paralelo.

Modifiquemos nuestro ejemplo de conteo de palabras para crear un índice invertido. La Figura 10.5 ilustra este ejemplo. Ahora, asumimos que la entrada a la función de mapa es un par (clave, valor) donde la clave es un ID de documento y el valor denota el contenido de la cadena de todas las palabras en ese documento. La función de mapa hará algo muy similar a lo que hemos visto en el ejemplo anterior: simplemente agrupa todos los conteos de esta palabra en este documento, generando nuevos pares. En los nuevos pares, cada clave es una palabra y el valor es el conteo de esta palabra en este documento seguido del ID del documento. Más tarde, en el índice invertido, nos gustaría mantener esta información del ID del documento, por lo que la función de mapa realiza un seguimiento de ella.

Después de la función de mapa, hay un mecanismo de clasificación que agruparía las mismas palabras y alimentaría estos datos a la función de reducción. Vemos que la entrada de la función de reducción se parece a una entrada de índice invertido. Es solo la palabra y todos los documentos que contienen la palabra y la frecuencia de la palabra en esos documentos. Todo lo que necesitamos hacer es simplemente concatenarlos en un bloque continuo de datos, y esto se puede almacenar en el sistema de archivos. La función de reducción hará un trabajo muy mínimo. El Algoritmo 10.1 se puede usar para esta construcción de índice invertido.

El Algoritmo 10.1, adaptado de Lin y Dyer [2010], describe las funciones de mapa y reducción. Un programador especificaría estas dos funciones para ejecutarse en la parte superior de un clúster de MapReduce. Como se describió antes, el mapa cuenta las ocurrencias de una palabra usando un array asociativo (diccionario), y sale todos los conteos junto con el ID del documento. La función de reducción simplemente concatena toda la entrada que se le ha dado y como una sola entrada para esta clave de ID de documento. A pesar de su simplicidad, esta función de MapReduce nos permite construir un índice invertido a una escala muy grande. Los datos pueden ser procesados por diferentes máquinas y el programador no tiene que preocuparse por los detalles. Así es como podemos hacer la construcción de índices en paralelo para la búsqueda web.

Para resumir, la indexación a escala web requiere algunas nuevas técnicas que van más allá de las técnicas de indexación tradicionales estándar. Principalmente, tenemos que almacenar el índice en múltiples máquinas, y esto generalmente se hace utilizando un sistema de archivos distribuido como el GFS. En segundo lugar, requiere crear el índice en paralelo porque es muy grande. Esto se hace utilizando el marco MapReduce. Es importante tener en cuenta que tanto el GFS como el marco MapReduce son muy generales, por lo que también pueden soportar muchas otras aplicaciones además de la indexación.

\section{Link Analysis}

Si A referencia a B, y A es bueno, suponemos que B es bueno. El texto del enlace que pone A para B (anchor text) es muy importante (page rank): es lo que dicen otros, en resumen, de B. 

En esta sección, vamos a continuar nuestra discusión sobre la búsqueda web, particularmente enfocándonos en cómo utilizar los enlaces entre páginas para mejorar la búsqueda. En la sección anterior, hablamos sobre cómo crear un índice. 

Primero, en la web tendemos a tener necesidades de información muy diferentes. Por ejemplo, las personas pueden buscar una página web o una página de entrada, lo cual es diferente de la búsqueda tradicional en bibliotecas donde las personas están principalmente interesadas en recopilar información literaria. Este tipo de consultas a menudo se llaman consultas de navegación, donde el propósito es navegar hacia una página específica. Para tales consultas, podríamos beneficiarnos del uso de información de enlaces. Por ejemplo, las consultas de navegación podrían ser facebook o yahoo finance. El usuario simplemente está tratando de llegar a esas páginas sin escribir explícitamente la URL en la barra de direcciones del navegador.

En segundo lugar, los documentos web tienen mucha más información que el texto puro; hay organización jerárquica y anotaciones como el diseño de la página, el título o los hipervínculos a otras páginas. Estas características proporcionan una oportunidad para usar información de contexto adicional del documento para mejorar la puntuación. Finalmente, la calidad de la información varía mucho. Todo esto significa que debemos considerar muchos factores para mejorar el algoritmo de clasificación estándar, dándonos una forma más robusta de clasificar las páginas y haciendo más difícil para los spammers manipular una señal para mejorar la clasificación de una sola página.

Como resultado de todas estas preocupaciones, los investigadores han realizado una serie de extensiones importantes a los algoritmos de clasificación estándar. Una es explotar los enlaces para mejorar la puntuación, que es el tema principal de esta sección. También hay algoritmos para explotar la información de retroalimentación implícita a gran escala en forma de clics. Por supuesto, eso pertenece a la categoría de técnicas de retroalimentación, y las técnicas de aprendizaje automático se utilizan a menudo allí. En general, los algoritmos de clasificación de búsqueda web se basan en algoritmos de aprendizaje automático para combinar todo tipo de características. Muchos de ellos se basan en modelos estándar como BM25 que discutimos en el Capítulo 6. La información de enlaces es una de las características importantes utilizadas en las funciones de puntuación combinadas en los sistemas modernos de búsqueda web.

La Figura 10.6 muestra una instantánea de una parte de la web. Podemos ver que hay muchos enlaces que conectan diferentes páginas, y en el centro, hay una descripción de un enlace que apunta al documento en el lado derecho. Este texto de descripción se llama texto ancla. En realidad, es increíblemente útil para los motores de búsqueda porque proporciona una descripción adicional de la página a la que se apunta. Por ejemplo, si alguien quiere marcar la página principal de Amazon.com, la persona podría crear un enlace llamado la gran librería en línea que apunta a Amazon. La descripción es muy similar a lo que el usuario escribiría en el cuadro de búsqueda cuando busca una página así. Supongamos que alguien escribe una consulta como librería en línea o gran librería en línea. La consulta coincidiría con este texto ancla en la página. Esto en realidad proporciona evidencia para coincidir con la página a la que se ha apuntado: la página de entrada de Amazon. Por lo tanto, si coincides con el texto ancla que describe el enlace a una página, proporciona una buena evidencia de la relevancia de la página a la que se apunta.

En la parte inferior de la Figura 10.6, hay algunos patrones de enlaces que pueden indicar la utilidad de un documento. Por ejemplo, en el lado derecho puedes ver que una página ha recibido muchos enlaces entrantes, lo que significa que muchas otras páginas están apuntando a esta página. Esto muestra que esta página es bastante útil. En el lado izquierdo puedes ver una página que apunta a muchas otras páginas. Esta es una página central que te permitiría ver muchas otras páginas. Llamamos al primer caso una página de autoridad y al segundo caso una página de concentrador. Esto significa que la información de enlaces puede ayudar de dos maneras; una es proporcionar texto adicional para la coincidencia (en el caso de los anclajes) y la otra es proporcionar algunas puntuaciones adicionales para las páginas web para caracterizar la probabilidad de que una página sea un concentrador o una autoridad.

\subsection{PageRank}
PageRank de Google, una técnica principal que se utilizó originalmente para el análisis de enlaces, es un buen ejemplo de cómo aprovechar la información de enlaces de las páginas. PageRank captura la popularidad de la página, que es otra palabra para autoridad. La intuición es que los enlaces son como citas en la literatura. Piensa en una página que apunta a otra página; esto es muy similar a un artículo que cita a otro artículo. Por lo tanto, si una página es citada con frecuencia, podemos asumir que esta página es más útil. PageRank aprovecha esta intuición y la implementa de manera metódica. En su sentido más simple, PageRank es esencialmente contar citas o contar enlaces entrantes.

Mejora esta idea simple de dos maneras. Una es considerar las citas indirectas. Esto significa que no solo miras el número de enlaces entrantes, sino que también miras los enlaces entrantes de tus enlaces entrantes, de manera recursiva. Si tus enlaces entrantes tienen muchos enlaces entrantes, tu página obtiene crédito por eso. En resumen, si las páginas importantes te están apuntando, tú también debes ser importante. Por otro lado, si las páginas que te están apuntando no son apuntadas por muchas otras páginas, entonces no obtienes tanto crédito. Este es el concepto de citas indirectas, o citas en cascada.

Nuevamente, podemos entender esta idea considerando artículos de investigación. Si eres citado por diez artículos que no son muy influyentes, eso no es tan bueno como si fueras citado por diez artículos que ellos mismos han atraído muchas otras citas. Claramente, este es un caso en el que nos gustaría considerar enlaces indirectos, que es exactamente lo que hace PageRank. La otra idea es que es bueno suavizar las citas para acomodar posibles citas que aún no se han observado. Supongamos que cada página tiene un conteo de citas pseudo no nulo. Esencialmente, estás tratando de imaginar que hay muchos enlaces virtuales que enlazarán todas las páginas juntas para que realmente obtengas citas pseudo de todos.

Otra forma de entender PageRank es el concepto de un navegante aleatorio visitando cada página web. Veamos este ejemplo en detalle, ilustrado en la Figura 10.7. A la izquierda, hay un pequeño gráfico, donde cada documento d1, d2, d3 y d4 es una página web, y los bordes entre documentos son hipervínculos que los conectan entre sí. Supongamos que un navegante aleatorio o caminante aleatorio puede estar en cualquiera de estas páginas. Cuando el navegante aleatorio decide moverse a una página diferente, puede seguir aleatoriamente un enlace desde la página actual o elegir aleatoriamente un documento para saltar desde toda la colección. Entonces, si el navegante aleatorio está en d1, con cierta probabilidad ese navegante aleatorio seguirá los enlaces a d3 o d4. El modelo de navegación aleatoria también asume que el navegante podría aburrirse a veces y decidir ignorar los enlaces reales, saltando aleatoriamente a cualquier página en la web. Si el navegante toma esa opción, podría llegar a cualquiera de las otras páginas aunque no haya un enlace directo a esa página. Basado en este modelo, podemos hacer la pregunta: "¿Qué tan probable es, en promedio, que el navegante llegue a una página en particular?" Esta probabilidad es precisamente lo que calcula PageRank.

La puntuación de PageRank de un documento di es la probabilidad promedio de que el navegante visite di. Intuitivamente, esto debería ser proporcional al conteo de enlaces entrantes. Si una página tiene un alto número de enlaces entrantes, entonces tendría una mayor probabilidad de ser visitada, ya que habrá más oportunidades de que el navegante siga un enlace allí. Así es como el modelo de navegación aleatoria captura la idea de contar los enlaces entrantes. Pero, también considera los enlaces entrantes indirectos; si las páginas que apuntan a di tienen muchos enlaces entrantes, eso significaría que el navegante aleatorio probablemente llegaría a una de ellas. Esto aumenta la probabilidad de visitar di. Esta es una buena manera de capturar tanto los enlaces directos como los indirectos.

Matemáticamente, podemos representar esta red de documentos como una matriz M, mostrada en el centro de la Figura 10.7. Cada fila representa una página de inicio. Por ejemplo, la primera fila indicaría la probabilidad de ir a cualquiera de las cuatro páginas desde d1. Vemos que solo hay dos entradas no nulas. Cada una es la mitad, ya que d1 apunta solo a otras dos páginas; por lo tanto, si podemos elegir aleatoriamente visitar cualquiera de ellas desde d1, cada una tendría una probabilidad de 1/2. Tenemos ceros para las dos primeras columnas de d1 ya que d1 no se enlaza a sí mismo y no se enlaza a d2. Por lo tanto, Mij es la probabilidad de ir de di a dj. Los valores de cada fila deben sumar uno, porque el navegante tendrá que ir precisamente a una de estas páginas. Ahora, ¿cómo podemos calcular la probabilidad de que un navegante visite una página en particular?

Podemos calcular la probabilidad de llegar a una página de la siguiente manera:
\begin{equation}
p_{t+1} (d_j) = \underbrace{(1 - \alpha) \sum_{i = 1}^N M_{ij}p_t (d_i)}_{\text{reach } d_j \text{ by following a link}} + \underbrace{\alpha \sum_{i = 1}^N \frac{1}{N}p_t(d_i)}_{\text{reach } d_j \text{ by random jumping}}
\end{equation}

En el lado izquierdo está la probabilidad de visitar la página \(d_j\) en el tiempo \(t+1\), el siguiente conteo de tiempo. En el lado derecho, podemos ver que la ecuación involucra la probabilidad en la página \(d_i\) en el tiempo \(t\), el paso de tiempo actual. La ecuación captura las dos posibilidades de llegar a una página \(d_j\) en el tiempo \(t+1\): a través de la navegación aleatoria o siguiendo un enlace. 

La primera parte de la ecuación captura la probabilidad de que el navegante aleatorio llegue a esta página siguiendo un enlace. El navegante aleatorio elige esta estrategia con una probabilidad de \(1 - \alpha\); por lo tanto, hay un factor de \(1 - \alpha\) antes de este término. Este término suma todas las posibles \(N\) páginas en las que el navegante podría haber estado en el tiempo \(t\). Dentro de la suma está el producto de dos probabilidades. Una es la probabilidad de que el navegante estuviera en \(d_i\) en el tiempo \(t\). Esa es \(p_t(d_i)\). La otra es la probabilidad de transición de \(d_i\) a \(d_j\), que sabemos que se representa como \(M_{ij}\). Entonces, para llegar a esta página \(d_j\), el navegante debe estar primero en \(d_i\) en el tiempo \(t\) y tendría que seguir el enlace para ir de \(d_i\) a \(d_j\).

La segunda parte es una suma similar. La única diferencia es que ahora la probabilidad de transición es uniforme: \(\frac{1}{N}\). Esta parte captura la probabilidad de llegar a esta página a través de un salto aleatorio, donde \(\alpha\) es la probabilidad de un salto aleatorio.

Esto también nos permite ver por qué PageRank captura un suavizado de la matriz de transición. Puedes pensar que este \(\frac{1}{N}\) proviene de otra matriz de transición que tiene todos los elementos como \(\frac{1}{N}\). Entonces, está claro que podemos combinar las dos partes. Debido a que son de la misma forma, podemos imaginar que hay una matriz diferente que es una combinación de esta \(M\) y la matriz uniforme \(I\). En este sentido, PageRank utiliza esta idea de suavizado para asegurar que no haya ninguna entrada 0 en la matriz de transición.

Ahora, podemos imaginar que si queremos calcular las probabilidades promedio, estas satisfarían esta ecuación sin considerar el índice de tiempo. Así que eliminemos el índice de tiempo y asumamos que serían iguales; esto nos daría \(N\) ecuaciones, ya que cada página tiene su propia ecuación. De manera similar, también hay precisamente \(N\) variables. Esto significa que ahora tenemos un sistema de \(N\) ecuaciones lineales con \(N\) variables. El problema se reduce a resolver este sistema de ecuaciones, que podemos escribir de la siguiente forma:

\begin{equation}
p (d_j) = \sum_{i = 1}^N \left[\frac{\alpha}{N} + (1 - \alpha)M_{ij}\right] p (d_i) \rightarrow \vec{p} = (\alpha I + (1 - \alpha)M)^T \vec{p}, 
\end{equation}

donde 

\begin{equation}
I_{ij} = \frac{1}{N} \quad \forall i, j
\end{equation}

El vector $\vec{p}$ es igual a la transpuesta de una matriz multiplicada por $\vec{p}$ nuevamente. La matriz transpuesta es, de hecho, la suma de 1 a $N$ escrita en forma de matriz. Recordemos de álgebra lineal que esta es precisamente la ecuación para un vector propio. Por lo tanto, esta ecuación se puede resolver utilizando un algoritmo iterativo. En este algoritmo iterativo, llamado iteración de potencia, simplemente comenzamos con un $\vec{p}$ aleatorio. Luego, repetidamente actualizamos $\vec{p}$ multiplicando la expresión de la matriz transpuesta por $\vec{p}$.

Veamos un ejemplo concreto: establezcamos $\alpha = 0.2$. Esto significa que hay un 20\% de probabilidad de saltar aleatoriamente a una página en toda la web y un 80\% de probabilidad de seguir aleatoriamente un enlace desde la página actual. Tenemos la matriz de transición original $M$ como antes que codifica los enlaces reales en el gráfico. Luego, tenemos esta matriz de transición de suavizado uniforme $I$ que representa el salto aleatorio. Las combinamos juntas con interpolación a través de $\alpha$ para formar otra matriz que llamamos $A$:

\begin{equation}
A = (1 - 0.2) M + 0.2 I = 0.8 
\begin{bmatrix}
0 & 0 & \frac{1}{2} & \frac{1}{2} \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
\frac{1}{2} & \frac{1}{2} & 0 & 0
\end{bmatrix}
+ 0.2 
\begin{bmatrix}
\frac{1}{4} & \frac{1}{4} & \frac{1}{4} & \frac{1}{4} \\
\frac{1}{4} & \frac{1}{4} & \frac{1}{4} & \frac{1}{4} \\
\frac{1}{4} & \frac{1}{4} & \frac{1}{4} & \frac{1}{4} \\
\frac{1}{4} & \frac{1}{4} & \frac{1}{4} & \frac{1}{4}
\end{bmatrix}
\end{equation}

El algoritmo PageRank inicializará aleatoriamente \(\vec{p}\) primero, y luego lo actualizará iterativamente utilizando la multiplicación de matrices. Si reescribimos esta multiplicación de matrices en términos de solo \(A\), obtendremos lo siguiente:

\begin{equation}
\begin{bmatrix}
p_{t+1}(d_1) \\
p_{t+1}(d_2) \\
p_{t+1}(d_3) \\
p_{t+1}(d_4)
\end{bmatrix}
= A^T
\begin{bmatrix}
p_t (d_1) \\
p_t (d_2) \\
p_t (d_3) \\
p_t (d_4)
\end{bmatrix}
= 
\begin{bmatrix}
0.5 & 0.85 & 0.05 & 0.45 \\
0.5 & 0.05 & 0.85 & 0.45 \\
0.45 & 0.05 & 0.05 & 0.05 \\
0.45 & 0.05 & 0.05 & 0.05
\end{bmatrix}
\begin{bmatrix}
p_t (d_1) \\
p_t (d_2) \\
p_t (d_3) \\
p_t (d_4)
\end{bmatrix}
\end{equation}

Si deseas calcular el valor actualizado para \(d_1\), multiplicas la fila superior en \(A\) por el vector columna de las puntuaciones de PageRank de la iteración anterior. Así es como actualizamos el vector; comenzamos con algunos valores iniciales y multiplicamos iterativamente las matrices, lo que genera un nuevo conjunto de puntuaciones. Repetimos esta multiplicación hasta que los valores en \(\vec{p}\) converjan. De álgebra lineal, sabemos que dado que no hay valores cero en la matriz, dicha iteración está garantizada para converger. En ese punto, tendremos las puntuaciones de PageRank para todas las páginas.

Curiosamente, esta fórmula de actualización puede interpretarse como la propagación de puntuaciones a través del gráfico. Podemos imaginar que tenemos valores inicializados en cada una de estas páginas, y si observas la ecuación, combinamos las puntuaciones de las páginas que llevarían a alcanzar una página. Es decir, observamos todas las páginas que apuntan a una página y combinamos sus puntuaciones con la puntuación propagada para obtener la siguiente puntuación para el documento actual. Repetimos esto para todos los documentos, lo que transfiere la masa de probabilidad a través de la red.

En la práctica, el cálculo de la puntuación de PageRank es bastante eficiente porque las matrices son dispersas, lo que significa que si no hay un enlace hacia la página actual, no tenemos que preocuparnos por ello en el cálculo. También es posible normalizar la ecuación, y eso dará una forma algo diferente, aunque la clasificación relativa de las páginas no cambiará. La normalización es para abordar el problema potencial de los enlaces salientes cero. En ese caso, las probabilidades de alcanzar la siguiente página desde la página actual no sumarán 1 porque hemos perdido algo de masa de probabilidad cuando asumimos que hay alguna probabilidad de que el navegante intente seguir enlaces (aunque en este caso no hay enlaces que seguir).

Hay muchas extensiones a PageRank. Una extensión es hacer PageRank específico para consultas, también llamado PageRank Personalizado. Por ejemplo, en este PageRank específico para temas, podemos simplemente asumir que cuando el navegante se aburre, no saltará aleatoriamente a cualquier página en la web. En su lugar, saltará solo a aquellas páginas que son relevantes para la consulta. Por ejemplo, si la consulta es sobre deportes, entonces podríamos asumir que cuando hacemos saltos aleatorios, saltamos aleatoriamente a una página de deportes. Al hacer esto, nuestras puntuaciones de PageRank se alinean con los deportes. Por lo tanto, si sabes que la consulta actual es sobre deportes, podemos usar esta puntuación de PageRank especializada para clasificar los resultados. Claramente, esto sería mejor que usar una puntuación de PageRank genérica para toda la web.

PageRank es un algoritmo general que puede usarse en muchas otras aplicaciones, como el análisis de redes, particularmente en redes sociales. Podemos imaginar que si calculas la puntuación de PageRank de una persona en una red social (donde un enlace indica una relación de amistad), obtendrás puntuaciones significativas para las personas.

\subsection{HITS}
Hemos hablado sobre PageRank como una forma de capturar páginas de autoridad. Al principio de esta sección, también mencionamos que las páginas de concentrador son útiles. Hay otro algoritmo que discutiremos llamado HITS que está diseñado para calcular ambas puntuaciones para cada página.

Las páginas de autoridad capturan la intuición de páginas ampliamente citadas. Las páginas de concentrador son aquellas que apuntan a buenas páginas de autoridad o contienen alguna colección de conocimiento en forma de enlaces. La idea principal del algoritmo HITS es un mecanismo de refuerzo para ayudar a mejorar la puntuación tanto de concentradores como de autoridades. Asumirá que las buenas autoridades son citadas por buenos concentradores. Eso significa que si eres citado por muchas páginas con buenas puntuaciones de concentrador, entonces eso aumenta tu puntuación de autoridad. De manera similar, los buenos concentradores son aquellos que apuntan a buenas autoridades. Así que si estás apuntando a muchas buenas páginas de autoridad, entonces tu puntuación de concentrador aumentará. Al igual que PageRank, HITS también es bastante general y tiene muchas aplicaciones en el análisis de gráficos y redes. Brevemente, describiremos cómo funciona.

La Figura 10.8 muestra lo siguiente. Primero, construimos la matriz de adyacencia \(A\); contiene un 1 en la posición \(A_{ij}\) si \(d_i\) enlaza a \(d_j\) y un cero en caso contrario. Definimos la puntuación de concentrador de una página \(h(d_i)\) como la suma de las puntuaciones de autoridad de todas las páginas a las que apunta. En la segunda ecuación, definimos la puntuación de autoridad de una página \(a(d_i)\) como la suma de las puntuaciones de concentrador de todas las páginas que apuntan a ella. Esto forma un mecanismo de refuerzo iterativo.

Estas dos ecuaciones también pueden escribirse en forma matricial. El vector de concentrador es igual al producto de la matriz de adyacencia y el vector de autoridad. De manera similar, la segunda ecuación puede escribirse como el vector de autoridad es igual al producto de \(A^T\) multiplicado por el vector de concentrador. Estas son solo diferentes formas de expresar estas ecuaciones. Lo interesante es que si observas las formas matriciales, puedes insertar la ecuación de autoridad en la primera. Es decir, puedes eliminar completamente el vector de autoridad, y obtienes la ecuación solo de las puntuaciones de concentrador. Podemos hacer el mismo truco para la fórmula de concentrador. Por lo tanto, aunque enmarcamos el problema como calcular concentradores y autoridades, en realidad podemos eliminar uno de ellos para obtener la ecuación para el otro.

La diferencia entre esto y PageRank es que ahora la matriz es en realidad una multiplicación de la matriz de adyacencia y su transpuesta. Matemáticamente, entonces, estaríamos resolviendo un problema muy similar. En HITS, inicializaríamos los valores a uno y aplicaríamos las ecuaciones matriciales \(A^T A\) y \(A A^T\). Todavía necesitamos normalizar la matriz de adyacencia después de cada iteración. Esto nos permitiría controlar el crecimiento de los valores; de lo contrario, crecerían cada vez más.

Para resumir, esta sección ha mostrado que la información de enlaces es muy útil. En particular, el texto ancla es una característica importante en la representación de texto de una página. También hablamos sobre los algoritmos PageRank y HITS como dos algoritmos principales de análisis de enlaces en la búsqueda web. Ambos pueden generar puntuaciones para las páginas que pueden usarse además de las funciones de clasificación estándar de IR. PageRank y HITS son algoritmos muy generales con variantes útiles, por lo que tienen muchas aplicaciones en el análisis de otros gráficos o redes además de la web.

\section{Aprendizaje para Clasificar}

En esta sección, discutiremos el uso del aprendizaje automático para combinar muchas características diferentes en una única función de clasificación para optimizar los resultados de búsqueda. Anteriormente, hemos discutido varias formas de clasificar documentos. Hablamos sobre algunos modelos de recuperación como BM25 o la probabilidad de consulta; estos pueden generar una puntuación basada en el contenido para relacionar el texto del documento con una consulta. También hablamos sobre enfoques basados en enlaces como PageRank que pueden dar puntuaciones adicionales para ayudarnos a mejorar la clasificación.

La pregunta ahora es ¿cómo podemos combinar todas estas características (y potencialmente muchas otras características) para hacer la clasificación? Esto será muy útil para clasificar páginas web no solo para mejorar la precisión, sino también para mejorar la robustez de la función de clasificación de manera que no sea fácil para un spammer simplemente alterar una o algunas características para promover una página.

La idea general del aprendizaje para clasificar es usar el aprendizaje automático para combinar estas características, optimizando el peso de diferentes características para generar la mejor función de clasificación. Asumimos que dada una pareja consulta-documento (q, d), podemos definir una serie de características. Estas características no tienen que ser necesariamente basadas en el contenido. Podrían ser una puntuación del documento con respecto a la consulta según una función de recuperación como BM25, la probabilidad de consulta, la normalización de longitud pivoteada, PL2, etc. También puede haber una puntuación basada en enlaces como PageRank o HITS, o una aplicación de modelos de recuperación al texto ancla de la página, que son las descripciones de los enlaces que apuntan a d. Todas estas pueden ser pistas sobre si este documento es relevante o no para la consulta. Incluso podemos incluir una característica como si la URL tiene una tilde porque esto podría indicar una página de inicio.

La pregunta es, por supuesto, ¿cómo podemos combinar estas características en una única puntuación? En este enfoque, simplemente hipotetizamos que la probabilidad de que este documento sea relevante para esta consulta es una función de todas estas características. Hipotetizamos que la probabilidad de relevancia está relacionada con estas características a través de una función particular que tiene algunos parámetros. Estos parámetros controlan la influencia de diferentes características en la relevancia final. Esto es, por supuesto, solo una suposición. Si esta suposición realmente tiene sentido sigue siendo una pregunta abierta.

Naturalmente, la siguiente pregunta es cómo estimar esos parámetros. ¿Cómo sabemos qué características deberían tener un alto peso y cuáles deberían tener un bajo peso? Esta es una tarea de entrenamiento o aprendizaje.

En este enfoque, usamos datos de entrenamiento. Estos son datos que han sido juzgados por usuarios, por lo que ya conocemos los juicios de relevancia. Sabemos qué documentos deberían estar altamente clasificados para qué consultas, y esta información puede basarse en juicios reales de usuarios o puede aproximarse simplemente usando información de clics como discutimos en el Capítulo 7. Intentaremos optimizar la precisión de recuperación de nuestro motor de búsqueda (usando, por ejemplo, MAP o NDCG) en los datos de entrenamiento ajustando estos parámetros. Los datos de entrenamiento se verían como una tabla de tuplas. Cada tupla tiene tres elementos: la consulta, el documento y el juicio. Veamos un método específico que se basa en la regresión logística:
\begin{equation}
    \log \frac{P(R = 1 | q, d)}{1 - P(R = 1 | q, d)} = \beta_0 + \sum_{i = 1}^n \beta_i X_i
\end{equation}

\textcolor{red}{Queremos muchos inlinks, de paginas buenas mejor, y si esas paginas no tienen muchos outlinks mejor.}

Este es uno de muchos métodos diferentes, y en realidad uno de los más sencillos.

En este enfoque, simplemente asumimos que la relevancia de un documento con respecto a la consulta está relacionada con una combinación lineal de todas las características. Aquí tenemos $X_i$ para denotar el valor de la característica $i$, y podemos tener tantas características como deseemos. Asumimos que estas características pueden combinarse de manera lineal. El peso de la característica $X_i$ está controlado por un parámetro $\beta_i$. Un $\beta_i$ más grande significaría que la característica tendría un peso mayor y contribuiría más a la función de puntuación.

La forma específica de la función también da la siguiente probabilidad de relevancia:

\begin{equation}
    P(R = 1 | q, d) = \frac{1}{1 + \exp\{-(\beta_0 + \sum_{i = 1}^n \beta_i X_i)\}}
\end{equation}

Sabemos que la probabilidad de relevancia está dentro del rango [0, 1] y asumimos que la función de puntuación es una forma transformada de la combinación lineal de características. Podríamos haber tenido una función de puntuación basada directamente en la combinación lineal de $\beta$ y $X$, pero entonces el valor de esta combinación lineal podría fácilmente superar 1. Por lo tanto, la razón por la que usamos regresión logística en lugar de regresión lineal es para mapear esta combinación al rango [0, 1]. Esto nos permite conectar la probabilidad de relevancia (que está entre 0 y 1) con una combinación lineal de coeficientes arbitrarios. Si reescribimos esta combinación de pesos en una función de probabilidad, obtendremos la puntuación predicha.

Si esta combinación de características y pesos nos da un valor alto, entonces el documento es más probable que sea relevante. Esto no es necesariamente la mejor hipótesis, pero es una forma simple de conectar estas características con la probabilidad de relevancia.

La siguiente tarea es ver cómo estimamos los parámetros para que la función pueda aplicarse verdaderamente; es decir, necesitamos estimar los valores de $\beta$. Veamos un ejemplo sencillo que se muestra en la Figura 10.9.

En este ejemplo, tenemos tres características. Una es la puntuación BM25 del documento para la consulta. Una es la puntuación PageRank del documento, que podría o no depender de la consulta. También podríamos tener una puntuación PageRank sensible al tema que dependería de la consulta. Por último, tenemos una puntuación BM25 en el texto ancla del documento. Estas son entonces las tres valores de características para una pareja particular (documento, consulta). En este caso, el documento es $d_1$ y el juicio dice que es relevante. El documento $d_2$ es otra instancia de entrenamiento con diferentes valores de características, pero en este caso no es relevante. Por supuesto, este es un ejemplo demasiado simplificado donde solo tenemos dos instancias, pero es suficiente para ilustrar el punto.

Usamos el estimador de máxima verosimilitud para estimar los parámetros. Es decir, vamos a predecir el estado de relevancia del documento basado en los valores de las características. La verosimilitud de observar el estado de relevancia de estos dos documentos usando nuestro modelo es

\begin{align}
p(\{q, d_1, R = 1\}, \{q, d_2, R = 0\}) =& \frac{1}{1 + \exp\{-(\beta_0 + 0.7\beta_1 + 0.11\beta_2 + 0.65\beta_3)\}} \notag \\
& \times \left(1 - \frac{1}{1 + \exp\{-(\beta_0 + 0.3\beta_1 + 0.05\beta_2 + 0.4\beta_3)\}}\right)
\end{align}

\textcolor{red}{pagerank es query inddependant. HITS no tuvo exito practico porque es muy costoso computacionalmente.}


Hipotetizamos que la probabilidad de relevancia está relacionada con las características de esta manera. Vamos a ver para qué valores de $\beta$ podemos predecir la relevancia de manera efectiva. La expresión para $d_1$ debería dar un valor más alto que la expresión para $d_2$; de hecho, esperamos que el valor de $d_1$ esté cerca de uno ya que es un documento relevante. Veamos cómo se puede expresar esto matemáticamente. Es similar a expresar la probabilidad de un documento, solo que no estamos hablando de la probabilidad de palabras, sino de la probabilidad de relevancia. Necesitamos insertar los valores de $X$. Los valores de $\beta$ aún son desconocidos, pero esta expresión nos da la probabilidad de que este documento sea relevante si asumimos tal modelo. Queremos maximizar esta probabilidad para $d_1$ ya que este es un documento relevante. Para el segundo documento, queremos predecir la probabilidad de que el documento no sea relevante. Esto significa que tenemos que calcular 1 menos la probabilidad de relevancia. Ese es el razonamiento detrás de toda esta expresión; es nuestra probabilidad de predecir estos dos valores de relevancia. Toda la ecuación es nuestra probabilidad de observar un $R = 1$ y un $R = 0$ para $d_1$ y $d_2$ respectivamente. Nuestro objetivo es entonces ajustar los valores de $\beta$ para que toda la expresión alcance su valor máximo. En otras palabras, veremos la función y elegiremos valores de $\beta$ para hacer que esta expresión sea lo más grande posible.

Después de aprender los parámetros de regresión, podemos usar esta expresión para cualquier nueva consulta y nuevo documento una vez que tengamos sus características. Esta fórmula se aplica entonces para generar una puntuación de clasificación para una consulta en particular.

Existen muchos algoritmos de aprendizaje más avanzados que los enfoques basados en regresión. Generalmente intentan optimizar teóricamente una medida de recuperación como MAP o NDCG. Nota que el objetivo de optimización que acabamos de discutir no está directamente relacionado con una medida de recuperación. Al maximizar la predicción de uno o cero, no necesariamente optimizamos la clasificación de esos documentos. Se puede imaginar que, aunque nuestra predicción puede no estar tan mal, la clasificación puede ser incorrecta. Podríamos tener una mayor probabilidad de relevancia para $d_2$ que para $d_1$. Así, eso no sería bueno desde una perspectiva de recuperación, aunque por verosimilitud la función no esté mal. Enfoques más avanzados intentarán corregir este problema. Por supuesto, el desafío es que el problema de optimización será más difícil de resolver. En contraste, podríamos tener otro caso donde predecimos probabilidades de relevancia alrededor de 0.9 para documentos no relevantes. Aunque la puntuación predicha es muy alta, siempre y cuando los documentos realmente relevantes reciban puntuaciones mayores a 0.9, la clasificación seguirá siendo aceptable para un usuario.

Estos enfoques de aprendizaje para clasificar son en realidad bastante generales. Pueden aplicarse a muchos otros problemas de clasificación además de los problemas de recuperación. Por ejemplo, sistemas de recomendación, publicidad computacional, resumen y muchas otras aplicaciones relevantes pueden resolverse utilizando este enfoque.

Para resumir, hablamos sobre el uso de aprendizaje automático para combinar características y predecir un resultado de clasificación. De hecho, el uso de aprendizaje automático en la recuperación de información comenzó hace muchas décadas. El feedback de Rocchio, discutido en el Capítulo 7, fue un enfoque de aprendizaje automático aplicado para aprender el feedback óptimo. Muchos algoritmos están impulsados por la disponibilidad de grandes cantidades de datos de entrenamiento en forma de clics. Estos datos proporcionan mucho conocimiento útil sobre la relevancia, y por eso se aplican métodos de aprendizaje automático para aprovechar esto. La necesidad de aprendizaje automático también está impulsada por el deseo de combinar muchos tipos diferentes de características para predecir una clasificación precisa. La búsqueda web especialmente impulsa esta necesidad ya que hay más características disponibles en la web que pueden ser aprovechadas para la búsqueda. Usar muchas características diferentes también aumenta la robustez de la función de puntuación, lo cual es útil para combatir el spam. Los motores de búsqueda modernos utilizan todo algún tipo de técnicas de aprendizaje automático para combinar muchas características y optimizar la clasificación, y esto es una característica principal de motores actuales como Google y Bing.

\section{Futuro del web search}

Dado que este capítulo concluye nuestra cobertura de los motores de búsqueda, hablaremos brevemente sobre algunas posibles tendencias futuras de la búsqueda web y los sistemas de recuperación de información inteligentes en general. Para mejorar aún más la precisión de un motor de búsqueda, es importante considerar casos especiales de necesidad de información. Una tendencia particular es tener cada vez más motores de búsqueda especializados y personalizados, que pueden llamarse motores de búsqueda verticales. Se espera que estos motores de búsqueda verticales sean más efectivos que los motores de búsqueda generales actuales porque podrían asumir que un usuario en particular pertenece a un grupo especial que podría tener una necesidad de información común. Debido a esta personalización, también es posible realizar la personalización. La búsqueda puede ser personalizada porque tenemos una mejor comprensión de los usuarios. Restringir el dominio del motor de búsqueda también puede tener algunas ventajas en el manejo de los documentos, porque tendríamos una mejor comprensión de estos documentos. Por ejemplo, palabras particulares pueden no ser ambiguas en tal dominio, por lo que podemos evitar el problema de la ambigüedad.

Otra tendencia que podemos esperar ver son los motores de búsqueda que son capaces de aprender con el tiempo, una forma de aprendizaje de por vida o aprendizaje continuo. Esto es muy atractivo porque significa que el motor de búsqueda podrá auto-mejorarse. A medida que más personas lo utilicen, el motor de búsqueda se volverá cada vez mejor. Esto ya está sucediendo, porque los motores de búsqueda pueden aprender del feedback de relevancia. Más usuarios lo utilizan, y la calidad del motor de búsqueda permite que las consultas populares que son tipeadas por muchos usuarios recuperen mejores resultados.

Una tercera tendencia podría ser la integración del acceso a la información. La búsqueda, navegación y recomendación podrían combinarse para formar un sistema de gestión de información completo. Al principio de este libro, hablamos sobre el acceso push versus el acceso pull; estos modos pueden combinarse. Por ejemplo, si un motor de búsqueda detecta que un usuario está insatisfecho con los resultados de búsqueda, se puede hacer una “nota”. En el futuro, si se rastrea un nuevo documento que coincida con la necesidad de información del usuario registrada en la nota, este nuevo documento podría ser enviado al usuario. Actualmente, la mayoría de los casos de recomendación de información son publicidad, pero en el futuro, puedes imaginar que la recomendación se integra de manera fluida en el sistema con acceso a la información multimodal.

Otra tendencia es que podríamos ver sistemas que intentan ir más allá de la búsqueda para apoyar las tareas de los usuarios. Después de todo, la razón por la cual las personas quieren buscar es para resolver un problema o tomar una decisión para realizar una tarea. Por ejemplo, los consumidores podrían buscar opiniones sobre productos para comprar un producto, por lo que sería beneficioso apoyar todo el flujo de trabajo de compra. Por ejemplo, a veces puedes ver la reseña mostrada directamente en los resultados de búsqueda; si el usuario decide comprar el producto, puede simplemente hacer clic en un botón para ir directamente al sitio de compras y realizar la compra. Aunque hay buen soporte para las compras, los motores de búsqueda actuales no proporcionan buen soporte para las tareas en muchas otras actividades. Los investigadores podrían querer encontrar trabajos relacionados o citas sugeridas. Actualmente, no hay mucho soporte para una tarea como escribir un artículo.

Podemos pensar en cualquier sistema inteligente—especialmente en sistemas inteligentes de información—especificado por tres nodos. Si conectamos estos nodos formando un triángulo, entonces podremos especificar un sistema de información. Podemos llamar a este triángulo el Triángulo de Datos-Usuario-Servicio. Las tres preguntas que planteas en las áreas siguen.


\begin{itemize}
\item  A quien sirves?
\item Que tipo de datos manejas?
\item que tipo de servicio proporcionas?
\end{itemize}


Estas preguntas especifican un sistema de información; hay muchas formas diferentes de conectarlas. Dependiendo de cómo estén conectadas, podemos especificar todo tipo de sistemas diferentes. Consideremos algunos ejemplos.

En la parte superior de la Figura 10.10 hay diferentes tipos de usuarios. En el lado izquierdo, hay diferentes tipos de datos o información, y en la parte inferior, hay diferentes funciones de servicio. Ahora imagina que puedes conectar todo esto de diferentes maneras. Por ejemplo, si conectas a todos con páginas web y soportas la búsqueda y navegación, obtienes una búsqueda web. Si conectamos a los empleados universitarios con documentos de la organización o documentos empresariales y apoyamos la búsqueda y navegación, obtenemos una búsqueda empresarial.

Podríamos conectar a científicos con información de literatura para proporcionar todo tipo de servicios, incluyendo búsqueda, navegación, alertas a nuevos documentos relevantes, minería o análisis de tendencias de investigación, o soporte para tareas y decisiones. Por ejemplo, podríamos ser capaces de proporcionar soporte para generar automáticamente una sección de trabajos relacionados para un artículo de investigación; esto estaría más cerca del soporte de tareas. Luego, podemos imaginar que este sistema de información inteligente sería un tipo de asistente de literatura.

Si conectamos a compradores en línea con artículos de blogs o reseñas de productos, entonces podemos ayudar a estas personas a mejorar su experiencia de compra. Podemos proporcionar capacidades de minería de datos para analizar reseñas, comparar productos y el sentimiento sobre los productos, y proporcionar soporte para tareas o decisiones en la elección de qué producto comprar. O, podemos conectar Personal de servicio al cliente con correos electrónicos de los clientes. Imagina un sistema que pueda proporcionar un análisis de estos correos electrónicos para identificar las principales quejas de los clientes. Podemos imaginar un sistema que podría proporcionar soporte para tareas generando automáticamente una respuesta a un correo electrónico de un cliente adjuntando inteligentemente un mensaje de promoción si es apropiado. Si detectan un mensaje positivo (no una queja), entonces podrían aprovechar esta oportunidad para adjuntar alguna información promocional. Si es una queja, entonces podrías ser capaz de generar automáticamente una respuesta genérica primero y decirle al cliente que puede esperar una respuesta detallada más tarde. Todo esto tiene como objetivo ayudar a las personas a mejorar su productividad.

La Figura 10.10 muestra la tendencia de la tecnología y caracteriza los sistemas de información inteligentes con tres ángulos. En el centro de la figura hay un triángulo que conecta consultas con palabras clave a una representación de bolsa de palabras. Eso significa que los motores de búsqueda actuales básicamente proporcionan soporte de búsqueda a los usuarios y modelan principalmente a los usuarios basándose en consultas de palabras clave, viendo los datos a través de una representación de bolsa de palabras. Los motores de búsqueda actuales realmente no "comprenden" la información en los documentos indexados. Considera algunas tendencias hacia cada nodo hacia una función más avanzada, alejándose del centro. Imagina si podemos ir más allá de las consultas de palabras clave, mirar el historial de búsqueda del usuario y luego modelar aún más al usuario para comprender completamente el entorno de tareas del usuario, el contexto u otra información. Claramente, esto está impulsando la personalización y un modelo de usuario más completo, que es una dirección principal para construir sistemas de información inteligentes. En el lado del documento, también podemos ir más allá de una implementación de bolsa de palabras para tener una representación de entidad-relación. Esto significa que reconoceremos los nombres de las personas, sus relaciones, ubicaciones y cualquier otra información potencialmente útil. Esto ya es factible con las técnicas actuales de procesamiento de lenguaje natural.

Google ha iniciado parte de este trabajo a través de su Knowledge Graph. Una vez que podemos llegar a ese nivel sin mucho esfuerzo manual humano, el motor de búsqueda puede proporcionar un servicio mucho mejor. En el futuro, nos gustaría tener una representación de conocimiento donde quizás podamos agregar algunas reglas de inferencia, haciendo que el motor de búsqueda sea más inteligente. Esto requiere un análisis semántico a gran escala, y quizás esto inicialmente sea más factible para motores de búsqueda verticales. Es decir, es más fácil progresar en un dominio particular.

En el lado de los servicios, vemos que necesitamos ir más allá de la búsqueda para apoyar el acceso a la información en general; la búsqueda es solo una forma de acceder a la información. Más allá del acceso, también necesitamos ayudar a las personas a digerir la información una vez que se encuentra, y este paso es hacerlo mediante el análisis de la información o minería de datos. Tenemos que encontrar patrones o convertir la información textual en conocimiento real que pueda ser usado en aplicaciones o conocimiento accionable que pueda ser utilizado para la toma de decisiones. Además, el conocimiento será utilizado para ayudar al usuario a mejorar la productividad en la finalización de una tarea.

En esta dimensión, anticipamos que los futuros sistemas de información inteligentes proporcionarán soporte interactivo para tareas. Debemos enfatizar "interactivo" aquí, porque es importante optimizar la inteligencia combinada de los usuarios y el sistema. Podemos obtener algo de ayuda de los usuarios de una manera natural sin asumir que el sistema tiene que hacer todo. Es decir, el usuario y la máquina pueden colaborar de manera inteligente y eficiente. Esta inteligencia combinada será alta y, en general, podemos minimizar el esfuerzo general del usuario en resolver su problema actual.

Esta es la visión general de los futuros sistemas de información inteligentes, y esperamos que pueda proporcionarnos algunas ideas sobre cómo realizar futuras innovaciones sobre lo que tenemos hoy y también motivar las técnicas adicionales que se cubrirán en los capítulos posteriores del libro.


\textcolor{red}{Text analysis hasta pagina }