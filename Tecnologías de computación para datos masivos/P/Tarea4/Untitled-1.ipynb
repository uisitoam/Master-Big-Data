{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ssh curso101@hadoop3.cesga.es\n",
    "\n",
    "module load anaconda3/2018.12\n",
    "python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usar: p1.py cite75_99.txt apat63_99.txt dfCitas.parquet dfInfo.parquet\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m     spark\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     exit(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m path_cite75_99 \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 16\u001b[0m path_apat63_99 \u001b[38;5;241m=\u001b[39m \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     17\u001b[0m output_dfCitas \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     18\u001b[0m output_dfInfo \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m4\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # Comprueba el número de argumentos\n",
    "    if len(sys.argv) != 5:\n",
    "        print(\"Usar: p1.py cite75_99.txt apat63_99.txt dfCitas.parquet dfInfo.parquet\")\n",
    "        exit(-1)\n",
    "\n",
    "    path_cite75_99 = sys.argv[1]\n",
    "    path_apat63_99 = sys.argv[2]\n",
    "    output_dfCitas = sys.argv[3]\n",
    "    output_dfInfo = sys.argv[4]\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Practica 1 de Tomas\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Reducir la verbosidad de los logs\n",
    "    spark.sparkContext.setLogLevel(\"FATAL\")\n",
    "\n",
    "    # a) Procesar cite75_99.txt para obtener el número de citas por patente\n",
    "    # Leer el archivo cite75_99.txt desde HDFS\n",
    "    df_cite = spark.read.csv(path_cite75_99, inferSchema=True, header=False)\n",
    "\n",
    "    # Renombrar las columnas\n",
    "    df_cite = df_cite.withColumnRenamed(\"_c0\", \"citing\").withColumnRenamed(\"_c1\", \"cited\")\n",
    "\n",
    "    # Contar el número de citas por patente citada\n",
    "    df_ncitas = df_cite.groupBy(\"cited\") \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed(\"cited\", \"NPatente\") \\\n",
    "        .withColumnRenamed(\"count\", \"ncitas\")\n",
    "\n",
    "    # Convertir NPatente y ncitas a enteros\n",
    "    df_ncitas = df_ncitas.withColumn(\"NPatente\", F.col(\"NPatente\").cast(\"integer\")) \\\n",
    "                         .withColumn(\"ncitas\", F.col(\"ncitas\").cast(\"integer\"))\n",
    "\n",
    "    # b) Procesar apat63_99.txt para obtener NPatente, Pais y Anho\n",
    "    # Leer el archivo apat63_99.txt como texto desde HDFS\n",
    "    df_apat = spark.read.text(path_apat63_99)\n",
    "\n",
    "    # Extraer las columnas de interés según las posiciones de los caracteres\n",
    "    df_info = df_apat.select(\n",
    "        F.trim(F.substring(df_apat.value, 1, 7)).alias(\"NPatente\"),\n",
    "        F.trim(F.substring(df_apat.value, 58, 2)).alias(\"Pais\"),\n",
    "        F.trim(F.substring(df_apat.value, 66, 4)).alias(\"Anho\")\n",
    "    )\n",
    "\n",
    "    # Convertir tipos de datos\n",
    "    df_info = df_info.withColumn(\"NPatente\", F.col(\"NPatente\").cast(\"integer\")) \\\n",
    "                     .withColumn(\"Anho\", F.col(\"Anho\").cast(\"integer\"))\n",
    "\n",
    "    # Guardar los DataFrames en formato Parquet con compresión gzip\n",
    "    df_ncitas.write.parquet(output_dfCitas, mode=\"overwrite\", compression=\"gzip\")\n",
    "    df_info.write.parquet(output_dfInfo, mode=\"overwrite\", compression=\"gzip\")\n",
    "\n",
    "    # Mostrar el número de particiones y archivos generados\n",
    "    num_partitions_ncitas = df_ncitas.rdd.getNumPartitions()\n",
    "    print(\"DataFrame df_ncitas tiene {} particiones.\".format(num_partitions_ncitas))\n",
    "\n",
    "    num_partitions_info = df_info.rdd.getNumPartitions()\n",
    "    print(\"DataFrame df_info tiene {} particiones.\".format(num_partitions_info))\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
