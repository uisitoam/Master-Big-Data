{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkU_F8hZLeOL"
   },
   "source": [
    "# Operaciones básicas en Spark\n",
    "- Spark opera con colecciones **inmutables y distribuidas** de elementos, manipulándolos en paralelo\n",
    "    - API estructurada: DataFrames y DataSets\n",
    "    - API de bajo nivel: RDDs (ya obsoleto ya que el API estructurada es más eficiente y más de alto nivel)\n",
    "\n",
    "-   Operaciones sobre estas colecciones\n",
    "    -   Creación\n",
    "    -   Transformaciones (ordenación, filtrado, etc.)\n",
    "    -   Realización acciones para obtener resultados\n",
    "\n",
    "-   Spark automáticamente distribuye los datos y paraleliza las operaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bT8r0anjLeOW"
   },
   "source": [
    "### Ejemplo: creación de un DataFrame a partir de un fichero CSV\n",
    "En este ejemplo, Spark infiere el esquema de los datos de forma automática\n",
    "\n",
    "  - Es preferible especificar el esquema de forma explícita, como veremos más adelante\n",
    "\n",
    "También se especifica que la primera línea es la cabecera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "format": "text/plain",
    "id": "EwPKO0reLeOY",
    "outputId": "d4c99bfe-6467-46a2-97f1-f22f84e0ba06",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 luisi  staff   6.9K Nov  5 16:42 2015-summary.csv\n",
      "DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count\n",
      "United States,Romania,15\n",
      "United States,Croatia,1\n",
      "United States,Ireland,344\n",
      "Egypt,United States,15\n",
      "United States,India,62\n",
      "United States,Singapore,1\n",
      "United States,Grenada,62\n",
      "Costa Rica,United States,588\n",
      "Senegal,United States,40\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "wget -q \"https://raw.githubusercontent.com/dsevilla/tcdm-public/24-25/datos/2015-summary.csv\"\n",
    "ls -lh 2015-summary.csv\n",
    "head 2015-summary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7SQV5O4sOcOn",
    "outputId": "2a2f7173-fb21-4a31-d3a5-d8c95cd4a396"
   },
   "outputs": [],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cZW1zRyPOo2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 16:40:29 WARN Utils: Your hostname, MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 172.18.26.240 instead (on interface en0)\n",
      "24/11/05 16:40:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/05 16:40:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\n",
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"Mi aplicacion\") \\\n",
    "  .config(\"spark.alguna.opcion.de.configuracion\", \"algun-valor\") \\\n",
    "  .master(\"local[*]\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto",
    "id": "6nIgZApNLeOc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "datosVuelos2015: DataFrame = (spark\n",
    "    .read\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(\"2015-summary.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3rdmzrqSLeOf",
    "outputId": "bf28e6c8-903d-487e-8faf-749d10c5a784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datosVuelos2015.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLGLlv8pLeOh",
    "outputId": "533cfc62-6e81-444a-e0d4-20c1befd0744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "datosVuelos2015.show()\n",
    "print(datosVuelos2015.count()) # cada fila del dataframe es un objeto de tipo row, es decir, una tabla es una lista de filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t4TRlMpzLeOh",
    "outputId": "fcda2346-d37d-43da-c4e6-2f3a2ad0a50e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datosVuelos2015.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYZBVIqiLeOi"
   },
   "source": [
    "### Rows\n",
    "\n",
    "Las filas de un DataFrame son objetos de tipo `Row`\n",
    "\n",
    "- API de Row en Python: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Row.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto",
    "id": "eZZFV6aJLeOm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)]\n",
      "<class 'list'>\n",
      "<class 'pyspark.sql.types.Row'>\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos las dos primeras fila del DataFrame\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "rows1_2: list[Row] = datosVuelos2015.take(2) # devuelve las 2 primeras filas del DataFrame como una lista de objetos Row\n",
    "print(rows1_2)\n",
    "print(type(rows1_2))\n",
    "print(type(rows1_2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto",
    "id": "lNQfTYQWLeOn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DEST_COUNTRY_NAME': 'United States', 'ORIGIN_COUNTRY_NAME': 'Romania', 'count': 15}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Obtén la primera fila como un diccionario Python\n",
    "print(rows1_2[0].asDict())\n",
    "print(type(rows1_2[0].asDict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1QwipCULeOp"
   },
   "source": [
    "### Particiones\n",
    "\n",
    "Spark divide las filas DataFrame en un conjunto de particiones\n",
    "\n",
    "-   El número de particiones por defecto es función del tamaño del cluster (número total de cores en todos los ejecutores) y del tamaño de los datos (número de bloques de los ficheros en HDFS)\n",
    "-   Para RDDs se puede especificar otro valor en el momento de crearlos\n",
    "-   También se puede modificar una vez creados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto",
    "id": "oCKT8sLhLeOp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de particiones: 1\n",
      "Número de particiones: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Número de particiones: {0}\"\n",
    "    .format(datosVuelos2015.rdd.getNumPartitions()))\n",
    "\n",
    "# Creo un nuevo DataFrame con 4 particiones\n",
    "datosVuelos2015_4P: DataFrame = datosVuelos2015.repartition(4)\n",
    "print(\"Número de particiones: {0}\"\n",
    "    .format(datosVuelos2015_4P.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2rp6vPfLeOr"
   },
   "source": [
    "### Transformaciones\n",
    "\n",
    "Operaciones que transforman los datos\n",
    "\n",
    "  - No modifican los datos de origen (*inmutabilidad*)\n",
    "  - Se computan de forma “perezosa” (*lazyness*)\n",
    "\n",
    "Dos tipos:\n",
    "\n",
    "  - Transformaciones *estrechas* (narrow)\n",
    "    - Cada partición de entrada contribuye a una única partición de salida\n",
    "    - No se modifica el número de particiones\n",
    "    - Normalmente se realizan en memoria\n",
    "  - Transformaciones *anchas* (wide)\n",
    "    - Cada partición de salida depende de varias (o todas) particiones de entrada\n",
    "    - Suponen un barajado de datos\n",
    "    - Pueden implicar un cambio en el número de particiones\n",
    "    - Pueden suponer escrituras en disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto",
    "id": "boqxgnc7LeOs"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de una transformación narrow\n",
    "datosVuelos2015_EEUU: DataFrame = datosVuelos2015\\\n",
    "    .replace(\"United States\", \"Estados Unidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "RoFCSMYxLeOt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de una transformación wide\n",
    "datosVuelos2015_Ord: DataFrame = datosVuelos2015_EEUU\\\n",
    "    .sort(\"count\", ascending=False)\n",
    "datosVuelos2015_Ord.cache() # cache() es una acción que guarda el DataFrame en memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsvBBIjmLeOu"
   },
   "source": [
    "### Acciones\n",
    "\n",
    "Obtienen un resultado, forzando a que se realicen las transformaciones pendientes\n",
    "\n",
    "  - En el momento de disparar la *acción* se crea un *plan* con las transformaciones necesarias para obtener los datos solicitados\n",
    "    - Se crea un Grafo Dirigido Acíclico (DAG) conectando las transformaciones\n",
    "    - Spark optimiza ese grafo, para eliminar transformaciones innecesarias o unir las que sea posible\n",
    "  - Las acciones traducen el DAG en un plan de ejecución\n",
    "\n",
    "Tipos de acciones\n",
    "\n",
    "  - Acciones para mostrar datos por consola\n",
    "  - Acciones para convertir datos Spark en datos del lenguaje\n",
    "  - Acciones para escribir datos a disco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto",
    "id": "T9H6efssLeOv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas en la tabla: 256\n",
      "[Row(DEST_COUNTRY_NAME='Estados Unidos', ORIGIN_COUNTRY_NAME='Estados Unidos', count=370002), Row(DEST_COUNTRY_NAME='Estados Unidos', ORIGIN_COUNTRY_NAME='Canada', count=8483), Row(DEST_COUNTRY_NAME='Canada', ORIGIN_COUNTRY_NAME='Estados Unidos', count=8399)]\n",
      "+------------------+-------------------+------+\n",
      "| DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+------------------+-------------------+------+\n",
      "|    Estados Unidos|     Estados Unidos|370002|\n",
      "|    Estados Unidos|             Canada|  8483|\n",
      "|            Canada|     Estados Unidos|  8399|\n",
      "|    Estados Unidos|             Mexico|  7187|\n",
      "|            Mexico|     Estados Unidos|  7140|\n",
      "|    United Kingdom|     Estados Unidos|  2025|\n",
      "|    Estados Unidos|     United Kingdom|  1970|\n",
      "|             Japan|     Estados Unidos|  1548|\n",
      "|    Estados Unidos|              Japan|  1496|\n",
      "|           Germany|     Estados Unidos|  1468|\n",
      "|    Estados Unidos| Dominican Republic|  1420|\n",
      "|Dominican Republic|     Estados Unidos|  1353|\n",
      "|    Estados Unidos|            Germany|  1336|\n",
      "|       South Korea|     Estados Unidos|  1048|\n",
      "|    Estados Unidos|        The Bahamas|   986|\n",
      "|       The Bahamas|     Estados Unidos|   955|\n",
      "|    Estados Unidos|             France|   952|\n",
      "|            France|     Estados Unidos|   935|\n",
      "|    Estados Unidos|              China|   920|\n",
      "|          Colombia|     Estados Unidos|   873|\n",
      "+------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de acciones\n",
    "print(\"Número de filas en la tabla: {0}\"\n",
    "    .format(datosVuelos2015_Ord.count()))\n",
    "\n",
    "print(datosVuelos2015_Ord.take(3))\n",
    "\n",
    "datosVuelos2015_Ord.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
