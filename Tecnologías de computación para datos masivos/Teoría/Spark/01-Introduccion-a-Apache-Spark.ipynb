{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmaXjOYU9BZ5"
   },
   "source": [
    "# Introducción a [Apache Spark](https://spark.apache.org/)\n",
    "\n",
    "![Spark](https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY1A1qo59BaB"
   },
   "source": [
    "## Plataforma de computación cluster rápida\n",
    "\n",
    "-   Extiende modelo MapReduce soportando de manera eficiente otros tipos\n",
    "    de computación:\n",
    "    -   queries interactivas\n",
    "    -   procesado _streaming_\n",
    "-   Soporta computaciones en memoria\n",
    "-   Mejora a MapReduce para aplicaciones complejas (10-20x más rápido)\n",
    "\n",
    "### Propósito general\n",
    "\n",
    "-   Modos de funcionamiento batch, interactivo o streaming\n",
    "-   Reduce el número de herramientas a emplear y mantener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdhFGnOE9BaC"
   },
   "source": [
    "### Historia\n",
    "\n",
    "-   Iniciado en el 2009 en el UC Berkeley RAD Lab (AMPLab)\n",
    "\n",
    "    -   Motivado por la ineficiencia de MapReduce para trabajos\n",
    "        iterativos e interactivos\n",
    "\n",
    "-   Mayores contribuidores: [Databricks](https://databricks.com/),\n",
    "    Yahoo! e Intel\n",
    "\n",
    "-   Declarado open source en marzo del 2010\n",
    "\n",
    "-   Transferido a la Apache Software Foundation en junio de 2013, TLP en\n",
    "    febrero de 2014\n",
    "\n",
    "-   Uno de los proyectos Big Data más activos\n",
    "\n",
    "-   Versión 1.0 lanzada en mayo de 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4678NCtQ9BaE"
   },
   "source": [
    "#### Características de Spark\n",
    "\n",
    "-   Soporta gran variedad de workloads: batch, queries interactivas,\n",
    "    streaming, machine learning, procesado de grafos\n",
    "\n",
    "-   APIs en Scala, Java, Python, SQL y R\n",
    "\n",
    "-   Shells interactivos en Scala, Python, SQL y R\n",
    "\n",
    "-   Se integra con otras soluciones BigData: HDFS, Cassandra, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZIsv7Aq9BaF"
   },
   "source": [
    "### La pila Spark\n",
    "![sparkstack](http://persoal.citius.usc.es/tf.pena/TCDM/figs/sparkstack.png)\n",
    "\n",
    "(Fuente: H. Karau, A. Konwinski, P. Wendell, M. Zaharia, \"Learning Spark\", O'Reilly, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dE0EWa-N9BaF"
   },
   "source": [
    "## APIs del Spark Core\n",
    "Spark ofrece dos APIs:\n",
    "\n",
    " - API estructurada o de alto nivel\n",
    " - API de bajo nivel\n",
    "\n",
    "Cada API ofrece diferentes tipos de datos:\n",
    "\n",
    " - Se recomienda usar la API estructurada por su mayor rendimiento\n",
    " - La API de bajo nivel permite un mayor control sobre la distribución de los datos\n",
    " - La API de alto nivel utiliza las primitivas de bajo nivel\n",
    "\n",
    "## Tipos de datos en la API estructurada\n",
    "\n",
    "### Datasets\n",
    "Colección distribuida de objetos del mismo tipo\n",
    "\n",
    "- Introducida en Spark > 1.6\n",
    "- El API para Datasets sólo está disponible en Scala y Java\n",
    "- No está disponible en Python ni R debido al tipado dinámico de estos lenguages\n",
    "\n",
    "### DataFrames\n",
    "Un DataFrame es un DataSet organizado en columnas con nombre\n",
    "\n",
    "- Conceptualmente equivalente a una tabla en una base de datos relacional o un dataframe en Python Pandas o R\n",
    "- El API para DataFrames está disponible en Scala, Java, Python y R\n",
    "- En [Java](http://spark.apache.org/docs/latest/api/java/index.html \"Interface Row\") y [Scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row \"trait Row extends Serializable\"), un DataFrame es un DataSet de objetos de tipo *Row*\n",
    "\n",
    "\n",
    "## Tipos de datos en la API de bajo nivel\n",
    "### RDDs (Resilient Distributed Datasets)\n",
    "\n",
    "Lista distribuida de objetos\n",
    "- Tipos de datos básico de Spark v1.X\n",
    "\n",
    "\n",
    "## Mejor rendimiento de la API estructurada\n",
    "\n",
    "- Spark con DataFrames y DataSets se aprovecha del uso de datos con estructura para optimizar el rendimiento utilizando el optimizador de consultas [Catalyst](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html \"Deep Dive into Spark SQL’s Catalyst Optimizer\")  y el motor de ejecución [Tungsten](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html \"Project Tungsten: Bringing Apache Spark Closer to Bare Metal\").\n",
    "\n",
    "<img src=\"https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM.png\" alt=\"Mejora de rendimiento\" style=\"width: 650px;\"/>\n",
    "\n",
    "Fuente: [Recent performance improvements in Apache Spark: SQL, Python, DataFrames, and More](https://databricks.com/blog/2015/04/24/recent-performance-improvements-in-apache-spark-sql-python-dataframes-and-more.html \"Recent performance improvements in Apache Spark: SQL, Python, DataFrames, and More\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_Wb31ZH9BaH"
   },
   "source": [
    "## Conceptos clave\n",
    "![sparkcontext](http://persoal.citius.usc.es/tf.pena/TCDM/figs/sparkcontext.png)\n",
    "\n",
    "(Fuente: H. Karau, A. Konwinski, P. Wendell, M. Zaharia, \"Learning Spark\", O'Reilly, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RsHsyhL9BaK"
   },
   "source": [
    "#### Driver\n",
    "\n",
    "-   Crea un `SparkContext`\n",
    "\n",
    "-   Convierte el programa de usuario en tareas:\n",
    "\n",
    "    -   `DAG` de operaciones lógico -> plan de ejecución físico\n",
    "\n",
    "-   Planifica las tareas en los ejecutores\n",
    "\n",
    "#### SparkSession y SparkContext\n",
    "\n",
    "-   `SparkSession`: punto de entrada de todas las funcionalidades de Spark\n",
    "\n",
    "    -   Permite especificar la configuración de la aplicación Spark\n",
    "    -   En el shell de Spark se crea automáticamente, y en el notebook se puede crear automáticamente, aunque aquí lo creamos a mano (variable `spark`)\n",
    "\n",
    "-   `SparkContext`: realiza la conexión con el cluster\n",
    "`\n",
    "    -   Se crea a partir del `SparkSession`\n",
    "    -   Punto de entrada para la API de bajo nivel\n",
    "    -   En el notebook (o el shell de Spark), se define automáticamente (variable `sc`)\n",
    "\n",
    "-   Creación en un script Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CP82EWfZ9BaH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840629 sha256=b7f5023445aa0c1809d7c615bd4f78a9d13ceb8730463a9da95cab2c87a3d52e\n",
      "  Stored in directory: /Users/luisi/Library/Caches/pip/wheels/97/f5/c0/947e2c0942b361ffe58651f36bd7f13772675b3863fd63d1b1\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ugerBGuGH4qL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 16:34:42 WARN Utils: Your hostname, MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 172.18.26.240 instead (on interface en0)\n",
      "24/11/05 16:34:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/05 16:34:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\n",
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"Mi aplicacion\") \\\n",
    "  .config(\"spark.alguna.opcion.de.configuracion\", \"algun-valor\") \\\n",
    "  .master(\"local[*]\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTjzXOuD9BaN"
   },
   "source": [
    "#### Executors\n",
    "\n",
    "-   Ejecutan las tareas individuales y devuelven los resultados al\n",
    "    Driver\n",
    "\n",
    "-   Proporcionan almacenamiento en memoria para los datos de las tareas\n",
    "\n",
    "#### Cluster Manager\n",
    "\n",
    "-   Componente *enchufable* en Spark\n",
    "\n",
    "-   YARN, Mesos o Spark Standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbgfWQ0t9BaO"
   },
   "source": [
    "## Instalación de Spark\n",
    "1. Descargar Apache Spark de http://spark.apache.org/downloads.html\n",
    "    - La versión \"Pre-built for Hadoop 3.x and later\" incorpora Hadoop\n",
    "    - También se puede descargar un versión sin Hadoop para usar una instalación de Hadoop ya disponible\n",
    "    - Alternativamente, es posible construir Spark desde el código fuente\n",
    "2. Extraer el fichero descargado\n",
    "\n",
    "## Ejecución de Spark\n",
    "1. Usando consolas interactivas\n",
    "    - Scala: `spark-shell`\n",
    "    - Python: `pyspark`\n",
    "        - Instalar los paquetes necesarios (instalad `jupyter` si planeáis utilizarlo):\n",
    "            - `pip install pyspark jupyter ipython`\n",
    "        - Añadir en el `PATH` las carpetas de las instalaciones con `pip`:\n",
    "            - `export PATH=$PATH:~/.local/bin`\n",
    "        - Si se quiere ejecutar Spark en el clúster hay que hacer que pueda encontrar la instalación de Hadoop:\n",
    "            - `export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop`\n",
    "        - Python con [IPython](https://ipython.org/): `PYSPARK_DRIVER_PYTHON=ipython pyspark`\n",
    "        - Python con [Jupyter](https://jupyter.org/): `PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" pyspark`\n",
    "    - R: `sparkR`\n",
    "    - SQL: `spark-sql`\n",
    "    - Usando [Apache Zeppelin](https://zeppelin.apache.org/)\n",
    "2. Lanzando un script con `spark-submit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto",
    "id": "ffS6avcC9BaQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de PySpark 3.5.3\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo: muestra la versión de PySpark\n",
    "print(\"Versión de PySpark {0}\".format(spark.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E-wz2xF9BaS"
   },
   "source": [
    "## Documentación\n",
    "La documentación oficial sobre Apache Spark esté en https://spark.apache.org/docs/latest/\n",
    "\n",
    "La documentación de las APIS para los distintos lenguajes está en:\n",
    "\n",
    "  - Python: https://spark.apache.org/docs/latest/api/python/\n",
    "  - Scala: https://spark.apache.org/docs/latest/api/scala/\n",
    "  - Java: https://spark.apache.org/docs/latest/api/java/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
