\chapter{Introducción a la probabilidad}\label{Chapter0} 
% chktex-file 8
% chktex-file 12
% chktex-file 13
% chktex-file 44

\section{Introducción}

El objetivo de los métodos estadísticos es usar evidencias empíricas para mejorar el conocimiento acerca de la población objetivo a partir de miembros representativos (muestras). Se estudia la población de interés midiendo un conjunto de características (variables). A través de métodos de inferencia estadística, se pueden inferir propiedades de la población a partir de la información de las muestras. Las variables se clasifican en:
\begin{itemize}
\item Cualitativas: toman valores como nombres o etiquetas. Pueden ser nominales u ordinales.
\item Cuantitativas: toman valores numéricos. Pueden ser discretas o continuas.
\end{itemize}

El análisis estadístico comienza con un problema donde se necesitan identificar posibles relaciones entre diferentes variables, explicar o predecir cómo cambia una variable respecto de otra, examinar el fundamento de una hipótesis mediante contrastes de hipótesis, etc. \\

\section{Análisis de datos exploratorio (EDA)}

Tras recoger los datos, antes de hacer inferencia estadística o tomar decisiones, se debe hacer una exploración de los datos. Para ello, es común recurrir a tablas de frecuencia y visualizaciones (suelen usarse gráficas de barras para variables categóricas o discretas e histogramas para variables continuas, al dar una idea de la densidad de los datos y describir su forma). Un \textit{boxplot} resume los datos recogiendo la mediana, el primer y tercer cuartil, el intervalo intercuartil y los \textit{outliers} o datos inusuales. \\

\subsection{Estadísticas de resumen}

En la exploración de datos también es común recurrir a estadísticas de resumen. Las estadísticas de resumen son números que resumen ciertas características de los datos. 
\begin{itemize}
\item Medidas de posición: 
\begin{itemize}
\item Media: $\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$
\item Mediana: valor que deja la mitad de los datos por debajo y la otra mitad por encima. No tiene por qué coincidir con la media.
\item Moda: valor que más se repite
\item Cuantiles: valores que dividen los datos en partes iguales. Como casos especiales se tienen los cuartiles ($Q_1$, $Q_2$, $Q_3$) y los percentiles.
\end{itemize}
\item Medidas de dispersión: 
\begin{itemize}
\item Varianza: $s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$. Poblaciones muy diferentes pueden tener la misma media y varianza.
\item Desviación estándar: $\sqrt{s^2}$
\item Rango intercuartil: $Q_3 - Q_1$
\end{itemize}
\item Medidas de forma: 
\begin{itemize}
\item Asimetría: mide la simetría de la distribución. Si es menor que 0, la distribución está desplazada a la derecha, si es 0, la distribución es simétrica, y si es mayor que 0, la distribución está desplazada a la izquierda.
\begin{equation}
\text{\textit{Skew}} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^3}{n s^3}
\end{equation}
\end{itemize}
\end{itemize}


\section{Fundamento de la inferencia estadística}

La inferencia estadística se preocupa principalmente de extraer conclusiones de la población basándose en datos (muestras) recogidos de la misma. Se tiene una serie de parámetros (características de la población) y un conjunto de estadísticas (características de la muestra). Generalmente, se acude a las siguientes herramientas:
\begin{itemize}
\item Estimaciones puntuales: un único valor que estima un parámetro.
\item Intervalos de confianza: intervalos en los que se espera encontrar un parámetro desconocido, con cierto nivel de confianza.
\item Contrastes de hipótesis: para la toma de decisiones ante incertidumbre.
\end{itemize}

Supóngase que se quiera estimar la media de la población $\mu$. Si el número de muestras es suficientemente grande, la media de la muestra $\bar{x}$ se distribuirá normalmente con media $\mu$ (decimos que $\bar{x}$ no presenta sesgo) y desviación estándar $\frac{\sigma}{\sqrt{n}}$ (debido al teorema del límite central). Si se desconoce $\sigma$, se puede usar $s$ en su lugar. \\

De forma general, se busca el valor de un parámetro poblacional $\theta$, pero no se puede medir de forma directa. Para estimarlo, se elige una estadística, es decir, una cantidad $\hat{\theta}$ calculada de la muestra para estimar el parámetro desconocido. Dos características importantes de una estadística son el sesgo (\textit{bias}) y el error estándar:
\begin{itemize}
\item Una estadística está sesgada si su valor esperado no coincide con el valor del parámetro que se quiere estimar. 
\item El error estándar de una estadística es la desviación estándar de la distribución muestral de la estadística.
\end{itemize}

\subsection{Intervalo de confianza}

SEa un parámetro poblacional desconocido $\theta$ y un $\alpha \in [0, 1]$. Un intervalo de confianza con un nivel de confianza $1 - \alpha$ da un rango estimado de valores $[L_1, L_2]$ (que depende de la muestra) tal que 
\begin{equation}
P(L_1 < \theta < L_2) \geq 1 - \alpha  
\end{equation} 

\noindent y tiene la forma 
\begin{equation*}
\text{IC} = \text{Estimación puntual } \pm \hspace{-0.65cm}\underbrace{\text{ Margen de error}}_{\text{(Valor crítico } \times \text{ Error estándar)}}
\end{equation*}

El valor crítico depende de la distribución muestral de la estadística. Como ejemplo, el intervalo de confianza para la media de una población normal con varianza conocida es
\begin{equation}
\text{IC} = \left(\bar{X} - \frac{\sigma}{\sqrt{n}} z_{\alpha/2}, \bar{X} + \frac{\sigma}{\sqrt{n}} z_{\alpha/2}\right)
\end{equation}

\subsubsection{La distribución normal}

La función de densidad de probabilidad de una distribución normal de media $\mu$ y varianza $\sigma^2$ es
\begin{equation}
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}, \quad x \in \mathbb{R}
\end{equation}

Tiene forma simétrica, la media y mediana tienen el mismo valor, cerca de un 68\% de los datos están a una desviación estándar de la media, cerca de un 95\% a dos desviaciones estándar y cerca de un 99.7\% a tres desviaciones estándar. \\

\subsubsection{Distribución de Student}

Las distribuciones de Student son distribuciones continuas teóricas usadas para diferentes análisis estadísticos. Su forma depende del parámetro de los grados de libertad (df). Es simétrica y con forma de campana, como $\mathcal{N}(0, 1)$, pero con colas más pesadas. A medida que aumenta df, las curvas se aproximan a la normal. \\

\subsection{Contrastes de hipótesis}

Cuando se busca entender o explicar algo, se suelen formular las preguntas en forma de hipótesis. En estadística, una hipótesis es un enunciado acerca de la distribución, algún parámetro, una relación entre distribuciones de probabilidad, etc. Se distinguen dos tipos de hipótesis:
\begin{itemize}
\item Hipótesis nula ($H_0$): es la hipótesis que se quiere contrastar.
\item Hipótesis alternativa ($H_1$): es la hipótesis que contradice la hipótesis nula.
\end{itemize}

Un contraste de hipótesis es un proceso de toma de decisión que examina los datos, y en base a la expectativa fijada por la hipótesis nula, lleva a la decisión de aceptar o rechazar la hipótesis nula. \\

\subsubsection{Procedimiento}

El procedimiento para realizar un contraste de hipótesis es el siguiente:
\begin{enumerate}
\item Especificar la hipótesis nula y la hipótesis alternativa.
\item Especificar el nivel de significancia $\alpha$.
\item Recoger la muestra.
\item Calcular las estadísticas de \textit{test}.
\item Determinar las regiones de aceptación/rechazo.
\item Sacar una conclusión acerca de la hipótesis nula.
\end{enumerate}

Otro método consiste en hacer uso del p-valor. El p-valor es la probabilidad de obtener una estadística más extrema que la necesaria para que la hipótesis nula sea correcta. Es el menor nivel de significancia para el cual se puede rechazar la hipótesis nula. Un p-valor pequeño indica que es improbable observar ese valor de la estadística si la hipótesis nula es cierta (los datos observados son inconsistentes con la hipótesis nula). 
\begin{enumerate}
\item Especificar la hipótesis nula y la hipótesis alternativa.
\item Especificar el nivel de significancia $\alpha$.
\item Recoger la muestra.
\item Calcular un \textit{test} estadístico.
\item Calcular el p-valor.
\end{enumerate}

\subsubsection{Errores en los contrastes de hipótesis}

En los contrastes de hipótesis, se pueden cometer dos tipos de errores (tabla \ref{tab:erroreshyp}):
\begin{itemize}
\item Error de tipo I: rechazar la hipótesis nula cuando es cierta ($\alpha = P(\text{Rechazar } H_0 / H_0 \text{ es cierta})$, nivel de significancia).
\item Error de tipo II: aceptar la hipótesis nula cuando es falsa ($\beta = P(\text{No rechazar } H_0 / H_0 \text{ es falsa})$). Aquí se define la potencia del contraste como la probabilidad de rechazar $H_0$ o de que $H_0$ sea falsa, es decir, $1 - \beta$.
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{ccccc}
\cline{3-4}
\cline{3-4}
 & \multicolumn{1}{l|}{} & \multicolumn{2}{c|}{Decision} & \\ 
\cline{3-4}& \multicolumn{1}{l|}{} & \multicolumn{1}{c}{No rechazar $H_0$} & \multicolumn{1}{c|}{Rechazar $H_0$} & \\ \cline{1-4}
\multicolumn{1}{|c|}{\multirow{2}{*}{Verdad}} & \multicolumn{1}{c|}{$H_0$ es cierta} & \begin{tabular}[c]{@{}l@{}}\textcolor{verde}{\ding{51}}\end{tabular} & \multicolumn{1}{l|}{Error de tipo I} &  \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{$H_0$ no es cierta} & Error de tipo II & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textcolor{verde}{\ding{51}}\end{tabular}} & \\ \cline{1-4}
\end{tabular}
\caption{Tipos de errores en los contrastes de hipótesis}
\label{tab:erroreshyp}
\end{table}
